nohup: ignoring input
Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

UMambaEnc: UMambaEnc(
  (encoder): ResidualMambaEncoder(
    (stem): StackedConvBlocks(
      (convs): Sequential(
        (0): ConvDropoutNormReLU(
          (conv): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
          (all_modules): Sequential(
            (0): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
            (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (2): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (5): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (decoder): UNetResDecoder(
    (encoder): ResidualMambaEncoder(
      (stem): StackedConvBlocks(
        (convs): Sequential(
          (0): ConvDropoutNormReLU(
            (conv): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
            (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
            (all_modules): Sequential(
              (0): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (stages): ModuleList(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (4): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (5): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1-2): 2 x StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (transpconvs): ModuleList(
      (0): ConvTranspose3d(32, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (1): ConvTranspose3d(64, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (2): ConvTranspose3d(128, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (3): ConvTranspose3d(128, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (4): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2))
    )
    (lzz_layers): ModuleList(
      (0): ModuleList(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1-2): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (1): ModuleList(
        (0-1): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2-3): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (2): ModuleList(
        (0-2): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (3): ModuleList(
        (0-2): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (4): ModuleList(
        (0-1): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2-4): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
    )
    (lzz_fc): ModuleList(
      (0-4): 5 x ModuleList(
        (0): Sequential(
          (0): Linear(in_features=288, out_features=72, bias=True)
          (1): Tanh()
        )
        (1): Sequential(
          (0): Linear(in_features=72, out_features=18, bias=True)
          (1): Tanh()
        )
        (2): Sequential(
          (0): Linear(in_features=18, out_features=1, bias=True)
          (1): Tanh()
        )
      )
    )
  )
)

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [40, 192, 192], 'median_image_size_in_voxels': [97.0, 512.0, 512.0], 'spacing': [2.5, 0.7958984971046448, 0.7958984971046448], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [3, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset701_AbdomenCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.7958984971046448, 0.7958984971046448], 'original_median_shape_after_transp': [97, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3071.0, 'mean': 97.29716491699219, 'median': 118.0, 'min': -1024.0, 'percentile_00_5': -958.0, 'percentile_99_5': 270.0, 'std': 137.8484649658203}}} 

2024-06-26 16:51:34.260628: unpacking dataset...
2024-06-26 16:51:34.260973: unpacking done...
2024-06-26 16:51:34.261832: do_dummy_2d_data_aug: False
2024-06-26 16:51:34.274991: Unable to plot network architecture:
2024-06-26 16:51:34.275384: No module named 'hiddenlayer'
2024-06-26 16:51:34.285187: 
2024-06-26 16:51:34.285686: Epoch 0
2024-06-26 16:51:34.286241: Current learning rate: 0.001
using pin_memory on device 0
using pin_memory on device 0
2024-06-26 16:52:29.361710: meanmse:       0.100510605
2024-06-26 16:52:29.362717: meanr2:        0.16565630050377447
2024-06-26 16:52:29.363530: train_loss 2.7764
2024-06-26 16:52:29.364164: val_loss 2.1865
2024-06-26 16:52:29.364670: Pseudo dice [0.5]
2024-06-26 16:52:29.365181: Epoch time: 55.08 s
2024-06-26 16:52:29.365606: Yayy! New best R2: 0.1657
2024-06-26 16:52:31.487697: 
2024-06-26 16:52:31.488422: Epoch 1
2024-06-26 16:52:31.488864: Current learning rate: 0.00099
2024-06-26 16:53:10.922139: meanmse:       0.09856235
2024-06-26 16:53:10.923390: meanr2:        0.2057334451093455
2024-06-26 16:53:10.923909: train_loss 1.9935
2024-06-26 16:53:10.924390: val_loss 2.0649
2024-06-26 16:53:10.924806: Pseudo dice [0.5]
2024-06-26 16:53:10.925307: Epoch time: 39.44 s
2024-06-26 16:53:10.925690: Yayy! New best R2: 0.2057
2024-06-26 16:53:13.241173: 
2024-06-26 16:53:13.241875: Epoch 2
2024-06-26 16:53:13.242309: Current learning rate: 0.00098
2024-06-26 16:53:52.957043: meanmse:       0.07576858
2024-06-26 16:53:52.958100: meanr2:        0.3819717599591956
2024-06-26 16:53:52.958594: train_loss 1.8297
2024-06-26 16:53:52.958959: val_loss 1.7497
2024-06-26 16:53:52.959302: Pseudo dice [0.5]
2024-06-26 16:53:52.959667: Epoch time: 39.72 s
2024-06-26 16:53:52.960021: Yayy! New best R2: 0.382
2024-06-26 16:53:55.205061: 
2024-06-26 16:53:55.205632: Epoch 3
2024-06-26 16:53:55.206056: Current learning rate: 0.00097
2024-06-26 16:54:34.281582: meanmse:       0.05751252
2024-06-26 16:54:34.283148: meanr2:        0.5434088888245037
2024-06-26 16:54:34.283718: train_loss 1.6198
2024-06-26 16:54:34.284118: val_loss 1.4546
2024-06-26 16:54:34.284497: Pseudo dice [0.5]
2024-06-26 16:54:34.284912: Epoch time: 39.08 s
2024-06-26 16:54:34.285347: Yayy! New best R2: 0.5434
2024-06-26 16:54:36.578240: 
2024-06-26 16:54:36.579164: Epoch 4
2024-06-26 16:54:36.579768: Current learning rate: 0.00096
2024-06-26 16:55:15.746403: meanmse:       0.04869562
2024-06-26 16:55:15.747908: meanr2:        0.594081636164046
2024-06-26 16:55:15.748810: train_loss 1.4918
2024-06-26 16:55:15.749459: val_loss 1.309
2024-06-26 16:55:15.749897: Pseudo dice [0.5]
2024-06-26 16:55:15.750396: Epoch time: 39.18 s
2024-06-26 16:55:15.750890: Yayy! New best R2: 0.5941
2024-06-26 16:55:18.258472: 
2024-06-26 16:55:18.259357: Epoch 5
2024-06-26 16:55:18.259933: Current learning rate: 0.00095
2024-06-26 16:55:57.318411: meanmse:       0.03983883
2024-06-26 16:55:57.319474: meanr2:        0.669072179889523
2024-06-26 16:55:57.320031: train_loss 1.323
2024-06-26 16:55:57.320487: val_loss 1.1364
2024-06-26 16:55:57.320943: Pseudo dice [0.5]
2024-06-26 16:55:57.321370: Epoch time: 39.07 s
2024-06-26 16:55:57.321773: Yayy! New best R2: 0.6691
2024-06-26 16:55:59.426818: 
2024-06-26 16:55:59.427774: Epoch 6
2024-06-26 16:55:59.428552: Current learning rate: 0.00095
2024-06-26 16:56:38.954374: meanmse:       0.032235146
2024-06-26 16:56:38.955338: meanr2:        0.7327245559868363
2024-06-26 16:56:38.955925: train_loss 1.1738
2024-06-26 16:56:38.956476: val_loss 0.9772
2024-06-26 16:56:38.956935: Pseudo dice [0.5]
2024-06-26 16:56:38.957380: Epoch time: 39.54 s
2024-06-26 16:56:38.957833: Yayy! New best R2: 0.7327
2024-06-26 16:56:41.134986: 
2024-06-26 16:56:41.135687: Epoch 7
2024-06-26 16:56:41.136479: Current learning rate: 0.00094
2024-06-26 16:57:20.602099: meanmse:       0.038958848
2024-06-26 16:57:20.603118: meanr2:        0.6829304717578487
2024-06-26 16:57:20.603647: train_loss 1.1593
2024-06-26 16:57:20.604047: val_loss 1.0894
2024-06-26 16:57:20.604461: Pseudo dice [0.5]
2024-06-26 16:57:20.604870: Epoch time: 39.48 s
2024-06-26 16:57:22.601091: 
2024-06-26 16:57:22.601904: Epoch 8
2024-06-26 16:57:22.602484: Current learning rate: 0.00093
2024-06-26 16:58:01.635172: meanmse:       0.023663737
2024-06-26 16:58:01.636559: meanr2:        0.8127245897751252
2024-06-26 16:58:01.637310: train_loss 1.0272
2024-06-26 16:58:01.637932: val_loss 0.8203
2024-06-26 16:58:01.638512: Pseudo dice [0.5]
2024-06-26 16:58:01.639133: Epoch time: 39.05 s
2024-06-26 16:58:01.639729: Yayy! New best R2: 0.8127
2024-06-26 16:58:03.916339: 
2024-06-26 16:58:03.923059: Epoch 9
2024-06-26 16:58:03.923688: Current learning rate: 0.00092
2024-06-26 16:58:43.035777: meanmse:       0.031909883
2024-06-26 16:58:43.037096: meanr2:        0.7418528250472676
2024-06-26 16:58:43.037712: train_loss 0.9384
2024-06-26 16:58:43.038252: val_loss 0.9255
2024-06-26 16:58:43.038746: Pseudo dice [0.5]
2024-06-26 16:58:43.039252: Epoch time: 39.13 s
2024-06-26 16:58:45.507033: 
2024-06-26 16:58:45.507589: Epoch 10
2024-06-26 16:58:45.507964: Current learning rate: 0.00091
2024-06-26 16:59:24.768397: meanmse:       0.034665853
2024-06-26 16:59:24.769401: meanr2:        0.7147898151473872
2024-06-26 16:59:24.769845: train_loss 0.9494
2024-06-26 16:59:24.770200: val_loss 0.9989
2024-06-26 16:59:24.770525: Pseudo dice [0.5]
2024-06-26 16:59:24.770874: Epoch time: 39.27 s
2024-06-26 16:59:26.608399: 
2024-06-26 16:59:26.609535: Epoch 11
2024-06-26 16:59:26.610013: Current learning rate: 0.0009
2024-06-26 17:00:05.661126: meanmse:       0.030944973
2024-06-26 17:00:05.662276: meanr2:        0.7495105820134333
2024-06-26 17:00:05.662874: train_loss 0.8762
2024-06-26 17:00:05.663594: val_loss 0.9044
2024-06-26 17:00:05.664072: Pseudo dice [0.5]
2024-06-26 17:00:05.664532: Epoch time: 39.06 s
2024-06-26 17:00:07.506757: 
2024-06-26 17:00:07.507398: Epoch 12
2024-06-26 17:00:07.507889: Current learning rate: 0.00089
2024-06-26 17:00:46.325311: meanmse:       0.028029168
2024-06-26 17:00:46.326689: meanr2:        0.7708137953358435
2024-06-26 17:00:46.327578: train_loss 0.8359
2024-06-26 17:00:46.328179: val_loss 0.8404
2024-06-26 17:00:46.328702: Pseudo dice [0.5]
2024-06-26 17:00:46.329215: Epoch time: 38.83 s
2024-06-26 17:00:48.274128: 
2024-06-26 17:00:48.275043: Epoch 13
2024-06-26 17:00:48.275603: Current learning rate: 0.00088
2024-06-26 17:01:27.704660: meanmse:       0.027343351
2024-06-26 17:01:27.705732: meanr2:        0.7744194581459856
2024-06-26 17:01:27.706229: train_loss 0.8681
2024-06-26 17:01:27.706624: val_loss 0.831
2024-06-26 17:01:27.706992: Pseudo dice [0.5]
2024-06-26 17:01:27.707392: Epoch time: 39.44 s
2024-06-26 17:01:30.132437: 
2024-06-26 17:01:30.133613: Epoch 14
2024-06-26 17:01:30.134259: Current learning rate: 0.00087
2024-06-26 17:02:09.292402: meanmse:       0.027835814
2024-06-26 17:02:09.293762: meanr2:        0.7753259606075807
2024-06-26 17:02:09.294282: train_loss 0.8071
2024-06-26 17:02:09.294622: val_loss 0.8262
2024-06-26 17:02:09.294950: Pseudo dice [0.5]
2024-06-26 17:02:09.295307: Epoch time: 39.17 s
2024-06-26 17:02:11.253324: 
2024-06-26 17:02:11.254177: Epoch 15
2024-06-26 17:02:11.254743: Current learning rate: 0.00086
2024-06-26 17:02:50.363967: meanmse:       0.027944438
2024-06-26 17:02:50.365011: meanr2:        0.7758926152405908
2024-06-26 17:02:50.365470: train_loss 0.8295
2024-06-26 17:02:50.365848: val_loss 0.8219
2024-06-26 17:02:50.366247: Pseudo dice [0.5]
2024-06-26 17:02:50.366628: Epoch time: 39.12 s
2024-06-26 17:02:52.299104: 
2024-06-26 17:02:52.299788: Epoch 16
2024-06-26 17:02:52.300333: Current learning rate: 0.00085
2024-06-26 17:03:31.049338: meanmse:       0.02420683
2024-06-26 17:03:31.050831: meanr2:        0.8064064949746189
2024-06-26 17:03:31.051487: train_loss 0.8301
2024-06-26 17:03:31.052084: val_loss 0.773
2024-06-26 17:03:31.052625: Pseudo dice [0.5]
2024-06-26 17:03:31.053170: Epoch time: 38.76 s
2024-06-26 17:03:33.004453: 
2024-06-26 17:03:33.005183: Epoch 17
2024-06-26 17:03:33.005662: Current learning rate: 0.00085
2024-06-26 17:04:12.611725: meanmse:       0.021629138
2024-06-26 17:04:12.613950: meanr2:        0.8243012514715709
2024-06-26 17:04:12.614511: train_loss 0.7588
2024-06-26 17:04:12.614919: val_loss 0.7303
2024-06-26 17:04:12.615331: Pseudo dice [0.5]
2024-06-26 17:04:12.615725: Epoch time: 39.62 s
2024-06-26 17:04:12.616117: Yayy! New best R2: 0.8243
2024-06-26 17:04:14.785660: 
2024-06-26 17:04:14.786380: Epoch 18
2024-06-26 17:04:14.787051: Current learning rate: 0.00084
2024-06-26 17:04:53.652171: meanmse:       0.029564293
2024-06-26 17:04:53.653649: meanr2:        0.7643059412640053
2024-06-26 17:04:53.654186: train_loss 0.8024
2024-06-26 17:04:53.654590: val_loss 0.8431
2024-06-26 17:04:53.655006: Pseudo dice [0.5]
2024-06-26 17:04:53.655384: Epoch time: 38.88 s
2024-06-26 17:04:55.553814: 
2024-06-26 17:04:55.554793: Epoch 19
2024-06-26 17:04:55.555573: Current learning rate: 0.00083
2024-06-26 17:05:34.895920: meanmse:       0.025878845
2024-06-26 17:05:34.897075: meanr2:        0.7917788788418629
2024-06-26 17:05:34.897609: train_loss 0.8011
2024-06-26 17:05:34.898021: val_loss 0.7671
2024-06-26 17:05:34.898434: Pseudo dice [0.5]
2024-06-26 17:05:34.898823: Epoch time: 39.35 s
2024-06-26 17:05:37.049195: 
2024-06-26 17:05:37.049970: Epoch 20
2024-06-26 17:05:37.050546: Current learning rate: 0.00082
2024-06-26 17:06:16.005120: meanmse:       0.024586322
2024-06-26 17:06:16.006378: meanr2:        0.8019030306403055
2024-06-26 17:06:16.006999: train_loss 0.8123
2024-06-26 17:06:16.007383: val_loss 0.7369
2024-06-26 17:06:16.007805: Pseudo dice [0.5]
2024-06-26 17:06:16.008211: Epoch time: 38.96 s
2024-06-26 17:06:17.953571: 
2024-06-26 17:06:17.954444: Epoch 21
2024-06-26 17:06:17.955138: Current learning rate: 0.00081
2024-06-26 17:06:56.902625: meanmse:       0.022128828
2024-06-26 17:06:56.903938: meanr2:        0.8172870784937133
2024-06-26 17:06:56.904549: train_loss 0.7515
2024-06-26 17:06:56.905637: val_loss 0.692
2024-06-26 17:06:56.906094: Pseudo dice [0.5]
2024-06-26 17:06:56.906580: Epoch time: 38.96 s
2024-06-26 17:06:58.811355: 
2024-06-26 17:06:58.812040: Epoch 22
2024-06-26 17:06:58.812519: Current learning rate: 0.0008
2024-06-26 17:07:37.994372: meanmse:       0.021229142
2024-06-26 17:07:37.995275: meanr2:        0.8213586303612286
2024-06-26 17:07:37.995711: train_loss 0.6763
2024-06-26 17:07:37.996117: val_loss 0.682
2024-06-26 17:07:37.996532: Pseudo dice [0.5]
2024-06-26 17:07:37.996957: Epoch time: 39.19 s
2024-06-26 17:07:39.810470: 
2024-06-26 17:07:39.811167: Epoch 23
2024-06-26 17:07:39.811689: Current learning rate: 0.00079
2024-06-26 17:08:18.655179: meanmse:       0.017332777
2024-06-26 17:08:18.656697: meanr2:        0.8581821077473655
2024-06-26 17:08:18.657379: train_loss 0.7203
2024-06-26 17:08:18.657975: val_loss 0.6042
2024-06-26 17:08:18.658493: Pseudo dice [0.5]
2024-06-26 17:08:18.659044: Epoch time: 38.85 s
2024-06-26 17:08:18.659641: Yayy! New best R2: 0.8582
2024-06-26 17:08:20.886832: 
2024-06-26 17:08:20.887461: Epoch 24
2024-06-26 17:08:20.887886: Current learning rate: 0.00078
2024-06-26 17:08:59.926549: meanmse:       0.019621877
2024-06-26 17:08:59.927774: meanr2:        0.8390072808902641
2024-06-26 17:08:59.928330: train_loss 0.7135
2024-06-26 17:08:59.928871: val_loss 0.6398
2024-06-26 17:08:59.929301: Pseudo dice [0.5]
2024-06-26 17:08:59.929711: Epoch time: 39.05 s
2024-06-26 17:09:01.983686: 
2024-06-26 17:09:01.984735: Epoch 25
2024-06-26 17:09:01.985359: Current learning rate: 0.00077
2024-06-26 17:09:40.994989: meanmse:       0.026900148
2024-06-26 17:09:40.996163: meanr2:        0.7809642392045557
2024-06-26 17:09:40.996623: train_loss 0.7244
2024-06-26 17:09:40.997020: val_loss 0.774
2024-06-26 17:09:40.997401: Pseudo dice [0.5]
2024-06-26 17:09:40.997836: Epoch time: 39.02 s
2024-06-26 17:09:42.748385: 
2024-06-26 17:09:42.749214: Epoch 26
2024-06-26 17:09:42.749713: Current learning rate: 0.00076
2024-06-26 17:10:22.092800: meanmse:       0.022840953
2024-06-26 17:10:22.093867: meanr2:        0.819093517224528
2024-06-26 17:10:22.094438: train_loss 0.6983
2024-06-26 17:10:22.094955: val_loss 0.7044
2024-06-26 17:10:22.095374: Pseudo dice [0.5]
2024-06-26 17:10:22.095785: Epoch time: 39.35 s
2024-06-26 17:10:24.056409: 
2024-06-26 17:10:24.058295: Epoch 27
2024-06-26 17:10:24.059191: Current learning rate: 0.00075
2024-06-26 17:11:03.097650: meanmse:       0.020768244
2024-06-26 17:11:03.098746: meanr2:        0.8341883286438683
2024-06-26 17:11:03.099243: train_loss 0.7544
2024-06-26 17:11:03.099614: val_loss 0.6541
2024-06-26 17:11:03.099971: Pseudo dice [0.5]
2024-06-26 17:11:03.100368: Epoch time: 39.05 s
2024-06-26 17:11:04.888697: 
2024-06-26 17:11:04.889619: Epoch 28
2024-06-26 17:11:04.890322: Current learning rate: 0.00074
2024-06-26 17:11:43.972233: meanmse:       0.028744923
2024-06-26 17:11:43.973304: meanr2:        0.7663591743375847
2024-06-26 17:11:43.973768: train_loss 0.6966
2024-06-26 17:11:43.974162: val_loss 0.793
2024-06-26 17:11:43.974556: Pseudo dice [0.5]
2024-06-26 17:11:43.974942: Epoch time: 39.09 s
2024-06-26 17:11:45.802479: 
2024-06-26 17:11:45.803079: Epoch 29
2024-06-26 17:11:45.803558: Current learning rate: 0.00073
2024-06-26 17:12:24.442313: meanmse:       0.016641209
2024-06-26 17:12:24.443558: meanr2:        0.8632397208737271
2024-06-26 17:12:24.444279: train_loss 0.6723
2024-06-26 17:12:24.444750: val_loss 0.5692
2024-06-26 17:12:24.445207: Pseudo dice [0.5]
2024-06-26 17:12:24.445683: Epoch time: 38.65 s
2024-06-26 17:12:24.733836: Yayy! New best R2: 0.8632
2024-06-26 17:12:26.892671: 
2024-06-26 17:12:26.893352: Epoch 30
2024-06-26 17:12:26.893786: Current learning rate: 0.00073
2024-06-26 17:13:06.098249: meanmse:       0.020138659
2024-06-26 17:13:06.099303: meanr2:        0.8358309352757931
2024-06-26 17:13:06.099774: train_loss 0.6672
2024-06-26 17:13:06.100179: val_loss 0.657
2024-06-26 17:13:06.100523: Pseudo dice [0.5]
2024-06-26 17:13:06.100866: Epoch time: 39.21 s
2024-06-26 17:13:07.892899: 
2024-06-26 17:13:07.893700: Epoch 31
2024-06-26 17:13:07.894287: Current learning rate: 0.00072
2024-06-26 17:13:46.861008: meanmse:       0.022910481
2024-06-26 17:13:46.862089: meanr2:        0.8177341570325272
2024-06-26 17:13:46.862550: train_loss 0.6692
2024-06-26 17:13:46.862963: val_loss 0.7028
2024-06-26 17:13:46.863325: Pseudo dice [0.5]
2024-06-26 17:13:46.863687: Epoch time: 38.98 s
2024-06-26 17:13:48.724479: 
2024-06-26 17:13:48.725746: Epoch 32
2024-06-26 17:13:48.726607: Current learning rate: 0.00071
2024-06-26 17:14:27.444522: meanmse:       0.022275683
2024-06-26 17:14:27.445696: meanr2:        0.8183096242514498
2024-06-26 17:14:27.446299: train_loss 0.6646
2024-06-26 17:14:27.446800: val_loss 0.6645
2024-06-26 17:14:27.447229: Pseudo dice [0.5]
2024-06-26 17:14:27.447662: Epoch time: 38.73 s
2024-06-26 17:14:29.288101: 
2024-06-26 17:14:29.288704: Epoch 33
2024-06-26 17:14:29.289109: Current learning rate: 0.0007
2024-06-26 17:15:08.160121: meanmse:       0.02132794
2024-06-26 17:15:08.161220: meanr2:        0.8253009016466926
2024-06-26 17:15:08.161694: train_loss 0.6434
2024-06-26 17:15:08.162088: val_loss 0.6634
2024-06-26 17:15:08.162473: Pseudo dice [0.5]
2024-06-26 17:15:08.162871: Epoch time: 38.88 s
2024-06-26 17:15:09.960577: 
2024-06-26 17:15:09.961334: Epoch 34
2024-06-26 17:15:09.961742: Current learning rate: 0.00069
2024-06-26 17:15:48.772011: meanmse:       0.023617966
2024-06-26 17:15:48.773124: meanr2:        0.806447296326605
2024-06-26 17:15:48.773589: train_loss 0.6026
2024-06-26 17:15:48.773987: val_loss 0.6917
2024-06-26 17:15:48.774385: Pseudo dice [0.5]
2024-06-26 17:15:48.774788: Epoch time: 38.82 s
2024-06-26 17:15:50.603064: 
2024-06-26 17:15:50.603735: Epoch 35
2024-06-26 17:15:50.604166: Current learning rate: 0.00068
2024-06-26 17:16:29.007162: meanmse:       0.021071
2024-06-26 17:16:29.008294: meanr2:        0.8294784899130384
2024-06-26 17:16:29.008803: train_loss 0.6313
2024-06-26 17:16:29.009231: val_loss 0.6438
2024-06-26 17:16:29.009681: Pseudo dice [0.5]
2024-06-26 17:16:29.010155: Epoch time: 38.41 s
2024-06-26 17:16:30.846285: 
2024-06-26 17:16:30.847026: Epoch 36
2024-06-26 17:16:30.847480: Current learning rate: 0.00067
2024-06-26 17:17:09.650969: meanmse:       0.01892946
2024-06-26 17:17:09.652203: meanr2:        0.8470562554863428
2024-06-26 17:17:09.652728: train_loss 0.6226
2024-06-26 17:17:09.653114: val_loss 0.6117
2024-06-26 17:17:09.653468: Pseudo dice [0.5]
2024-06-26 17:17:09.653845: Epoch time: 38.81 s
2024-06-26 17:17:11.531184: 
2024-06-26 17:17:11.533487: Epoch 37
2024-06-26 17:17:11.534195: Current learning rate: 0.00066
2024-06-26 17:17:50.006680: meanmse:       0.016564293
2024-06-26 17:17:50.008541: meanr2:        0.8661144417186027
2024-06-26 17:17:50.009203: train_loss 0.5934
2024-06-26 17:17:50.009642: val_loss 0.5631
2024-06-26 17:17:50.010132: Pseudo dice [0.5]
2024-06-26 17:17:50.010585: Epoch time: 38.48 s
2024-06-26 17:17:50.011052: Yayy! New best R2: 0.8661
2024-06-26 17:17:52.075330: 
2024-06-26 17:17:52.075995: Epoch 38
2024-06-26 17:17:52.076432: Current learning rate: 0.00065
2024-06-26 17:18:30.602934: meanmse:       0.023398904
2024-06-26 17:18:30.603998: meanr2:        0.8163355165807719
2024-06-26 17:18:30.604504: train_loss 0.5791
2024-06-26 17:18:30.604921: val_loss 0.6859
2024-06-26 17:18:30.605337: Pseudo dice [0.5]
2024-06-26 17:18:30.605792: Epoch time: 38.54 s
2024-06-26 17:18:32.631458: 
2024-06-26 17:18:32.632277: Epoch 39
2024-06-26 17:18:32.632848: Current learning rate: 0.00064
2024-06-26 17:19:11.051487: meanmse:       0.021101056
2024-06-26 17:19:11.052609: meanr2:        0.8331300157271585
2024-06-26 17:19:11.053247: train_loss 0.5807
2024-06-26 17:19:11.053818: val_loss 0.6313
2024-06-26 17:19:11.054361: Pseudo dice [0.5]
2024-06-26 17:19:11.054862: Epoch time: 38.43 s
2024-06-26 17:19:13.223601: 
2024-06-26 17:19:13.224470: Epoch 40
2024-06-26 17:19:13.224969: Current learning rate: 0.00063
2024-06-26 17:19:51.798791: meanmse:       0.0191688
2024-06-26 17:19:51.800026: meanr2:        0.847650803668174
2024-06-26 17:19:51.800599: train_loss 0.6246
2024-06-26 17:19:51.801060: val_loss 0.6039
2024-06-26 17:19:51.801488: Pseudo dice [0.5]
2024-06-26 17:19:51.801962: Epoch time: 38.58 s
2024-06-26 17:19:53.617439: 
2024-06-26 17:19:53.618346: Epoch 41
2024-06-26 17:19:53.618900: Current learning rate: 0.00062
2024-06-26 17:20:32.114830: meanmse:       0.018792035
2024-06-26 17:20:32.116026: meanr2:        0.8445642594117626
2024-06-26 17:20:32.116527: train_loss 0.5888
2024-06-26 17:20:32.116926: val_loss 0.5812
2024-06-26 17:20:32.117378: Pseudo dice [0.5]
2024-06-26 17:20:32.117840: Epoch time: 38.5 s
2024-06-26 17:20:33.935239: 
2024-06-26 17:20:33.936068: Epoch 42
2024-06-26 17:20:33.936669: Current learning rate: 0.00061
2024-06-26 17:21:12.548656: meanmse:       0.024835164
2024-06-26 17:21:12.550047: meanr2:        0.794557181270148
2024-06-26 17:21:12.550563: train_loss 0.6266
2024-06-26 17:21:12.550953: val_loss 0.6963
2024-06-26 17:21:12.551344: Pseudo dice [0.5]
2024-06-26 17:21:12.551749: Epoch time: 38.62 s
2024-06-26 17:21:14.336245: 
2024-06-26 17:21:14.337122: Epoch 43
2024-06-26 17:21:14.337754: Current learning rate: 0.0006
2024-06-26 17:21:52.892841: meanmse:       0.022987662
2024-06-26 17:21:52.893873: meanr2:        0.8123059308769469
2024-06-26 17:21:52.894390: train_loss 0.568
2024-06-26 17:21:52.894809: val_loss 0.6782
2024-06-26 17:21:52.895201: Pseudo dice [0.5]
2024-06-26 17:21:52.895616: Epoch time: 38.56 s
2024-06-26 17:21:54.686801: 
2024-06-26 17:21:54.687398: Epoch 44
2024-06-26 17:21:54.687821: Current learning rate: 0.00059
2024-06-26 17:22:33.317975: meanmse:       0.021782143
2024-06-26 17:22:33.321103: meanr2:        0.8217871635888642
2024-06-26 17:22:33.321905: train_loss 0.584
2024-06-26 17:22:33.322621: val_loss 0.6392
2024-06-26 17:22:33.327534: Pseudo dice [0.5]
2024-06-26 17:22:33.328032: Epoch time: 38.64 s
2024-06-26 17:22:35.048654: 
2024-06-26 17:22:35.049578: Epoch 45
2024-06-26 17:22:35.050061: Current learning rate: 0.00058
2024-06-26 17:23:14.155665: meanmse:       0.019982731
2024-06-26 17:23:14.156767: meanr2:        0.8367347919487785
2024-06-26 17:23:14.157415: train_loss 0.5589
2024-06-26 17:23:14.157942: val_loss 0.6154
2024-06-26 17:23:14.158385: Pseudo dice [0.5]
2024-06-26 17:23:14.158858: Epoch time: 39.12 s
2024-06-26 17:23:16.015934: 
2024-06-26 17:23:16.016541: Epoch 46
2024-06-26 17:23:16.016990: Current learning rate: 0.00057
2024-06-26 17:23:54.460925: meanmse:       0.018438525
2024-06-26 17:23:54.462149: meanr2:        0.846204987114631
2024-06-26 17:23:54.462688: train_loss 0.6528
2024-06-26 17:23:54.463219: val_loss 0.5842
2024-06-26 17:23:54.463655: Pseudo dice [0.5]
2024-06-26 17:23:54.464122: Epoch time: 38.45 s
2024-06-26 17:23:56.252911: 
2024-06-26 17:23:56.253644: Epoch 47
2024-06-26 17:23:56.254216: Current learning rate: 0.00056
2024-06-26 17:24:34.472810: meanmse:       0.023989277
2024-06-26 17:24:34.474072: meanr2:        0.8090209170506505
2024-06-26 17:24:34.474699: train_loss 0.562
2024-06-26 17:24:34.475170: val_loss 0.7089
2024-06-26 17:24:34.475663: Pseudo dice [0.5]
2024-06-26 17:24:34.476144: Epoch time: 38.23 s
2024-06-26 17:24:36.256592: 
2024-06-26 17:24:36.257473: Epoch 48
2024-06-26 17:24:36.258036: Current learning rate: 0.00056
2024-06-26 17:25:14.684578: meanmse:       0.02195196
2024-06-26 17:25:14.685530: meanr2:        0.822104525529248
2024-06-26 17:25:14.686008: train_loss 0.5952
2024-06-26 17:25:14.686388: val_loss 0.6652
2024-06-26 17:25:14.686767: Pseudo dice [0.5]
2024-06-26 17:25:14.687156: Epoch time: 38.44 s
2024-06-26 17:25:16.471959: 
2024-06-26 17:25:16.472627: Epoch 49
2024-06-26 17:25:16.473078: Current learning rate: 0.00055
2024-06-26 17:25:55.134959: meanmse:       0.022759784
2024-06-26 17:25:55.136727: meanr2:        0.8140048466297497
2024-06-26 17:25:55.137401: train_loss 0.5812
2024-06-26 17:25:55.137942: val_loss 0.6592
2024-06-26 17:25:55.138474: Pseudo dice [0.5]
2024-06-26 17:25:55.139091: Epoch time: 38.67 s
2024-06-26 17:25:57.129426: 
2024-06-26 17:25:57.130275: Epoch 50
2024-06-26 17:25:57.130774: Current learning rate: 0.00054
2024-06-26 17:26:35.648032: meanmse:       0.02029539
2024-06-26 17:26:35.649561: meanr2:        0.8326939845462974
2024-06-26 17:26:35.650263: train_loss 0.6056
2024-06-26 17:26:35.650861: val_loss 0.6244
2024-06-26 17:26:35.651420: Pseudo dice [0.5]
2024-06-26 17:26:35.652000: Epoch time: 38.53 s
2024-06-26 17:26:37.570076: 
2024-06-26 17:26:37.570784: Epoch 51
2024-06-26 17:26:37.571369: Current learning rate: 0.00053
2024-06-26 17:27:16.067498: meanmse:       0.0178286
2024-06-26 17:27:16.068747: meanr2:        0.8545037444876766
2024-06-26 17:27:16.069382: train_loss 0.6034
2024-06-26 17:27:16.069843: val_loss 0.5851
2024-06-26 17:27:16.070289: Pseudo dice [0.5]
2024-06-26 17:27:16.070760: Epoch time: 38.51 s
2024-06-26 17:27:17.847801: 
2024-06-26 17:27:17.848633: Epoch 52
2024-06-26 17:27:17.849156: Current learning rate: 0.00052
2024-06-26 17:27:56.374402: meanmse:       0.018273251
2024-06-26 17:27:56.375749: meanr2:        0.8536907338767592
2024-06-26 17:27:56.376346: train_loss 0.5301
2024-06-26 17:27:56.376815: val_loss 0.5924
2024-06-26 17:27:56.377289: Pseudo dice [0.5]
2024-06-26 17:27:56.377796: Epoch time: 38.53 s
2024-06-26 17:27:58.195204: 
2024-06-26 17:27:58.196238: Epoch 53
2024-06-26 17:27:58.196720: Current learning rate: 0.00051
2024-06-26 17:28:36.564890: meanmse:       0.017633716
2024-06-26 17:28:36.565877: meanr2:        0.8582996073495256
2024-06-26 17:28:36.566345: train_loss 0.5421
2024-06-26 17:28:36.566756: val_loss 0.5559
2024-06-26 17:28:36.567170: Pseudo dice [0.5]
2024-06-26 17:28:36.567668: Epoch time: 38.38 s
2024-06-26 17:28:38.372895: 
2024-06-26 17:28:38.373584: Epoch 54
2024-06-26 17:28:38.374014: Current learning rate: 0.0005
2024-06-26 17:29:16.815972: meanmse:       0.017457685
2024-06-26 17:29:16.817294: meanr2:        0.8576932791419476
2024-06-26 17:29:16.817883: train_loss 0.533
2024-06-26 17:29:16.818464: val_loss 0.5532
2024-06-26 17:29:16.818993: Pseudo dice [0.5]
2024-06-26 17:29:16.819541: Epoch time: 38.45 s
2024-06-26 17:29:18.605231: 
2024-06-26 17:29:18.605867: Epoch 55
2024-06-26 17:29:18.606423: Current learning rate: 0.00049
2024-06-26 17:29:57.529640: meanmse:       0.019029533
2024-06-26 17:29:57.531003: meanr2:        0.8483980507476261
2024-06-26 17:29:57.531529: train_loss 0.5256
2024-06-26 17:29:57.531997: val_loss 0.5557
2024-06-26 17:29:57.532438: Pseudo dice [0.5]
2024-06-26 17:29:57.532876: Epoch time: 38.93 s
2024-06-26 17:29:59.268572: 
2024-06-26 17:29:59.269711: Epoch 56
2024-06-26 17:29:59.270244: Current learning rate: 0.00048
2024-06-26 17:30:37.683721: meanmse:       0.016408956
2024-06-26 17:30:37.684663: meanr2:        0.8688051310254354
2024-06-26 17:30:37.685325: train_loss 0.5367
2024-06-26 17:30:37.685727: val_loss 0.531
2024-06-26 17:30:37.686089: Pseudo dice [0.5]
2024-06-26 17:30:37.686447: Epoch time: 38.42 s
2024-06-26 17:30:37.686811: Yayy! New best R2: 0.8688
2024-06-26 17:30:39.731435: 
2024-06-26 17:30:39.732026: Epoch 57
2024-06-26 17:30:39.732420: Current learning rate: 0.00047
2024-06-26 17:31:18.133003: meanmse:       0.016734356
2024-06-26 17:31:18.134154: meanr2:        0.8617778673050662
2024-06-26 17:31:18.134656: train_loss 0.5125
2024-06-26 17:31:18.135056: val_loss 0.5316
2024-06-26 17:31:18.135528: Pseudo dice [0.5]
2024-06-26 17:31:18.136038: Epoch time: 38.41 s
2024-06-26 17:31:20.150521: 
2024-06-26 17:31:20.151506: Epoch 58
2024-06-26 17:31:20.152020: Current learning rate: 0.00046
2024-06-26 17:31:58.538672: meanmse:       0.016912537
2024-06-26 17:31:58.539652: meanr2:        0.8601033320875525
2024-06-26 17:31:58.540217: train_loss 0.5527
2024-06-26 17:31:58.540699: val_loss 0.5455
2024-06-26 17:31:58.541130: Pseudo dice [0.5]
2024-06-26 17:31:58.541542: Epoch time: 38.4 s
2024-06-26 17:32:00.340804: 
2024-06-26 17:32:00.341729: Epoch 59
2024-06-26 17:32:00.342338: Current learning rate: 0.00045
2024-06-26 17:32:38.886959: meanmse:       0.018421348
2024-06-26 17:32:38.888038: meanr2:        0.8515216404348046
2024-06-26 17:32:38.888521: train_loss 0.5259
2024-06-26 17:32:38.888919: val_loss 0.5646
2024-06-26 17:32:38.889315: Pseudo dice [0.5]
2024-06-26 17:32:38.889703: Epoch time: 38.55 s
2024-06-26 17:32:40.958177: 
2024-06-26 17:32:40.958975: Epoch 60
2024-06-26 17:32:40.959511: Current learning rate: 0.00044
2024-06-26 17:33:19.345914: meanmse:       0.0147830695
2024-06-26 17:33:19.347043: meanr2:        0.8810808313993235
2024-06-26 17:33:19.347796: train_loss 0.5327
2024-06-26 17:33:19.348239: val_loss 0.4886
2024-06-26 17:33:19.348729: Pseudo dice [0.5]
2024-06-26 17:33:19.349154: Epoch time: 38.4 s
2024-06-26 17:33:19.349571: Yayy! New best R2: 0.8811
2024-06-26 17:33:21.354003: 
2024-06-26 17:33:21.354828: Epoch 61
2024-06-26 17:33:21.355330: Current learning rate: 0.00043
2024-06-26 17:33:59.602506: meanmse:       0.017841497
2024-06-26 17:33:59.603469: meanr2:        0.8563867344287325
2024-06-26 17:33:59.603966: train_loss 0.5094
2024-06-26 17:33:59.604351: val_loss 0.5607
2024-06-26 17:33:59.604729: Pseudo dice [0.5]
2024-06-26 17:33:59.605104: Epoch time: 38.26 s
2024-06-26 17:34:01.503580: 
2024-06-26 17:34:01.504224: Epoch 62
2024-06-26 17:34:01.504669: Current learning rate: 0.00042
2024-06-26 17:34:39.719326: meanmse:       0.020467034
2024-06-26 17:34:39.720453: meanr2:        0.8329888788037859
2024-06-26 17:34:39.720989: train_loss 0.511
2024-06-26 17:34:39.721375: val_loss 0.5958
2024-06-26 17:34:39.721745: Pseudo dice [0.5]
2024-06-26 17:34:39.722138: Epoch time: 38.22 s
2024-06-26 17:34:41.505687: 
2024-06-26 17:34:41.506727: Epoch 63
2024-06-26 17:34:41.507203: Current learning rate: 0.00041
2024-06-26 17:35:20.093655: meanmse:       0.017046072
2024-06-26 17:35:20.094614: meanr2:        0.8638503676938882
2024-06-26 17:35:20.095042: train_loss 0.5191
2024-06-26 17:35:20.095415: val_loss 0.5348
2024-06-26 17:35:20.095763: Pseudo dice [0.5]
2024-06-26 17:35:20.096147: Epoch time: 38.6 s
2024-06-26 17:35:21.873037: 
2024-06-26 17:35:21.873713: Epoch 64
2024-06-26 17:35:21.874213: Current learning rate: 0.0004
2024-06-26 17:36:00.694304: meanmse:       0.021191653
2024-06-26 17:36:00.695308: meanr2:        0.8300398340234427
2024-06-26 17:36:00.695785: train_loss 0.5289
2024-06-26 17:36:00.696211: val_loss 0.6094
2024-06-26 17:36:00.696600: Pseudo dice [0.5]
2024-06-26 17:36:00.697007: Epoch time: 38.83 s
2024-06-26 17:36:02.534090: 
2024-06-26 17:36:02.535350: Epoch 65
2024-06-26 17:36:02.536029: Current learning rate: 0.00039
2024-06-26 17:36:40.704709: meanmse:       0.020917764
2024-06-26 17:36:40.705892: meanr2:        0.8313958947002156
2024-06-26 17:36:40.706405: train_loss 0.5361
2024-06-26 17:36:40.706794: val_loss 0.6231
2024-06-26 17:36:40.707180: Pseudo dice [0.5]
2024-06-26 17:36:40.707564: Epoch time: 38.18 s
2024-06-26 17:36:42.581903: 
2024-06-26 17:36:42.582624: Epoch 66
2024-06-26 17:36:42.583089: Current learning rate: 0.00038
2024-06-26 17:37:21.533168: meanmse:       0.015132052
2024-06-26 17:37:21.534448: meanr2:        0.8748995237767502
2024-06-26 17:37:21.534940: train_loss 0.4868
2024-06-26 17:37:21.535380: val_loss 0.4907
2024-06-26 17:37:21.535810: Pseudo dice [0.5]
2024-06-26 17:37:21.536234: Epoch time: 38.96 s
2024-06-26 17:37:23.311961: 
2024-06-26 17:37:23.312567: Epoch 67
2024-06-26 17:37:23.312987: Current learning rate: 0.00037
2024-06-26 17:38:01.527314: meanmse:       0.017389478
2024-06-26 17:38:01.528775: meanr2:        0.8583835833536682
2024-06-26 17:38:01.529351: train_loss 0.4821
2024-06-26 17:38:01.529904: val_loss 0.5391
2024-06-26 17:38:01.530486: Pseudo dice [0.5]
2024-06-26 17:38:01.531007: Epoch time: 38.22 s
2024-06-26 17:38:03.313241: 
2024-06-26 17:38:03.313824: Epoch 68
2024-06-26 17:38:03.314244: Current learning rate: 0.00036
2024-06-26 17:38:41.422940: meanmse:       0.015188139
2024-06-26 17:38:41.424585: meanr2:        0.8789793363272257
2024-06-26 17:38:41.425219: train_loss 0.4818
2024-06-26 17:38:41.425725: val_loss 0.4917
2024-06-26 17:38:41.426402: Pseudo dice [0.5]
2024-06-26 17:38:41.426989: Epoch time: 38.12 s
2024-06-26 17:38:43.264809: 
2024-06-26 17:38:43.265630: Epoch 69
2024-06-26 17:38:43.266187: Current learning rate: 0.00035
2024-06-26 17:39:21.329909: meanmse:       0.018828996
2024-06-26 17:39:21.331000: meanr2:        0.848037784659257
2024-06-26 17:39:21.331568: train_loss 0.4547
2024-06-26 17:39:21.332007: val_loss 0.5588
2024-06-26 17:39:21.332424: Pseudo dice [0.5]
2024-06-26 17:39:21.332829: Epoch time: 38.07 s
2024-06-26 17:39:23.617769: 
2024-06-26 17:39:23.618498: Epoch 70
2024-06-26 17:39:23.618992: Current learning rate: 0.00034
2024-06-26 17:40:02.054316: meanmse:       0.019383477
2024-06-26 17:40:02.055739: meanr2:        0.8435928604545814
2024-06-26 17:40:02.056268: train_loss 0.4865
2024-06-26 17:40:02.056663: val_loss 0.5774
2024-06-26 17:40:02.057175: Pseudo dice [0.5]
2024-06-26 17:40:02.057612: Epoch time: 38.44 s
2024-06-26 17:40:03.859397: 
2024-06-26 17:40:03.860543: Epoch 71
2024-06-26 17:40:03.861203: Current learning rate: 0.00033
2024-06-26 17:40:42.203612: meanmse:       0.017553864
2024-06-26 17:40:42.204649: meanr2:        0.856648325214054
2024-06-26 17:40:42.205110: train_loss 0.4835
2024-06-26 17:40:42.205559: val_loss 0.5532
2024-06-26 17:40:42.205950: Pseudo dice [0.5]
2024-06-26 17:40:42.206365: Epoch time: 38.36 s
2024-06-26 17:40:44.025892: 
2024-06-26 17:40:44.026725: Epoch 72
2024-06-26 17:40:44.027182: Current learning rate: 0.00032
2024-06-26 17:41:22.659822: meanmse:       0.0148255
2024-06-26 17:41:22.661144: meanr2:        0.8780946340942065
2024-06-26 17:41:22.661808: train_loss 0.4826
2024-06-26 17:41:22.662380: val_loss 0.5007
2024-06-26 17:41:22.662876: Pseudo dice [0.5]
2024-06-26 17:41:22.663406: Epoch time: 38.64 s
2024-06-26 17:41:24.465708: 
2024-06-26 17:41:24.466354: Epoch 73
2024-06-26 17:41:24.466804: Current learning rate: 0.00031
2024-06-26 17:42:03.069038: meanmse:       0.017947825
2024-06-26 17:42:03.070163: meanr2:        0.8541828124349117
2024-06-26 17:42:03.070807: train_loss 0.4782
2024-06-26 17:42:03.071317: val_loss 0.5527
2024-06-26 17:42:03.071797: Pseudo dice [0.5]
2024-06-26 17:42:03.072216: Epoch time: 38.61 s
2024-06-26 17:42:04.918608: 
2024-06-26 17:42:04.919432: Epoch 74
2024-06-26 17:42:04.919882: Current learning rate: 0.0003
2024-06-26 17:42:43.354672: meanmse:       0.017496727
2024-06-26 17:42:43.355937: meanr2:        0.8599028959296896
2024-06-26 17:42:43.356481: train_loss 0.5095
2024-06-26 17:42:43.356939: val_loss 0.5504
2024-06-26 17:42:43.357418: Pseudo dice [0.5]
2024-06-26 17:42:43.357883: Epoch time: 38.44 s
2024-06-26 17:42:45.268400: 
2024-06-26 17:42:45.269164: Epoch 75
2024-06-26 17:42:45.269655: Current learning rate: 0.00029
2024-06-26 17:43:23.794656: meanmse:       0.013084055
2024-06-26 17:43:23.795583: meanr2:        0.893871790565761
2024-06-26 17:43:23.796244: train_loss 0.4895
2024-06-26 17:43:23.796620: val_loss 0.4413
2024-06-26 17:43:23.796971: Pseudo dice [0.5]
2024-06-26 17:43:23.797336: Epoch time: 38.53 s
2024-06-26 17:43:23.797709: Yayy! New best R2: 0.8939
2024-06-26 17:43:25.927544: 
2024-06-26 17:43:25.928493: Epoch 76
2024-06-26 17:43:25.929120: Current learning rate: 0.00028
2024-06-26 17:44:04.541163: meanmse:       0.017946016
2024-06-26 17:44:04.542358: meanr2:        0.8561204466365823
2024-06-26 17:44:04.542878: train_loss 0.4885
2024-06-26 17:44:04.543380: val_loss 0.5415
2024-06-26 17:44:04.543820: Pseudo dice [0.5]
2024-06-26 17:44:04.544290: Epoch time: 38.62 s
2024-06-26 17:44:06.458213: 
2024-06-26 17:44:06.459215: Epoch 77
2024-06-26 17:44:06.459670: Current learning rate: 0.00027
2024-06-26 17:44:45.004387: meanmse:       0.014767735
2024-06-26 17:44:45.005403: meanr2:        0.8808161300686685
2024-06-26 17:44:45.005880: train_loss 0.4769
2024-06-26 17:44:45.006293: val_loss 0.4911
2024-06-26 17:44:45.006699: Pseudo dice [0.5]
2024-06-26 17:44:45.007046: Epoch time: 38.55 s
2024-06-26 17:44:46.847433: 
2024-06-26 17:44:46.848212: Epoch 78
2024-06-26 17:44:46.848710: Current learning rate: 0.00026
2024-06-26 17:45:25.230334: meanmse:       0.01908848
2024-06-26 17:45:25.231540: meanr2:        0.8455608654401179
2024-06-26 17:45:25.232146: train_loss 0.4893
2024-06-26 17:45:25.232648: val_loss 0.5609
2024-06-26 17:45:25.233103: Pseudo dice [0.5]
2024-06-26 17:45:25.233583: Epoch time: 38.39 s
2024-06-26 17:45:27.341175: 
2024-06-26 17:45:27.343172: Epoch 79
2024-06-26 17:45:27.348823: Current learning rate: 0.00025
2024-06-26 17:46:06.795587: meanmse:       0.01812199
2024-06-26 17:46:06.796829: meanr2:        0.8560900021570531
2024-06-26 17:46:06.797390: train_loss 0.5059
2024-06-26 17:46:06.797868: val_loss 0.5665
2024-06-26 17:46:06.798296: Pseudo dice [0.5]
2024-06-26 17:46:06.798727: Epoch time: 39.46 s
2024-06-26 17:46:09.043294: 
2024-06-26 17:46:09.043865: Epoch 80
2024-06-26 17:46:09.044506: Current learning rate: 0.00023
2024-06-26 17:46:47.593795: meanmse:       0.015196457
2024-06-26 17:46:47.594735: meanr2:        0.8751368357108098
2024-06-26 17:46:47.595186: train_loss 0.5231
2024-06-26 17:46:47.595544: val_loss 0.5067
2024-06-26 17:46:47.595898: Pseudo dice [0.5]
2024-06-26 17:46:47.596295: Epoch time: 38.56 s
2024-06-26 17:46:49.443326: 
2024-06-26 17:46:49.444190: Epoch 81
2024-06-26 17:46:49.444699: Current learning rate: 0.00022
2024-06-26 17:47:28.215154: meanmse:       0.014658353
2024-06-26 17:47:28.216205: meanr2:        0.8796748204055771
2024-06-26 17:47:28.216673: train_loss 0.4769
2024-06-26 17:47:28.217055: val_loss 0.4627
2024-06-26 17:47:28.217498: Pseudo dice [0.5]
2024-06-26 17:47:28.217988: Epoch time: 38.78 s
2024-06-26 17:47:30.385664: 
2024-06-26 17:47:30.386343: Epoch 82
2024-06-26 17:47:30.386772: Current learning rate: 0.00021
2024-06-26 17:48:08.669397: meanmse:       0.012193132
2024-06-26 17:48:08.670750: meanr2:        0.9024368637836631
2024-06-26 17:48:08.671315: train_loss 0.4962
2024-06-26 17:48:08.671802: val_loss 0.4199
2024-06-26 17:48:08.672318: Pseudo dice [0.5]
2024-06-26 17:48:08.672845: Epoch time: 38.29 s
2024-06-26 17:48:08.673335: Yayy! New best R2: 0.9024
2024-06-26 17:48:10.770749: 
2024-06-26 17:48:10.777264: Epoch 83
2024-06-26 17:48:10.778110: Current learning rate: 0.0002
2024-06-26 17:48:48.860633: meanmse:       0.015142956
2024-06-26 17:48:48.861791: meanr2:        0.8778425030751614
2024-06-26 17:48:48.862273: train_loss 0.4424
2024-06-26 17:48:48.862657: val_loss 0.4864
2024-06-26 17:48:48.863086: Pseudo dice [0.5]
2024-06-26 17:48:48.863512: Epoch time: 38.1 s
2024-06-26 17:48:50.573666: 
2024-06-26 17:48:50.574346: Epoch 84
2024-06-26 17:48:50.574828: Current learning rate: 0.00019
2024-06-26 17:49:28.989573: meanmse:       0.01835108
2024-06-26 17:49:28.990582: meanr2:        0.8504819408146681
2024-06-26 17:49:28.991056: train_loss 0.4589
2024-06-26 17:49:28.991440: val_loss 0.553
2024-06-26 17:49:28.991792: Pseudo dice [0.5]
2024-06-26 17:49:28.992162: Epoch time: 38.42 s
2024-06-26 17:49:30.840084: 
2024-06-26 17:49:30.840782: Epoch 85
2024-06-26 17:49:30.841246: Current learning rate: 0.00018
2024-06-26 17:50:09.656278: meanmse:       0.014805306
2024-06-26 17:50:09.657389: meanr2:        0.8822997798251009
2024-06-26 17:50:09.657939: train_loss 0.4764
2024-06-26 17:50:09.658440: val_loss 0.4996
2024-06-26 17:50:09.658918: Pseudo dice [0.5]
2024-06-26 17:50:09.659411: Epoch time: 38.82 s
2024-06-26 17:50:11.423539: 
2024-06-26 17:50:11.424549: Epoch 86
2024-06-26 17:50:11.425112: Current learning rate: 0.00017
2024-06-26 17:50:50.126249: meanmse:       0.018120106
2024-06-26 17:50:50.127576: meanr2:        0.8516081335352688
2024-06-26 17:50:50.128210: train_loss 0.4238
2024-06-26 17:50:50.128761: val_loss 0.5281
2024-06-26 17:50:50.129312: Pseudo dice [0.5]
2024-06-26 17:50:50.129849: Epoch time: 38.71 s
2024-06-26 17:50:52.070261: 
2024-06-26 17:50:52.071109: Epoch 87
2024-06-26 17:50:52.071635: Current learning rate: 0.00016
2024-06-26 17:51:30.700143: meanmse:       0.020577973
2024-06-26 17:51:30.701208: meanr2:        0.8356846629456028
2024-06-26 17:51:30.701748: train_loss 0.4393
2024-06-26 17:51:30.702113: val_loss 0.5892
2024-06-26 17:51:30.702458: Pseudo dice [0.5]
2024-06-26 17:51:30.702811: Epoch time: 38.64 s
2024-06-26 17:51:32.503772: 
2024-06-26 17:51:32.504428: Epoch 88
2024-06-26 17:51:32.504864: Current learning rate: 0.00015
2024-06-26 17:52:10.926522: meanmse:       0.019556057
2024-06-26 17:52:10.930628: meanr2:        0.8402417253247173
2024-06-26 17:52:10.931139: train_loss 0.4526
2024-06-26 17:52:10.931538: val_loss 0.5757
2024-06-26 17:52:10.931927: Pseudo dice [0.5]
2024-06-26 17:52:10.932326: Epoch time: 38.43 s
2024-06-26 17:52:12.811116: 
2024-06-26 17:52:12.811714: Epoch 89
2024-06-26 17:52:12.812145: Current learning rate: 0.00014
2024-06-26 17:52:51.405694: meanmse:       0.014057242
2024-06-26 17:52:51.406943: meanr2:        0.8851157591262897
2024-06-26 17:52:51.407484: train_loss 0.4433
2024-06-26 17:52:51.407927: val_loss 0.4929
2024-06-26 17:52:51.408365: Pseudo dice [0.5]
2024-06-26 17:52:51.408818: Epoch time: 38.6 s
2024-06-26 17:52:53.472383: 
2024-06-26 17:52:53.473174: Epoch 90
2024-06-26 17:52:53.473598: Current learning rate: 0.00013
2024-06-26 17:53:32.141884: meanmse:       0.017021589
2024-06-26 17:53:32.143122: meanr2:        0.8629033505074593
2024-06-26 17:53:32.143652: train_loss 0.4293
2024-06-26 17:53:32.144178: val_loss 0.5386
2024-06-26 17:53:32.144712: Pseudo dice [0.5]
2024-06-26 17:53:32.145176: Epoch time: 38.68 s
2024-06-26 17:53:33.857355: 
2024-06-26 17:53:33.858234: Epoch 91
2024-06-26 17:53:33.858826: Current learning rate: 0.00011
2024-06-26 17:54:12.608833: meanmse:       0.02068267
2024-06-26 17:54:12.609595: meanr2:        0.8341328775018813
2024-06-26 17:54:12.610107: train_loss 0.4136
2024-06-26 17:54:12.610586: val_loss 0.6026
2024-06-26 17:54:12.611010: Pseudo dice [0.5]
2024-06-26 17:54:12.611450: Epoch time: 38.76 s
2024-06-26 17:54:14.295668: 
2024-06-26 17:54:14.296556: Epoch 92
2024-06-26 17:54:14.297197: Current learning rate: 0.0001
2024-06-26 17:54:52.808155: meanmse:       0.0199138
2024-06-26 17:54:52.809079: meanr2:        0.8400562625938275
2024-06-26 17:54:52.809546: train_loss 0.4304
2024-06-26 17:54:52.809879: val_loss 0.5733
2024-06-26 17:54:52.810209: Pseudo dice [0.5]
2024-06-26 17:54:52.810553: Epoch time: 38.52 s
2024-06-26 17:54:54.537986: 
2024-06-26 17:54:54.539012: Epoch 93
2024-06-26 17:54:54.539627: Current learning rate: 9e-05
2024-06-26 17:55:33.114292: meanmse:       0.02194923
2024-06-26 17:55:33.115232: meanr2:        0.8233883571903463
2024-06-26 17:55:33.115808: train_loss 0.415
2024-06-26 17:55:33.116243: val_loss 0.603
2024-06-26 17:55:33.116632: Pseudo dice [0.5]
2024-06-26 17:55:33.117070: Epoch time: 38.58 s
2024-06-26 17:55:34.847951: 
2024-06-26 17:55:34.848712: Epoch 94
2024-06-26 17:55:34.849393: Current learning rate: 8e-05
2024-06-26 17:56:13.112602: meanmse:       0.016047
2024-06-26 17:56:13.113697: meanr2:        0.8725360312467733
2024-06-26 17:56:13.114397: train_loss 0.4073
2024-06-26 17:56:13.114772: val_loss 0.5127
2024-06-26 17:56:13.115134: Pseudo dice [0.5]
2024-06-26 17:56:13.115570: Epoch time: 38.27 s
2024-06-26 17:56:14.867796: 
2024-06-26 17:56:14.868556: Epoch 95
2024-06-26 17:56:14.869040: Current learning rate: 7e-05
2024-06-26 17:56:53.299426: meanmse:       0.013466357
2024-06-26 17:56:53.300336: meanr2:        0.8885861862553884
2024-06-26 17:56:53.300838: train_loss 0.4065
2024-06-26 17:56:53.301312: val_loss 0.4523
2024-06-26 17:56:53.301790: Pseudo dice [0.5]
2024-06-26 17:56:53.302258: Epoch time: 38.44 s
2024-06-26 17:56:55.059469: 
2024-06-26 17:56:55.060335: Epoch 96
2024-06-26 17:56:55.060863: Current learning rate: 6e-05
2024-06-26 17:57:33.340438: meanmse:       0.019889332
2024-06-26 17:57:33.341832: meanr2:        0.8388777128915064
2024-06-26 17:57:33.342364: train_loss 0.42
2024-06-26 17:57:33.342885: val_loss 0.5874
2024-06-26 17:57:33.343345: Pseudo dice [0.5]
2024-06-26 17:57:33.343922: Epoch time: 38.29 s
2024-06-26 17:57:35.259970: 
2024-06-26 17:57:35.260966: Epoch 97
2024-06-26 17:57:35.261506: Current learning rate: 4e-05
2024-06-26 17:58:13.704790: meanmse:       0.019960104
2024-06-26 17:58:13.705494: meanr2:        0.8417778750404223
2024-06-26 17:58:13.705947: train_loss 0.4041
2024-06-26 17:58:13.706309: val_loss 0.5734
2024-06-26 17:58:13.706680: Pseudo dice [0.5]
2024-06-26 17:58:13.707069: Epoch time: 38.45 s
2024-06-26 17:58:15.476802: 
2024-06-26 17:58:15.477512: Epoch 98
2024-06-26 17:58:15.478118: Current learning rate: 3e-05
2024-06-26 17:58:54.138573: meanmse:       0.01733499
2024-06-26 17:58:54.139490: meanr2:        0.8573224987435114
2024-06-26 17:58:54.139938: train_loss 0.3912
2024-06-26 17:58:54.140341: val_loss 0.5262
2024-06-26 17:58:54.140738: Pseudo dice [0.5]
2024-06-26 17:58:54.141225: Epoch time: 38.67 s
2024-06-26 17:58:55.919483: 
2024-06-26 17:58:55.920283: Epoch 99
2024-06-26 17:58:55.920779: Current learning rate: 2e-05
2024-06-26 17:59:34.518754: meanmse:       0.01664713
2024-06-26 17:59:34.519740: meanr2:        0.8669280631722667
2024-06-26 17:59:34.520378: train_loss 0.4374
2024-06-26 17:59:34.520813: val_loss 0.5197
2024-06-26 17:59:34.521249: Pseudo dice [0.5]
2024-06-26 17:59:34.521714: Epoch time: 38.61 s
2024-06-26 17:59:36.832448: Training done.
