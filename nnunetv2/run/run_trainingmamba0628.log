nohup: ignoring input
Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

UMambaEnc: UMambaEnc(
  (encoder): ResidualMambaEncoder(
    (stem): StackedConvBlocks(
      (convs): Sequential(
        (0): ConvDropoutNormReLU(
          (conv): Conv3d(3, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
          (all_modules): Sequential(
            (0): Conv3d(3, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
            (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (2): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (5): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (mamba_layers): ModuleList(
      (0): MambaLayer(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=32, out_features=128, bias=False)
          (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
          (act): SiLU()
          (x_proj): Linear(in_features=64, out_features=34, bias=False)
          (dt_proj): Linear(in_features=2, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=32, bias=False)
        )
      )
      (1): MambaLayer(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=64, out_features=256, bias=False)
          (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
          (act): SiLU()
          (x_proj): Linear(in_features=128, out_features=36, bias=False)
          (dt_proj): Linear(in_features=4, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=64, bias=False)
        )
      )
      (2-3): 2 x MambaLayer(
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=128, out_features=512, bias=False)
          (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
          (act): SiLU()
          (x_proj): Linear(in_features=256, out_features=40, bias=False)
          (dt_proj): Linear(in_features=8, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=128, bias=False)
        )
      )
      (4): MambaLayer(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=64, out_features=256, bias=False)
          (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
          (act): SiLU()
          (x_proj): Linear(in_features=128, out_features=36, bias=False)
          (dt_proj): Linear(in_features=4, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=64, bias=False)
        )
      )
      (5): MambaLayer(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=32, out_features=128, bias=False)
          (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
          (act): SiLU()
          (x_proj): Linear(in_features=64, out_features=34, bias=False)
          (dt_proj): Linear(in_features=2, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=32, bias=False)
        )
      )
    )
  )
  (decoder): UNetResDecoder(
    (encoder): ResidualMambaEncoder(
      (stem): StackedConvBlocks(
        (convs): Sequential(
          (0): ConvDropoutNormReLU(
            (conv): Conv3d(3, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
            (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
            (all_modules): Sequential(
              (0): Conv3d(3, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (stages): ModuleList(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (4): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (5): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (mamba_layers): ModuleList(
        (0): MambaLayer(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=32, out_features=128, bias=False)
            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
            (act): SiLU()
            (x_proj): Linear(in_features=64, out_features=34, bias=False)
            (dt_proj): Linear(in_features=2, out_features=64, bias=True)
            (out_proj): Linear(in_features=64, out_features=32, bias=False)
          )
        )
        (1): MambaLayer(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (2-3): 2 x MambaLayer(
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (4): MambaLayer(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (5): MambaLayer(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=32, out_features=128, bias=False)
            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
            (act): SiLU()
            (x_proj): Linear(in_features=64, out_features=34, bias=False)
            (dt_proj): Linear(in_features=2, out_features=64, bias=True)
            (out_proj): Linear(in_features=64, out_features=32, bias=False)
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1-2): 2 x StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (transpconvs): ModuleList(
      (0): ConvTranspose3d(32, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (1): ConvTranspose3d(64, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (2): ConvTranspose3d(128, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (3): ConvTranspose3d(128, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (4): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2))
    )
    (lzz_layers): ModuleList(
      (0): ModuleList(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1-2): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (1): ModuleList(
        (0-1): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2-3): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (2): ModuleList(
        (0-2): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (3): ModuleList(
        (0-2): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (4): ModuleList(
        (0-1): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2-4): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
    )
    (lzz_fc): ModuleList(
      (0-4): 5 x ModuleList(
        (0): Sequential(
          (0): Linear(in_features=288, out_features=72, bias=True)
          (1): Tanh()
        )
        (1): Sequential(
          (0): Linear(in_features=72, out_features=18, bias=True)
          (1): Tanh()
        )
        (2): Sequential(
          (0): Linear(in_features=18, out_features=1, bias=True)
          (1): Tanh()
        )
      )
    )
  )
)

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [40, 192, 192], 'median_image_size_in_voxels': [97.0, 512.0, 512.0], 'spacing': [2.5, 0.7958984971046448, 0.7958984971046448], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [3, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset701_AbdomenCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.7958984971046448, 0.7958984971046448], 'original_median_shape_after_transp': [97, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3071.0, 'mean': 97.29716491699219, 'median': 118.0, 'min': -1024.0, 'percentile_00_5': -958.0, 'percentile_99_5': 270.0, 'std': 137.8484649658203}}} 

2024-06-28 15:23:57.358947: unpacking dataset...
2024-06-28 15:23:57.359357: unpacking done...
2024-06-28 15:23:57.360319: do_dummy_2d_data_aug: False
2024-06-28 15:23:57.375891: Unable to plot network architecture:
2024-06-28 15:23:57.376325: No module named 'hiddenlayer'
2024-06-28 15:23:57.387274: 
2024-06-28 15:23:57.387747: Epoch 0
2024-06-28 15:23:57.388163: Current learning rate: 0.001
using pin_memory on device 0
using pin_memory on device 0
2024-06-28 15:25:12.730848: meanmse:       0.12528487
2024-06-28 15:25:12.732086: meanr2:        -0.023983994183380684
2024-06-28 15:25:12.732799: train_loss 2.8378
2024-06-28 15:25:12.733382: val_loss 2.5494
2024-06-28 15:25:12.733989: Pseudo dice [0.5]
2024-06-28 15:25:12.734544: Epoch time: 75.35 s
2024-06-28 15:25:12.735055: Yayy! New best R2: -0.024
2024-06-28 15:25:15.192377: 
2024-06-28 15:25:15.193145: Epoch 1
2024-06-28 15:25:15.193716: Current learning rate: 0.00099
2024-06-28 15:26:16.882044: meanmse:       0.12788261
2024-06-28 15:26:16.883254: meanr2:        -0.05106276808787729
2024-06-28 15:26:16.883845: train_loss 2.57
2024-06-28 15:26:16.884221: val_loss 2.5527
2024-06-28 15:26:16.884626: Pseudo dice [0.5]
2024-06-28 15:26:16.885030: Epoch time: 61.7 s
2024-06-28 15:26:18.798499: 
2024-06-28 15:26:18.799216: Epoch 2
2024-06-28 15:26:18.799793: Current learning rate: 0.00098
2024-06-28 15:27:20.690133: meanmse:       0.1211246
2024-06-28 15:27:20.691158: meanr2:        0.013750174261447111
2024-06-28 15:27:20.691637: train_loss 2.529
2024-06-28 15:27:20.692045: val_loss 2.5043
2024-06-28 15:27:20.692476: Pseudo dice [0.5]
2024-06-28 15:27:20.692914: Epoch time: 61.9 s
2024-06-28 15:27:20.693330: Yayy! New best R2: 0.0138
2024-06-28 15:27:23.596304: 
2024-06-28 15:27:23.596974: Epoch 3
2024-06-28 15:27:23.597378: Current learning rate: 0.00097
2024-06-28 15:28:25.731973: meanmse:       0.08765872
2024-06-28 15:28:25.733373: meanr2:        0.2920350598302381
2024-06-28 15:28:25.734013: train_loss 2.3101
2024-06-28 15:28:25.734533: val_loss 2.0026
2024-06-28 15:28:25.735420: Pseudo dice [0.5]
2024-06-28 15:28:25.735925: Epoch time: 62.14 s
2024-06-28 15:28:25.736372: Yayy! New best R2: 0.292
2024-06-28 15:28:28.078454: 
2024-06-28 15:28:28.079281: Epoch 4
2024-06-28 15:28:28.079778: Current learning rate: 0.00096
2024-06-28 15:29:30.098029: meanmse:       0.07661578
2024-06-28 15:29:30.099055: meanr2:        0.37222350547801797
2024-06-28 15:29:30.099515: train_loss 1.8758
2024-06-28 15:29:30.099924: val_loss 1.8734
2024-06-28 15:29:30.100421: Pseudo dice [0.5]
2024-06-28 15:29:30.100860: Epoch time: 62.03 s
2024-06-28 15:29:30.101249: Yayy! New best R2: 0.3722
2024-06-28 15:29:32.652089: 
2024-06-28 15:29:32.652788: Epoch 5
2024-06-28 15:29:32.653288: Current learning rate: 0.00095
2024-06-28 15:30:34.459522: meanmse:       0.07699988
2024-06-28 15:30:34.461059: meanr2:        0.3774982045965091
2024-06-28 15:30:34.461700: train_loss 1.6378
2024-06-28 15:30:34.462126: val_loss 1.8668
2024-06-28 15:30:34.462601: Pseudo dice [0.5]
2024-06-28 15:30:34.463063: Epoch time: 61.82 s
2024-06-28 15:30:34.463779: Yayy! New best R2: 0.3775
2024-06-28 15:30:36.900179: 
2024-06-28 15:30:36.900766: Epoch 6
2024-06-28 15:30:36.901214: Current learning rate: 0.00095
2024-06-28 15:31:38.913080: meanmse:       0.05464034
2024-06-28 15:31:38.914412: meanr2:        0.5514619245721243
2024-06-28 15:31:38.914964: train_loss 1.3326
2024-06-28 15:31:38.915387: val_loss 1.4544
2024-06-28 15:31:38.915784: Pseudo dice [0.5]
2024-06-28 15:31:38.916229: Epoch time: 62.02 s
2024-06-28 15:31:38.916645: Yayy! New best R2: 0.5515
2024-06-28 15:31:41.284669: 
2024-06-28 15:31:41.285292: Epoch 7
2024-06-28 15:31:41.285714: Current learning rate: 0.00094
2024-06-28 15:32:43.367241: meanmse:       0.045674335
2024-06-28 15:32:43.368192: meanr2:        0.6327976479828146
2024-06-28 15:32:43.368656: train_loss 1.242
2024-06-28 15:32:43.369011: val_loss 1.3096
2024-06-28 15:32:43.369360: Pseudo dice [0.5]
2024-06-28 15:32:43.369708: Epoch time: 62.09 s
2024-06-28 15:32:43.370049: Yayy! New best R2: 0.6328
2024-06-28 15:32:45.973937: 
2024-06-28 15:32:45.974867: Epoch 8
2024-06-28 15:32:45.975422: Current learning rate: 0.00093
2024-06-28 15:33:47.872048: meanmse:       0.04034498
2024-06-28 15:33:47.873119: meanr2:        0.6756951602305143
2024-06-28 15:33:47.873645: train_loss 1.1227
2024-06-28 15:33:47.874009: val_loss 1.1675
2024-06-28 15:33:47.874341: Pseudo dice [0.5]
2024-06-28 15:33:47.874680: Epoch time: 61.91 s
2024-06-28 15:33:47.875011: Yayy! New best R2: 0.6757
2024-06-28 15:33:50.162888: 
2024-06-28 15:33:50.163666: Epoch 9
2024-06-28 15:33:50.164280: Current learning rate: 0.00092
2024-06-28 15:34:51.889352: meanmse:       0.02601743
2024-06-28 15:34:51.893900: meanr2:        0.7876803502023797
2024-06-28 15:34:51.894575: train_loss 1.0041
2024-06-28 15:34:51.894967: val_loss 0.8772
2024-06-28 15:34:51.895350: Pseudo dice [0.5]
2024-06-28 15:34:51.895763: Epoch time: 61.74 s
2024-06-28 15:34:52.186793: Yayy! New best R2: 0.7877
2024-06-28 15:34:54.608312: 
2024-06-28 15:34:54.609104: Epoch 10
2024-06-28 15:34:54.609693: Current learning rate: 0.00091
2024-06-28 15:35:56.507340: meanmse:       0.034923222
2024-06-28 15:35:56.508262: meanr2:        0.7122000306062403
2024-06-28 15:35:56.508658: train_loss 0.9876
2024-06-28 15:35:56.509037: val_loss 1.0627
2024-06-28 15:35:56.509402: Pseudo dice [0.5]
2024-06-28 15:35:56.509744: Epoch time: 61.91 s
2024-06-28 15:35:58.433716: 
2024-06-28 15:35:58.434794: Epoch 11
2024-06-28 15:35:58.435654: Current learning rate: 0.0009
2024-06-28 15:37:00.329185: meanmse:       0.031821847
2024-06-28 15:37:00.331011: meanr2:        0.7405473270954359
2024-06-28 15:37:00.331715: train_loss 0.9001
2024-06-28 15:37:00.335837: val_loss 0.9674
2024-06-28 15:37:00.336492: Pseudo dice [0.5]
2024-06-28 15:37:00.337039: Epoch time: 61.91 s
2024-06-28 15:37:02.370817: 
2024-06-28 15:37:02.371561: Epoch 12
2024-06-28 15:37:02.372097: Current learning rate: 0.00089
2024-06-28 15:38:04.167951: meanmse:       0.02312655
2024-06-28 15:38:04.169045: meanr2:        0.8125858533189763
2024-06-28 15:38:04.169729: train_loss 0.9325
2024-06-28 15:38:04.170133: val_loss 0.8254
2024-06-28 15:38:04.170500: Pseudo dice [0.5]
2024-06-28 15:38:04.170928: Epoch time: 61.81 s
2024-06-28 15:38:04.171431: Yayy! New best R2: 0.8126
2024-06-28 15:38:06.629795: 
2024-06-28 15:38:06.630760: Epoch 13
2024-06-28 15:38:06.631302: Current learning rate: 0.00088
2024-06-28 15:39:08.467384: meanmse:       0.03506902
2024-06-28 15:39:08.468572: meanr2:        0.7152996628517426
2024-06-28 15:39:08.469161: train_loss 0.8502
2024-06-28 15:39:08.469615: val_loss 1.0317
2024-06-28 15:39:08.469991: Pseudo dice [0.5]
2024-06-28 15:39:08.470390: Epoch time: 61.85 s
2024-06-28 15:39:10.532577: 
2024-06-28 15:39:10.533482: Epoch 14
2024-06-28 15:39:10.534241: Current learning rate: 0.00087
2024-06-28 15:40:12.453986: meanmse:       0.029059362
2024-06-28 15:40:12.455003: meanr2:        0.7654827856735245
2024-06-28 15:40:12.455474: train_loss 0.8324
2024-06-28 15:40:12.457467: val_loss 0.9029
2024-06-28 15:40:12.457850: Pseudo dice [0.5]
2024-06-28 15:40:12.458254: Epoch time: 61.93 s
2024-06-28 15:40:14.431015: 
2024-06-28 15:40:14.431788: Epoch 15
2024-06-28 15:40:14.432306: Current learning rate: 0.00086
2024-06-28 15:41:16.278370: meanmse:       0.03026458
2024-06-28 15:41:16.279211: meanr2:        0.7586363046496125
2024-06-28 15:41:16.279625: train_loss 0.9053
2024-06-28 15:41:16.279945: val_loss 0.9217
2024-06-28 15:41:16.280424: Pseudo dice [0.5]
2024-06-28 15:41:16.280791: Epoch time: 61.86 s
2024-06-28 15:41:18.812187: 
2024-06-28 15:41:18.812705: Epoch 16
2024-06-28 15:41:18.813177: Current learning rate: 0.00085
2024-06-28 15:42:20.707195: meanmse:       0.03528464
2024-06-28 15:42:20.708175: meanr2:        0.7133796947728624
2024-06-28 15:42:20.708720: train_loss 0.8143
2024-06-28 15:42:20.709315: val_loss 1.0151
2024-06-28 15:42:20.709786: Pseudo dice [0.5]
2024-06-28 15:42:20.710226: Epoch time: 61.9 s
2024-06-28 15:42:22.773137: 
2024-06-28 15:42:22.773853: Epoch 17
2024-06-28 15:42:22.774289: Current learning rate: 0.00085
2024-06-28 15:43:24.769494: meanmse:       0.037342306
2024-06-28 15:43:24.770775: meanr2:        0.7046983498605808
2024-06-28 15:43:24.771509: train_loss 0.853
2024-06-28 15:43:24.772003: val_loss 1.0648
2024-06-28 15:43:24.772438: Pseudo dice [0.5]
2024-06-28 15:43:24.772818: Epoch time: 62.0 s
2024-06-28 15:43:26.855659: 
2024-06-28 15:43:26.856368: Epoch 18
2024-06-28 15:43:26.856913: Current learning rate: 0.00084
2024-06-28 15:44:28.835950: meanmse:       0.03148316
2024-06-28 15:44:28.837467: meanr2:        0.7469084212141798
2024-06-28 15:44:28.838162: train_loss 0.7861
2024-06-28 15:44:28.838639: val_loss 0.9218
2024-06-28 15:44:28.839098: Pseudo dice [0.5]
2024-06-28 15:44:28.839645: Epoch time: 61.99 s
2024-06-28 15:44:31.079094: 
2024-06-28 15:44:31.079781: Epoch 19
2024-06-28 15:44:31.080235: Current learning rate: 0.00083
2024-06-28 15:45:33.004819: meanmse:       0.02397899
2024-06-28 15:45:33.006196: meanr2:        0.8043408083473023
2024-06-28 15:45:33.006842: train_loss 0.7674
2024-06-28 15:45:33.007343: val_loss 0.7948
2024-06-28 15:45:33.007793: Pseudo dice [0.5]
2024-06-28 15:45:33.008271: Epoch time: 61.93 s
2024-06-28 15:45:35.335891: 
2024-06-28 15:45:35.336710: Epoch 20
2024-06-28 15:45:35.337398: Current learning rate: 0.00082
2024-06-28 15:46:37.183721: meanmse:       0.025353735
2024-06-28 15:46:37.184788: meanr2:        0.7965169084371314
2024-06-28 15:46:37.185293: train_loss 0.7354
2024-06-28 15:46:37.185755: val_loss 0.8156
2024-06-28 15:46:37.186177: Pseudo dice [0.5]
2024-06-28 15:46:37.186598: Epoch time: 61.86 s
2024-06-28 15:46:39.255229: 
2024-06-28 15:46:39.256114: Epoch 21
2024-06-28 15:46:39.256863: Current learning rate: 0.00081
2024-06-28 15:47:41.270733: meanmse:       0.023645204
2024-06-28 15:47:41.272191: meanr2:        0.8046484426302477
2024-06-28 15:47:41.272847: train_loss 0.7474
2024-06-28 15:47:41.273268: val_loss 0.7699
2024-06-28 15:47:41.273665: Pseudo dice [0.5]
2024-06-28 15:47:41.274115: Epoch time: 62.03 s
2024-06-28 15:47:43.114655: 
2024-06-28 15:47:43.115492: Epoch 22
2024-06-28 15:47:43.116147: Current learning rate: 0.0008
2024-06-28 15:48:44.971891: meanmse:       0.026358949
2024-06-28 15:48:44.973258: meanr2:        0.7869487290360607
2024-06-28 15:48:44.973793: train_loss 0.6976
2024-06-28 15:48:44.974331: val_loss 0.8176
2024-06-28 15:48:44.974767: Pseudo dice [0.5]
2024-06-28 15:48:44.975242: Epoch time: 61.87 s
2024-06-28 15:48:46.859454: 
2024-06-28 15:48:46.860148: Epoch 23
2024-06-28 15:48:46.860698: Current learning rate: 0.00079
2024-06-28 15:49:48.810259: meanmse:       0.032908466
2024-06-28 15:49:48.811520: meanr2:        0.733724976098332
2024-06-28 15:49:48.812162: train_loss 0.6851
2024-06-28 15:49:48.812676: val_loss 0.9156
2024-06-28 15:49:48.813268: Pseudo dice [0.5]
2024-06-28 15:49:48.813884: Epoch time: 61.96 s
2024-06-28 15:49:50.745277: 
2024-06-28 15:49:50.746270: Epoch 24
2024-06-28 15:49:50.747027: Current learning rate: 0.00078
2024-06-28 15:50:52.660973: meanmse:       0.034049734
2024-06-28 15:50:52.662312: meanr2:        0.7230868335494937
2024-06-28 15:50:52.662928: train_loss 0.7412
2024-06-28 15:50:52.663403: val_loss 0.9567
2024-06-28 15:50:52.663850: Pseudo dice [0.5]
2024-06-28 15:50:52.664303: Epoch time: 61.93 s
2024-06-28 15:50:55.076891: 
2024-06-28 15:50:55.077578: Epoch 25
2024-06-28 15:50:55.078089: Current learning rate: 0.00077
2024-06-28 15:51:57.025583: meanmse:       0.036187142
2024-06-28 15:51:57.027174: meanr2:        0.7055448730480762
2024-06-28 15:51:57.027776: train_loss 0.6962
2024-06-28 15:51:57.028523: val_loss 0.9821
2024-06-28 15:51:57.029166: Pseudo dice [0.5]
2024-06-28 15:51:57.029725: Epoch time: 61.96 s
2024-06-28 15:51:58.923831: 
2024-06-28 15:51:58.926054: Epoch 26
2024-06-28 15:51:58.926639: Current learning rate: 0.00076
2024-06-28 15:53:00.774885: meanmse:       0.022158038
2024-06-28 15:53:00.776109: meanr2:        0.8198045771805859
2024-06-28 15:53:00.776628: train_loss 0.687
2024-06-28 15:53:00.777040: val_loss 0.7021
2024-06-28 15:53:00.777514: Pseudo dice [0.5]
2024-06-28 15:53:00.778010: Epoch time: 61.86 s
2024-06-28 15:53:00.778485: Yayy! New best R2: 0.8198
2024-06-28 15:53:02.887405: 
2024-06-28 15:53:02.888113: Epoch 27
2024-06-28 15:53:02.888558: Current learning rate: 0.00075
2024-06-28 15:54:04.697599: meanmse:       0.028100247
2024-06-28 15:54:04.698589: meanr2:        0.7729095039622869
2024-06-28 15:54:04.699063: train_loss 0.6715
2024-06-28 15:54:04.699442: val_loss 0.8207
2024-06-28 15:54:04.699812: Pseudo dice [0.5]
2024-06-28 15:54:04.700385: Epoch time: 61.82 s
2024-06-28 15:54:06.626753: 
2024-06-28 15:54:06.627550: Epoch 28
2024-06-28 15:54:06.628053: Current learning rate: 0.00074
2024-06-28 15:55:08.643111: meanmse:       0.026816426
2024-06-28 15:55:08.644700: meanr2:        0.7836375814890139
2024-06-28 15:55:08.645369: train_loss 0.6698
2024-06-28 15:55:08.645921: val_loss 0.7689
2024-06-28 15:55:08.646476: Pseudo dice [0.5]
2024-06-28 15:55:08.646998: Epoch time: 62.03 s
2024-06-28 15:55:10.507834: 
2024-06-28 15:55:10.508556: Epoch 29
2024-06-28 15:55:10.509020: Current learning rate: 0.00073
2024-06-28 15:56:12.361520: meanmse:       0.021135403
2024-06-28 15:56:12.362933: meanr2:        0.8258650012114219
2024-06-28 15:56:12.363471: train_loss 0.6293
2024-06-28 15:56:12.363959: val_loss 0.6638
2024-06-28 15:56:12.364472: Pseudo dice [0.5]
2024-06-28 15:56:12.365273: Epoch time: 61.86 s
2024-06-28 15:56:12.658998: Yayy! New best R2: 0.8259
2024-06-28 15:56:14.929533: 
2024-06-28 15:56:14.930413: Epoch 30
2024-06-28 15:56:14.930966: Current learning rate: 0.00073
2024-06-28 15:57:16.843376: meanmse:       0.024368215
2024-06-28 15:57:16.846380: meanr2:        0.8077624964719502
2024-06-28 15:57:16.846943: train_loss 0.637
2024-06-28 15:57:16.847353: val_loss 0.7161
2024-06-28 15:57:16.847790: Pseudo dice [0.5]
2024-06-28 15:57:16.848225: Epoch time: 61.92 s
2024-06-28 15:57:18.689837: 
2024-06-28 15:57:18.690593: Epoch 31
2024-06-28 15:57:18.691027: Current learning rate: 0.00072
2024-06-28 15:58:20.724097: meanmse:       0.027262406
2024-06-28 15:58:20.725173: meanr2:        0.7703042587298093
2024-06-28 15:58:20.725727: train_loss 0.6195
2024-06-28 15:58:20.726198: val_loss 0.7612
2024-06-28 15:58:20.726639: Pseudo dice [0.5]
2024-06-28 15:58:20.727062: Epoch time: 62.04 s
2024-06-28 15:58:22.608608: 
2024-06-28 15:58:22.609166: Epoch 32
2024-06-28 15:58:22.609626: Current learning rate: 0.00071
2024-06-28 15:59:24.536270: meanmse:       0.024828745
2024-06-28 15:59:24.537527: meanr2:        0.8004567252825548
2024-06-28 15:59:24.538061: train_loss 0.6202
2024-06-28 15:59:24.538794: val_loss 0.7123
2024-06-28 15:59:24.540474: Pseudo dice [0.5]
2024-06-28 15:59:24.540935: Epoch time: 61.94 s
2024-06-28 15:59:26.459943: 
2024-06-28 15:59:26.460986: Epoch 33
2024-06-28 15:59:26.461556: Current learning rate: 0.0007
2024-06-28 16:00:28.372904: meanmse:       0.03190216
2024-06-28 16:00:28.374214: meanr2:        0.7434884190716482
2024-06-28 16:00:28.374769: train_loss 0.6566
2024-06-28 16:00:28.375228: val_loss 0.8352
2024-06-28 16:00:28.375811: Pseudo dice [0.5]
2024-06-28 16:00:28.376292: Epoch time: 61.92 s
2024-06-28 16:00:30.569866: 
2024-06-28 16:00:30.570949: Epoch 34
2024-06-28 16:00:30.571557: Current learning rate: 0.00069
2024-06-28 16:01:32.499010: meanmse:       0.026885694
2024-06-28 16:01:32.500175: meanr2:        0.779492363739713
2024-06-28 16:01:32.500720: train_loss 0.6429
2024-06-28 16:01:32.501165: val_loss 0.7373
2024-06-28 16:01:32.501595: Pseudo dice [0.5]
2024-06-28 16:01:32.502054: Epoch time: 61.94 s
2024-06-28 16:01:34.344570: 
2024-06-28 16:01:34.345445: Epoch 35
2024-06-28 16:01:34.345964: Current learning rate: 0.00068
2024-06-28 16:02:36.302744: meanmse:       0.021609263
2024-06-28 16:02:36.303917: meanr2:        0.824203109593581
2024-06-28 16:02:36.304406: train_loss 0.6081
2024-06-28 16:02:36.304820: val_loss 0.662
2024-06-28 16:02:36.305227: Pseudo dice [0.5]
2024-06-28 16:02:36.305649: Epoch time: 61.97 s
2024-06-28 16:02:38.210502: 
2024-06-28 16:02:38.211091: Epoch 36
2024-06-28 16:02:38.211538: Current learning rate: 0.00067
2024-06-28 16:03:40.073056: meanmse:       0.024808805
2024-06-28 16:03:40.074886: meanr2:        0.8023730659888664
2024-06-28 16:03:40.075638: train_loss 0.5862
2024-06-28 16:03:40.076281: val_loss 0.7063
2024-06-28 16:03:40.076824: Pseudo dice [0.5]
2024-06-28 16:03:40.077297: Epoch time: 61.87 s
2024-06-28 16:03:41.972011: 
2024-06-28 16:03:41.972900: Epoch 37
2024-06-28 16:03:41.973411: Current learning rate: 0.00066
2024-06-28 16:04:44.080560: meanmse:       0.02262552
2024-06-28 16:04:44.081655: meanr2:        0.8150177107285046
2024-06-28 16:04:44.082097: train_loss 0.5345
2024-06-28 16:04:44.082463: val_loss 0.6589
2024-06-28 16:04:44.082830: Pseudo dice [0.5]
2024-06-28 16:04:44.083197: Epoch time: 62.12 s
2024-06-28 16:04:46.045097: 
2024-06-28 16:04:46.046108: Epoch 38
2024-06-28 16:04:46.046621: Current learning rate: 0.00065
2024-06-28 16:05:48.051101: meanmse:       0.027133591
2024-06-28 16:05:48.052389: meanr2:        0.7796257800409062
2024-06-28 16:05:48.053076: train_loss 0.5262
2024-06-28 16:05:48.053551: val_loss 0.7577
2024-06-28 16:05:48.054033: Pseudo dice [0.5]
2024-06-28 16:05:48.054525: Epoch time: 62.02 s
2024-06-28 16:05:49.906836: 
2024-06-28 16:05:49.907632: Epoch 39
2024-06-28 16:05:49.908081: Current learning rate: 0.00064
2024-06-28 16:06:51.885795: meanmse:       0.034906816
2024-06-28 16:06:51.887013: meanr2:        0.7149004318867254
2024-06-28 16:06:51.887539: train_loss 0.5675
2024-06-28 16:06:51.888021: val_loss 0.8834
2024-06-28 16:06:51.888473: Pseudo dice [0.5]
2024-06-28 16:06:51.888949: Epoch time: 61.99 s
2024-06-28 16:06:54.576836: 
2024-06-28 16:06:54.577461: Epoch 40
2024-06-28 16:06:54.577976: Current learning rate: 0.00063
2024-06-28 16:07:56.478909: meanmse:       0.027915746
2024-06-28 16:07:56.480065: meanr2:        0.7748951018943392
2024-06-28 16:07:56.480591: train_loss 0.5436
2024-06-28 16:07:56.481042: val_loss 0.7521
2024-06-28 16:07:56.481527: Pseudo dice [0.5]
2024-06-28 16:07:56.481989: Epoch time: 61.91 s
2024-06-28 16:07:58.409216: 
2024-06-28 16:07:58.409789: Epoch 41
2024-06-28 16:07:58.410174: Current learning rate: 0.00062
2024-06-28 16:09:00.344650: meanmse:       0.017807635
2024-06-28 16:09:00.345874: meanr2:        0.8587015027035823
2024-06-28 16:09:00.346502: train_loss 0.5344
2024-06-28 16:09:00.347045: val_loss 0.575
2024-06-28 16:09:00.347510: Pseudo dice [0.5]
2024-06-28 16:09:00.348011: Epoch time: 61.94 s
2024-06-28 16:09:00.348539: Yayy! New best R2: 0.8587
2024-06-28 16:09:02.524970: 
2024-06-28 16:09:02.525959: Epoch 42
2024-06-28 16:09:02.526576: Current learning rate: 0.00061
2024-06-28 16:10:04.480195: meanmse:       0.026346559
2024-06-28 16:10:04.481350: meanr2:        0.7906404102316958
2024-06-28 16:10:04.481815: train_loss 0.5096
2024-06-28 16:10:04.482308: val_loss 0.7269
2024-06-28 16:10:04.482798: Pseudo dice [0.5]
2024-06-28 16:10:04.483208: Epoch time: 61.97 s
2024-06-28 16:10:06.586323: 
2024-06-28 16:10:06.587110: Epoch 43
2024-06-28 16:10:06.587649: Current learning rate: 0.0006
2024-06-28 16:11:08.443114: meanmse:       0.022700015
2024-06-28 16:11:08.444379: meanr2:        0.8157341688907046
2024-06-28 16:11:08.444992: train_loss 0.5802
2024-06-28 16:11:08.445476: val_loss 0.6716
2024-06-28 16:11:08.445949: Pseudo dice [0.5]
2024-06-28 16:11:08.446436: Epoch time: 61.87 s
2024-06-28 16:11:10.377954: 
2024-06-28 16:11:10.378465: Epoch 44
2024-06-28 16:11:10.378890: Current learning rate: 0.00059
2024-06-28 16:12:12.324097: meanmse:       0.033491816
2024-06-28 16:12:12.325449: meanr2:        0.7329801405677625
2024-06-28 16:12:12.326056: train_loss 0.5419
2024-06-28 16:12:12.326610: val_loss 0.8492
2024-06-28 16:12:12.332107: Pseudo dice [0.5]
2024-06-28 16:12:12.333696: Epoch time: 61.95 s
2024-06-28 16:12:14.118242: 
2024-06-28 16:12:14.119347: Epoch 45
2024-06-28 16:12:14.119974: Current learning rate: 0.00058
2024-06-28 16:13:16.178121: meanmse:       0.025966085
2024-06-28 16:13:16.180216: meanr2:        0.7878783877724552
2024-06-28 16:13:16.181204: train_loss 0.5431
2024-06-28 16:13:16.181961: val_loss 0.7121
2024-06-28 16:13:16.182662: Pseudo dice [0.5]
2024-06-28 16:13:16.183370: Epoch time: 62.07 s
2024-06-28 16:13:18.165442: 
2024-06-28 16:13:18.166394: Epoch 46
2024-06-28 16:13:18.167018: Current learning rate: 0.00057
2024-06-28 16:14:20.052413: meanmse:       0.034542844
2024-06-28 16:14:20.053682: meanr2:        0.7207940299501933
2024-06-28 16:14:20.054286: train_loss 0.5356
2024-06-28 16:14:20.054809: val_loss 0.8749
2024-06-28 16:14:20.055274: Pseudo dice [0.5]
2024-06-28 16:14:20.055775: Epoch time: 61.9 s
2024-06-28 16:14:22.124647: 
2024-06-28 16:14:22.125435: Epoch 47
2024-06-28 16:14:22.125951: Current learning rate: 0.00056
2024-06-28 16:15:23.974308: meanmse:       0.031187098
2024-06-28 16:15:23.975374: meanr2:        0.7477937300368236
2024-06-28 16:15:23.975855: train_loss 0.5596
2024-06-28 16:15:23.976276: val_loss 0.7947
2024-06-28 16:15:23.976690: Pseudo dice [0.5]
2024-06-28 16:15:23.977204: Epoch time: 61.86 s
2024-06-28 16:15:25.850535: 
2024-06-28 16:15:25.851377: Epoch 48
2024-06-28 16:15:25.851851: Current learning rate: 0.00056
2024-06-28 16:16:27.759674: meanmse:       0.028534193
2024-06-28 16:16:27.761108: meanr2:        0.7696341303134965
2024-06-28 16:16:27.761863: train_loss 0.4988
2024-06-28 16:16:27.762426: val_loss 0.7811
2024-06-28 16:16:27.763008: Pseudo dice [0.5]
2024-06-28 16:16:27.763483: Epoch time: 61.92 s
2024-06-28 16:16:29.621855: 
2024-06-28 16:16:29.622807: Epoch 49
2024-06-28 16:16:29.623337: Current learning rate: 0.00055
2024-06-28 16:17:31.553267: meanmse:       0.030363696
2024-06-28 16:17:31.554828: meanr2:        0.7556491002985737
2024-06-28 16:17:31.555464: train_loss 0.4849
2024-06-28 16:17:31.556054: val_loss 0.8045
2024-06-28 16:17:31.556563: Pseudo dice [0.5]
2024-06-28 16:17:31.557142: Epoch time: 61.94 s
2024-06-28 16:17:33.944100: 
2024-06-28 16:17:33.944942: Epoch 50
2024-06-28 16:17:33.945420: Current learning rate: 0.00054
2024-06-28 16:18:35.820361: meanmse:       0.020283757
2024-06-28 16:18:35.821592: meanr2:        0.8373535493541642
2024-06-28 16:18:35.822181: train_loss 0.4855
2024-06-28 16:18:35.822689: val_loss 0.6236
2024-06-28 16:18:35.823170: Pseudo dice [0.5]
2024-06-28 16:18:35.823646: Epoch time: 61.88 s
2024-06-28 16:18:37.583669: 
2024-06-28 16:18:37.584380: Epoch 51
2024-06-28 16:18:37.584889: Current learning rate: 0.00053
2024-06-28 16:19:39.506339: meanmse:       0.021438232
2024-06-28 16:19:39.507703: meanr2:        0.821488354923464
2024-06-28 16:19:39.508251: train_loss 0.5221
2024-06-28 16:19:39.508714: val_loss 0.6674
2024-06-28 16:19:39.509203: Pseudo dice [0.5]
2024-06-28 16:19:39.509719: Epoch time: 61.93 s
2024-06-28 16:19:41.369946: 
2024-06-28 16:19:41.370753: Epoch 52
2024-06-28 16:19:41.371273: Current learning rate: 0.00052
2024-06-28 16:20:43.339551: meanmse:       0.015413921
2024-06-28 16:20:43.341154: meanr2:        0.8728031103461472
2024-06-28 16:20:43.341708: train_loss 0.5109
2024-06-28 16:20:43.342207: val_loss 0.5409
2024-06-28 16:20:43.342730: Pseudo dice [0.5]
2024-06-28 16:20:43.343255: Epoch time: 61.98 s
2024-06-28 16:20:43.343742: Yayy! New best R2: 0.8728
2024-06-28 16:20:45.796502: 
2024-06-28 16:20:45.797188: Epoch 53
2024-06-28 16:20:45.797682: Current learning rate: 0.00051
2024-06-28 16:21:47.890759: meanmse:       0.025900137
2024-06-28 16:21:47.892100: meanr2:        0.7893070965653748
2024-06-28 16:21:47.892714: train_loss 0.4932
2024-06-28 16:21:47.893296: val_loss 0.7043
2024-06-28 16:21:47.893739: Pseudo dice [0.5]
2024-06-28 16:21:47.894157: Epoch time: 62.1 s
2024-06-28 16:21:49.727586: 
2024-06-28 16:21:49.728399: Epoch 54
2024-06-28 16:21:49.728926: Current learning rate: 0.0005
2024-06-28 16:22:51.629778: meanmse:       0.01949088
2024-06-28 16:22:51.632972: meanr2:        0.8419431437270198
2024-06-28 16:22:51.639173: train_loss 0.4842
2024-06-28 16:22:51.639930: val_loss 0.5984
2024-06-28 16:22:51.640472: Pseudo dice [0.5]
2024-06-28 16:22:51.641126: Epoch time: 61.92 s
2024-06-28 16:22:53.494377: 
2024-06-28 16:22:53.495009: Epoch 55
2024-06-28 16:22:53.495510: Current learning rate: 0.00049
2024-06-28 16:23:55.484332: meanmse:       0.016233053
2024-06-28 16:23:55.485686: meanr2:        0.8694427499615767
2024-06-28 16:23:55.486250: train_loss 0.5251
2024-06-28 16:23:55.486690: val_loss 0.5342
2024-06-28 16:23:55.487144: Pseudo dice [0.5]
2024-06-28 16:23:55.487599: Epoch time: 62.0 s
2024-06-28 16:23:57.365263: 
2024-06-28 16:23:57.366257: Epoch 56
2024-06-28 16:23:57.366880: Current learning rate: 0.00048
2024-06-28 16:24:59.272189: meanmse:       0.015990803
2024-06-28 16:24:59.273406: meanr2:        0.8682588060742973
2024-06-28 16:24:59.273884: train_loss 0.4559
2024-06-28 16:24:59.274288: val_loss 0.5315
2024-06-28 16:24:59.274701: Pseudo dice [0.5]
2024-06-28 16:24:59.275104: Epoch time: 61.92 s
2024-06-28 16:25:01.165975: 
2024-06-28 16:25:01.166619: Epoch 57
2024-06-28 16:25:01.167009: Current learning rate: 0.00047
2024-06-28 16:26:03.038122: meanmse:       0.02411794
2024-06-28 16:26:03.039521: meanr2:        0.8086007884076114
2024-06-28 16:26:03.040166: train_loss 0.4604
2024-06-28 16:26:03.040693: val_loss 0.6802
2024-06-28 16:26:03.041187: Pseudo dice [0.5]
2024-06-28 16:26:03.041690: Epoch time: 61.88 s
2024-06-28 16:26:04.839885: 
2024-06-28 16:26:04.840976: Epoch 58
2024-06-28 16:26:04.841593: Current learning rate: 0.00046
2024-06-28 16:27:06.904998: meanmse:       0.037938062
2024-06-28 16:27:06.906265: meanr2:        0.6926847321738641
2024-06-28 16:27:06.906802: train_loss 0.4674
2024-06-28 16:27:06.907280: val_loss 0.9066
2024-06-28 16:27:06.907747: Pseudo dice [0.5]
2024-06-28 16:27:06.908213: Epoch time: 62.07 s
2024-06-28 16:27:09.044999: 
2024-06-28 16:27:09.045953: Epoch 59
2024-06-28 16:27:09.046502: Current learning rate: 0.00045
2024-06-28 16:28:10.997833: meanmse:       0.026882155
2024-06-28 16:28:10.999133: meanr2:        0.7803362373676351
2024-06-28 16:28:10.999671: train_loss 0.4808
2024-06-28 16:28:11.000149: val_loss 0.7351
2024-06-28 16:28:11.000591: Pseudo dice [0.5]
2024-06-28 16:28:11.001051: Epoch time: 61.96 s
2024-06-28 16:28:13.177865: 
2024-06-28 16:28:13.178710: Epoch 60
2024-06-28 16:28:13.179297: Current learning rate: 0.00044
2024-06-28 16:29:15.156235: meanmse:       0.024182998
2024-06-28 16:29:15.157590: meanr2:        0.8124908017180367
2024-06-28 16:29:15.158189: train_loss 0.4372
2024-06-28 16:29:15.158665: val_loss 0.6719
2024-06-28 16:29:15.159146: Pseudo dice [0.5]
2024-06-28 16:29:15.159634: Epoch time: 61.99 s
2024-06-28 16:29:17.060180: 
2024-06-28 16:29:17.061182: Epoch 61
2024-06-28 16:29:17.061802: Current learning rate: 0.00043
2024-06-28 16:30:19.093429: meanmse:       0.022467775
2024-06-28 16:30:19.094768: meanr2:        0.8157582056374476
2024-06-28 16:30:19.095333: train_loss 0.4709
2024-06-28 16:30:19.095789: val_loss 0.6282
2024-06-28 16:30:19.096237: Pseudo dice [0.5]
2024-06-28 16:30:19.096685: Epoch time: 62.04 s
2024-06-28 16:30:21.276642: 
2024-06-28 16:30:21.277536: Epoch 62
2024-06-28 16:30:21.278070: Current learning rate: 0.00042
2024-06-28 16:31:23.190476: meanmse:       0.024858423
2024-06-28 16:31:23.191641: meanr2:        0.7958486878871166
2024-06-28 16:31:23.192182: train_loss 0.4544
2024-06-28 16:31:23.192627: val_loss 0.678
2024-06-28 16:31:23.193076: Pseudo dice [0.5]
2024-06-28 16:31:23.193539: Epoch time: 61.92 s
2024-06-28 16:31:25.108036: 
2024-06-28 16:31:25.108806: Epoch 63
2024-06-28 16:31:25.109340: Current learning rate: 0.00041
2024-06-28 16:32:27.112213: meanmse:       0.018981619
2024-06-28 16:32:27.113302: meanr2:        0.8425552755941313
2024-06-28 16:32:27.113788: train_loss 0.4684
2024-06-28 16:32:27.114318: val_loss 0.5844
2024-06-28 16:32:27.114954: Pseudo dice [0.5]
2024-06-28 16:32:27.115507: Epoch time: 62.01 s
2024-06-28 16:32:29.068182: 
2024-06-28 16:32:29.069027: Epoch 64
2024-06-28 16:32:29.069649: Current learning rate: 0.0004
2024-06-28 16:33:31.230734: meanmse:       0.024538375
2024-06-28 16:33:31.232099: meanr2:        0.799088114801464
2024-06-28 16:33:31.232694: train_loss 0.4765
2024-06-28 16:33:31.233189: val_loss 0.6803
2024-06-28 16:33:31.233660: Pseudo dice [0.5]
2024-06-28 16:33:31.234187: Epoch time: 62.17 s
2024-06-28 16:33:33.278184: 
2024-06-28 16:33:33.278866: Epoch 65
2024-06-28 16:33:33.279402: Current learning rate: 0.00039
2024-06-28 16:34:34.988828: meanmse:       0.024843924
2024-06-28 16:34:34.989950: meanr2:        0.8012104496936228
2024-06-28 16:34:34.990412: train_loss 0.4731
2024-06-28 16:34:34.990795: val_loss 0.6913
2024-06-28 16:34:34.991195: Pseudo dice [0.5]
2024-06-28 16:34:34.991601: Epoch time: 61.72 s
2024-06-28 16:34:36.815975: 
2024-06-28 16:34:36.816589: Epoch 66
2024-06-28 16:34:36.817007: Current learning rate: 0.00038
2024-06-28 16:35:38.531396: meanmse:       0.023323582
2024-06-28 16:35:38.532634: meanr2:        0.8134449907576496
2024-06-28 16:35:38.533193: train_loss 0.4433
2024-06-28 16:35:38.533594: val_loss 0.6462
2024-06-28 16:35:38.533993: Pseudo dice [0.5]
2024-06-28 16:35:38.534451: Epoch time: 61.72 s
2024-06-28 16:35:40.483578: 
2024-06-28 16:35:40.484809: Epoch 67
2024-06-28 16:35:40.485365: Current learning rate: 0.00037
2024-06-28 16:36:42.226172: meanmse:       0.020106485
2024-06-28 16:36:42.231042: meanr2:        0.8330498874694411
2024-06-28 16:36:42.231722: train_loss 0.4639
2024-06-28 16:36:42.232258: val_loss 0.5886
2024-06-28 16:36:42.232782: Pseudo dice [0.5]
2024-06-28 16:36:42.233304: Epoch time: 61.76 s
2024-06-28 16:36:44.116053: 
2024-06-28 16:36:44.116776: Epoch 68
2024-06-28 16:36:44.119102: Current learning rate: 0.00036
2024-06-28 16:37:45.864629: meanmse:       0.01553533
2024-06-28 16:37:45.865796: meanr2:        0.8754835704595777
2024-06-28 16:37:45.866297: train_loss 0.4321
2024-06-28 16:37:45.866709: val_loss 0.5047
2024-06-28 16:37:45.871754: Pseudo dice [0.5]
2024-06-28 16:37:45.872191: Epoch time: 61.76 s
2024-06-28 16:37:45.872563: Yayy! New best R2: 0.8755
2024-06-28 16:37:48.175638: 
2024-06-28 16:37:48.176646: Epoch 69
2024-06-28 16:37:48.177415: Current learning rate: 0.00035
2024-06-28 16:38:49.933795: meanmse:       0.017367281
2024-06-28 16:38:49.935022: meanr2:        0.8571837420295403
2024-06-28 16:38:49.935573: train_loss 0.4374
2024-06-28 16:38:49.936018: val_loss 0.54
2024-06-28 16:38:49.936449: Pseudo dice [0.5]
2024-06-28 16:38:49.936871: Epoch time: 61.77 s
2024-06-28 16:38:52.375690: 
2024-06-28 16:38:52.376547: Epoch 70
2024-06-28 16:38:52.377172: Current learning rate: 0.00034
2024-06-28 16:39:54.112357: meanmse:       0.017592322
2024-06-28 16:39:54.113663: meanr2:        0.8580680708723715
2024-06-28 16:39:54.114195: train_loss 0.4223
2024-06-28 16:39:54.114639: val_loss 0.5459
2024-06-28 16:39:54.115080: Pseudo dice [0.5]
2024-06-28 16:39:54.115523: Epoch time: 61.75 s
2024-06-28 16:39:56.199101: 
2024-06-28 16:39:56.199683: Epoch 71
2024-06-28 16:39:56.200141: Current learning rate: 0.00033
2024-06-28 16:40:58.051483: meanmse:       0.016271286
2024-06-28 16:40:58.052750: meanr2:        0.8699135763363078
2024-06-28 16:40:58.053267: train_loss 0.4481
2024-06-28 16:40:58.053693: val_loss 0.5161
2024-06-28 16:40:58.054096: Pseudo dice [0.5]
2024-06-28 16:40:58.054547: Epoch time: 61.86 s
2024-06-28 16:40:59.900378: 
2024-06-28 16:40:59.901197: Epoch 72
2024-06-28 16:40:59.901695: Current learning rate: 0.00032
2024-06-28 16:42:01.601177: meanmse:       0.02622
2024-06-28 16:42:01.602438: meanr2:        0.786253345273691
2024-06-28 16:42:01.602988: train_loss 0.4545
2024-06-28 16:42:01.603472: val_loss 0.7012
2024-06-28 16:42:01.609229: Pseudo dice [0.5]
2024-06-28 16:42:01.609769: Epoch time: 61.71 s
2024-06-28 16:42:03.541638: 
2024-06-28 16:42:03.542511: Epoch 73
2024-06-28 16:42:03.543131: Current learning rate: 0.00031
2024-06-28 16:43:05.452753: meanmse:       0.018364258
2024-06-28 16:43:05.453917: meanr2:        0.8490008844831789
2024-06-28 16:43:05.454447: train_loss 0.4272
2024-06-28 16:43:05.454824: val_loss 0.5689
2024-06-28 16:43:05.455212: Pseudo dice [0.5]
2024-06-28 16:43:05.455626: Epoch time: 61.92 s
2024-06-28 16:43:07.626703: 
2024-06-28 16:43:07.627680: Epoch 74
2024-06-28 16:43:07.628265: Current learning rate: 0.0003
2024-06-28 16:44:09.506374: meanmse:       0.015837656
2024-06-28 16:44:09.507485: meanr2:        0.8714942109139024
2024-06-28 16:44:09.507929: train_loss 0.4488
2024-06-28 16:44:09.508325: val_loss 0.5016
2024-06-28 16:44:09.508750: Pseudo dice [0.5]
2024-06-28 16:44:09.509126: Epoch time: 61.89 s
2024-06-28 16:44:11.412914: 
2024-06-28 16:44:11.413954: Epoch 75
2024-06-28 16:44:11.414485: Current learning rate: 0.00029
2024-06-28 16:45:13.161910: meanmse:       0.023398887
2024-06-28 16:45:13.163974: meanr2:        0.8094056226515715
2024-06-28 16:45:13.164573: train_loss 0.4313
2024-06-28 16:45:13.164998: val_loss 0.6345
2024-06-28 16:45:13.165385: Pseudo dice [0.5]
2024-06-28 16:45:13.165842: Epoch time: 61.76 s
2024-06-28 16:45:15.089426: 
2024-06-28 16:45:15.090092: Epoch 76
2024-06-28 16:45:15.090656: Current learning rate: 0.00028
2024-06-28 16:46:16.853973: meanmse:       0.025180709
2024-06-28 16:46:16.855040: meanr2:        0.7930602087865001
2024-06-28 16:46:16.855531: train_loss 0.421
2024-06-28 16:46:16.855917: val_loss 0.6625
2024-06-28 16:46:16.856374: Pseudo dice [0.5]
2024-06-28 16:46:16.856797: Epoch time: 61.77 s
2024-06-28 16:46:19.126620: 
2024-06-28 16:46:19.127324: Epoch 77
2024-06-28 16:46:19.127954: Current learning rate: 0.00027
2024-06-28 16:47:20.853706: meanmse:       0.026215084
2024-06-28 16:47:20.855325: meanr2:        0.7899502726260952
2024-06-28 16:47:20.855987: train_loss 0.3886
2024-06-28 16:47:20.856713: val_loss 0.6932
2024-06-28 16:47:20.857423: Pseudo dice [0.5]
2024-06-28 16:47:20.857934: Epoch time: 61.74 s
2024-06-28 16:47:22.802936: 
2024-06-28 16:47:22.803646: Epoch 78
2024-06-28 16:47:22.804145: Current learning rate: 0.00026
2024-06-28 16:48:24.570912: meanmse:       0.027393276
2024-06-28 16:48:24.572019: meanr2:        0.7782770310542467
2024-06-28 16:48:24.572498: train_loss 0.4369
2024-06-28 16:48:24.572903: val_loss 0.7209
2024-06-28 16:48:24.573321: Pseudo dice [0.5]
2024-06-28 16:48:24.573727: Epoch time: 61.78 s
2024-06-28 16:48:26.478286: 
2024-06-28 16:48:26.479071: Epoch 79
2024-06-28 16:48:26.479561: Current learning rate: 0.00025
2024-06-28 16:49:28.465894: meanmse:       0.018491197
2024-06-28 16:49:28.467263: meanr2:        0.8489068507283651
2024-06-28 16:49:28.467836: train_loss 0.3921
2024-06-28 16:49:28.468371: val_loss 0.5362
2024-06-28 16:49:28.468859: Pseudo dice [0.5]
2024-06-28 16:49:28.469368: Epoch time: 62.0 s
2024-06-28 16:49:30.743278: 
2024-06-28 16:49:30.744359: Epoch 80
2024-06-28 16:49:30.745073: Current learning rate: 0.00023
2024-06-28 16:50:32.569250: meanmse:       0.02047745
2024-06-28 16:50:32.570252: meanr2:        0.8264548064008949
2024-06-28 16:50:32.570727: train_loss 0.3749
2024-06-28 16:50:32.571101: val_loss 0.5822
2024-06-28 16:50:32.571497: Pseudo dice [0.5]
2024-06-28 16:50:32.571888: Epoch time: 61.84 s
2024-06-28 16:50:34.465437: 
2024-06-28 16:50:34.466335: Epoch 81
2024-06-28 16:50:34.466888: Current learning rate: 0.00022
2024-06-28 16:51:36.164878: meanmse:       0.015455878
2024-06-28 16:51:36.166250: meanr2:        0.8746729242848126
2024-06-28 16:51:36.166827: train_loss 0.3831
2024-06-28 16:51:36.167311: val_loss 0.4955
2024-06-28 16:51:36.167812: Pseudo dice [0.5]
2024-06-28 16:51:36.168290: Epoch time: 61.71 s
2024-06-28 16:51:38.064136: 
2024-06-28 16:51:38.064936: Epoch 82
2024-06-28 16:51:38.065411: Current learning rate: 0.00021
2024-06-28 16:52:39.970532: meanmse:       0.018561784
2024-06-28 16:52:39.971477: meanr2:        0.8488151258419948
2024-06-28 16:52:39.971951: train_loss 0.3913
2024-06-28 16:52:39.972311: val_loss 0.5545
2024-06-28 16:52:39.972637: Pseudo dice [0.5]
2024-06-28 16:52:39.972970: Epoch time: 61.91 s
2024-06-28 16:52:41.903702: 
2024-06-28 16:52:41.904780: Epoch 83
2024-06-28 16:52:41.905299: Current learning rate: 0.0002
2024-06-28 16:53:43.970689: meanmse:       0.023230724
2024-06-28 16:53:43.972583: meanr2:        0.8116135790124286
2024-06-28 16:53:43.973300: train_loss 0.3894
2024-06-28 16:53:43.973808: val_loss 0.6269
2024-06-28 16:53:43.974241: Pseudo dice [0.5]
2024-06-28 16:53:43.974636: Epoch time: 62.08 s
2024-06-28 16:53:45.898546: 
2024-06-28 16:53:45.899284: Epoch 84
2024-06-28 16:53:45.899764: Current learning rate: 0.00019
2024-06-28 16:54:47.869924: meanmse:       0.016538931
2024-06-28 16:54:47.870945: meanr2:        0.8673377046887634
2024-06-28 16:54:47.871507: train_loss 0.398
2024-06-28 16:54:47.871896: val_loss 0.505
2024-06-28 16:54:47.872260: Pseudo dice [0.5]
2024-06-28 16:54:47.872623: Epoch time: 61.98 s
2024-06-28 16:54:49.675508: 
2024-06-28 16:54:49.677165: Epoch 85
2024-06-28 16:54:49.677815: Current learning rate: 0.00018
2024-06-28 16:55:51.590014: meanmse:       0.02256776
2024-06-28 16:55:51.591413: meanr2:        0.8182607982921499
2024-06-28 16:55:51.592204: train_loss 0.3928
2024-06-28 16:55:51.592783: val_loss 0.6322
2024-06-28 16:55:51.593364: Pseudo dice [0.5]
2024-06-28 16:55:51.593959: Epoch time: 61.92 s
2024-06-28 16:55:53.430706: 
2024-06-28 16:55:53.431628: Epoch 86
2024-06-28 16:55:53.432277: Current learning rate: 0.00017
2024-06-28 16:56:55.170505: meanmse:       0.021635292
2024-06-28 16:56:55.172149: meanr2:        0.8246448638282259
2024-06-28 16:56:55.172794: train_loss 0.3946
2024-06-28 16:56:55.173447: val_loss 0.6069
2024-06-28 16:56:55.173971: Pseudo dice [0.5]
2024-06-28 16:56:55.174536: Epoch time: 61.75 s
2024-06-28 16:56:56.973744: 
2024-06-28 16:56:56.974501: Epoch 87
2024-06-28 16:56:56.974998: Current learning rate: 0.00016
2024-06-28 16:57:58.728761: meanmse:       0.02227093
2024-06-28 16:57:58.729950: meanr2:        0.8216755377380831
2024-06-28 16:57:58.730493: train_loss 0.3813
2024-06-28 16:57:58.730935: val_loss 0.6163
2024-06-28 16:57:58.731367: Pseudo dice [0.5]
2024-06-28 16:57:58.731813: Epoch time: 61.76 s
2024-06-28 16:58:00.593233: 
2024-06-28 16:58:00.593906: Epoch 88
2024-06-28 16:58:00.594362: Current learning rate: 0.00015
2024-06-28 16:59:02.508617: meanmse:       0.025304727
2024-06-28 16:59:02.509806: meanr2:        0.7969070228124
2024-06-28 16:59:02.510295: train_loss 0.4176
2024-06-28 16:59:02.510717: val_loss 0.6735
2024-06-28 16:59:02.511111: Pseudo dice [0.5]
2024-06-28 16:59:02.511486: Epoch time: 61.92 s
2024-06-28 16:59:04.415336: 
2024-06-28 16:59:04.416017: Epoch 89
2024-06-28 16:59:04.416431: Current learning rate: 0.00014
2024-06-28 17:00:06.267023: meanmse:       0.018666677
2024-06-28 17:00:06.268395: meanr2:        0.8503848679812159
2024-06-28 17:00:06.268956: train_loss 0.3851
2024-06-28 17:00:06.269503: val_loss 0.5431
2024-06-28 17:00:06.269975: Pseudo dice [0.5]
2024-06-28 17:00:06.270431: Epoch time: 61.86 s
2024-06-28 17:00:08.429850: 
2024-06-28 17:00:08.430599: Epoch 90
2024-06-28 17:00:08.431203: Current learning rate: 0.00013
2024-06-28 17:01:10.268643: meanmse:       0.023502799
2024-06-28 17:01:10.273018: meanr2:        0.8082359630951802
2024-06-28 17:01:10.273495: train_loss 0.3613
2024-06-28 17:01:10.273852: val_loss 0.6344
2024-06-28 17:01:10.274215: Pseudo dice [0.5]
2024-06-28 17:01:10.274668: Epoch time: 61.85 s
2024-06-28 17:01:12.020715: 
2024-06-28 17:01:12.021446: Epoch 91
2024-06-28 17:01:12.021957: Current learning rate: 0.00011
2024-06-28 17:02:13.727901: meanmse:       0.019730324
2024-06-28 17:02:13.729451: meanr2:        0.8414275460210536
2024-06-28 17:02:13.730090: train_loss 0.3577
2024-06-28 17:02:13.730696: val_loss 0.5687
2024-06-28 17:02:13.731313: Pseudo dice [0.5]
2024-06-28 17:02:13.731873: Epoch time: 61.72 s
2024-06-28 17:02:15.829305: 
2024-06-28 17:02:15.830189: Epoch 92
2024-06-28 17:02:15.830739: Current learning rate: 0.0001
2024-06-28 17:03:17.567369: meanmse:       0.016187442
2024-06-28 17:03:17.568254: meanr2:        0.8671205341054825
2024-06-28 17:03:17.568715: train_loss 0.3889
2024-06-28 17:03:17.569067: val_loss 0.4871
2024-06-28 17:03:17.569440: Pseudo dice [0.5]
2024-06-28 17:03:17.569795: Epoch time: 61.75 s
2024-06-28 17:03:19.333955: 
2024-06-28 17:03:19.334784: Epoch 93
2024-06-28 17:03:19.335261: Current learning rate: 9e-05
2024-06-28 17:04:21.129862: meanmse:       0.02440145
2024-06-28 17:04:21.131141: meanr2:        0.8029879434032768
2024-06-28 17:04:21.131680: train_loss 0.3578
2024-06-28 17:04:21.132135: val_loss 0.6378
2024-06-28 17:04:21.132533: Pseudo dice [0.5]
2024-06-28 17:04:21.132941: Epoch time: 61.8 s
2024-06-28 17:04:22.918443: 
2024-06-28 17:04:22.919256: Epoch 94
2024-06-28 17:04:22.919848: Current learning rate: 8e-05
2024-06-28 17:05:24.849127: meanmse:       0.0251801
2024-06-28 17:05:24.850411: meanr2:        0.7953724460196604
2024-06-28 17:05:24.851145: train_loss 0.3482
2024-06-28 17:05:24.852231: val_loss 0.6538
2024-06-28 17:05:24.853267: Pseudo dice [0.5]
2024-06-28 17:05:24.854128: Epoch time: 61.94 s
2024-06-28 17:05:26.737552: 
2024-06-28 17:05:26.738467: Epoch 95
2024-06-28 17:05:26.739069: Current learning rate: 7e-05
2024-06-28 17:06:28.621749: meanmse:       0.02578091
2024-06-28 17:06:28.623143: meanr2:        0.7959139099993638
2024-06-28 17:06:28.623819: train_loss 0.3364
2024-06-28 17:06:28.624276: val_loss 0.6847
2024-06-28 17:06:28.624723: Pseudo dice [0.5]
2024-06-28 17:06:28.625257: Epoch time: 61.9 s
2024-06-28 17:06:30.476261: 
2024-06-28 17:06:30.477007: Epoch 96
2024-06-28 17:06:30.477549: Current learning rate: 6e-05
2024-06-28 17:07:32.207205: meanmse:       0.025303178
2024-06-28 17:07:32.208373: meanr2:        0.7974829774582074
2024-06-28 17:07:32.208839: train_loss 0.3426
2024-06-28 17:07:32.209237: val_loss 0.6669
2024-06-28 17:07:32.209630: Pseudo dice [0.5]
2024-06-28 17:07:32.210012: Epoch time: 61.74 s
2024-06-28 17:07:34.062597: 
2024-06-28 17:07:34.063319: Epoch 97
2024-06-28 17:07:34.063797: Current learning rate: 4e-05
2024-06-28 17:08:36.074958: meanmse:       0.022496974
2024-06-28 17:08:36.076281: meanr2:        0.8191774661439609
2024-06-28 17:08:36.076893: train_loss 0.3478
2024-06-28 17:08:36.077348: val_loss 0.6281
2024-06-28 17:08:36.077782: Pseudo dice [0.5]
2024-06-28 17:08:36.078341: Epoch time: 62.02 s
2024-06-28 17:08:37.813169: 
2024-06-28 17:08:37.813917: Epoch 98
2024-06-28 17:08:37.814450: Current learning rate: 3e-05
2024-06-28 17:09:39.753495: meanmse:       0.02182246
2024-06-28 17:09:39.755049: meanr2:        0.826794598812656
2024-06-28 17:09:39.755710: train_loss 0.3448
2024-06-28 17:09:39.756244: val_loss 0.6008
2024-06-28 17:09:39.756760: Pseudo dice [0.5]
2024-06-28 17:09:39.757298: Epoch time: 61.95 s
2024-06-28 17:09:41.645182: 
2024-06-28 17:09:41.646221: Epoch 99
2024-06-28 17:09:41.646863: Current learning rate: 2e-05
2024-06-28 17:10:43.405197: meanmse:       0.02319483
2024-06-28 17:10:43.406119: meanr2:        0.8080615780629811
2024-06-28 17:10:43.406625: train_loss 0.3291
2024-06-28 17:10:43.406968: val_loss 0.6307
2024-06-28 17:10:43.407322: Pseudo dice [0.5]
2024-06-28 17:10:43.407718: Epoch time: 61.77 s
2024-06-28 17:10:45.870721: Training done.
