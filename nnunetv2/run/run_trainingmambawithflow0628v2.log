nohup: ignoring input
Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

UMambaEnc: UMambaEnc(
  (encoder): ResidualMambaEncoder(
    (stem): StackedConvBlocks(
      (convs): Sequential(
        (0): ConvDropoutNormReLU(
          (conv): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
          (all_modules): Sequential(
            (0): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
            (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (2): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (5): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (mamba_layers): ModuleList(
      (0): MambaLayer(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=32, out_features=128, bias=False)
          (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
          (act): SiLU()
          (x_proj): Linear(in_features=64, out_features=34, bias=False)
          (dt_proj): Linear(in_features=2, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=32, bias=False)
        )
      )
      (1): MambaLayer(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=64, out_features=256, bias=False)
          (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
          (act): SiLU()
          (x_proj): Linear(in_features=128, out_features=36, bias=False)
          (dt_proj): Linear(in_features=4, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=64, bias=False)
        )
      )
      (2-3): 2 x MambaLayer(
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=128, out_features=512, bias=False)
          (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
          (act): SiLU()
          (x_proj): Linear(in_features=256, out_features=40, bias=False)
          (dt_proj): Linear(in_features=8, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=128, bias=False)
        )
      )
      (4): MambaLayer(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=64, out_features=256, bias=False)
          (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
          (act): SiLU()
          (x_proj): Linear(in_features=128, out_features=36, bias=False)
          (dt_proj): Linear(in_features=4, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=64, bias=False)
        )
      )
      (5): MambaLayer(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=32, out_features=128, bias=False)
          (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
          (act): SiLU()
          (x_proj): Linear(in_features=64, out_features=34, bias=False)
          (dt_proj): Linear(in_features=2, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=32, bias=False)
        )
      )
    )
  )
  (decoder): UNetResDecoder(
    (encoder): ResidualMambaEncoder(
      (stem): StackedConvBlocks(
        (convs): Sequential(
          (0): ConvDropoutNormReLU(
            (conv): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
            (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
            (all_modules): Sequential(
              (0): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (stages): ModuleList(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (4): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (5): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (mamba_layers): ModuleList(
        (0): MambaLayer(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=32, out_features=128, bias=False)
            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
            (act): SiLU()
            (x_proj): Linear(in_features=64, out_features=34, bias=False)
            (dt_proj): Linear(in_features=2, out_features=64, bias=True)
            (out_proj): Linear(in_features=64, out_features=32, bias=False)
          )
        )
        (1): MambaLayer(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (2-3): 2 x MambaLayer(
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (4): MambaLayer(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (5): MambaLayer(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=32, out_features=128, bias=False)
            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
            (act): SiLU()
            (x_proj): Linear(in_features=64, out_features=34, bias=False)
            (dt_proj): Linear(in_features=2, out_features=64, bias=True)
            (out_proj): Linear(in_features=64, out_features=32, bias=False)
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1-2): 2 x StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (transpconvs): ModuleList(
      (0): ConvTranspose3d(32, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (1): ConvTranspose3d(64, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (2): ConvTranspose3d(128, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (3): ConvTranspose3d(128, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (4): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2))
    )
    (lzz_layers): ModuleList(
      (0): ModuleList(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1-2): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (1): ModuleList(
        (0-1): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2-3): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (2): ModuleList(
        (0-2): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (3): ModuleList(
        (0-2): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (4): ModuleList(
        (0-1): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2-4): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
    )
    (lzz_fc): ModuleList(
      (0-4): 5 x ModuleList(
        (0): Sequential(
          (0): Linear(in_features=288, out_features=72, bias=True)
          (1): Tanh()
        )
        (1): Sequential(
          (0): Linear(in_features=72, out_features=18, bias=True)
          (1): Tanh()
        )
        (2): Sequential(
          (0): Linear(in_features=18, out_features=1, bias=True)
          (1): Tanh()
        )
      )
    )
  )
)

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [40, 192, 192], 'median_image_size_in_voxels': [97.0, 512.0, 512.0], 'spacing': [2.5, 0.7958984971046448, 0.7958984971046448], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [3, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset701_AbdomenCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.7958984971046448, 0.7958984971046448], 'original_median_shape_after_transp': [97, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3071.0, 'mean': 97.29716491699219, 'median': 118.0, 'min': -1024.0, 'percentile_00_5': -958.0, 'percentile_99_5': 270.0, 'std': 137.8484649658203}}} 

2024-06-28 12:08:23.406394: unpacking dataset...
2024-06-28 12:08:23.406827: unpacking done...
2024-06-28 12:08:23.407611: do_dummy_2d_data_aug: False
2024-06-28 12:08:23.421832: Unable to plot network architecture:
2024-06-28 12:08:23.422349: No module named 'hiddenlayer'
2024-06-28 12:08:23.441005: 
2024-06-28 12:08:23.442195: Epoch 0
2024-06-28 12:08:23.443420: Current learning rate: 0.001
using pin_memory on device 0
using pin_memory on device 0
2024-06-28 12:09:44.066331: meanmse:       0.11683169
2024-06-28 12:09:44.067507: meanr2:        0.04690358502450807
2024-06-28 12:09:44.068039: train_loss 2.7041
2024-06-28 12:09:44.068482: val_loss 2.44
2024-06-28 12:09:44.068935: Pseudo dice [0.5]
2024-06-28 12:09:44.069353: Epoch time: 80.64 s
2024-06-28 12:09:44.069763: Yayy! New best R2: 0.0469
2024-06-28 12:09:46.299439: 
2024-06-28 12:09:46.300079: Epoch 1
2024-06-28 12:09:46.300476: Current learning rate: 0.00099
2024-06-28 12:10:50.451614: meanmse:       0.10074563
2024-06-28 12:10:50.452685: meanr2:        0.18587926937898083
2024-06-28 12:10:50.453248: train_loss 2.2757
2024-06-28 12:10:50.453707: val_loss 2.1523
2024-06-28 12:10:50.454149: Pseudo dice [0.5]
2024-06-28 12:10:50.454586: Epoch time: 64.16 s
2024-06-28 12:10:50.454997: Yayy! New best R2: 0.1859
2024-06-28 12:10:52.660609: 
2024-06-28 12:10:52.661679: Epoch 2
2024-06-28 12:10:52.662368: Current learning rate: 0.00098
2024-06-28 12:11:57.304618: meanmse:       0.084524915
2024-06-28 12:11:57.306422: meanr2:        0.32693511545088866
2024-06-28 12:11:57.306953: train_loss 2.0358
2024-06-28 12:11:57.307348: val_loss 1.8846
2024-06-28 12:11:57.307712: Pseudo dice [0.5]
2024-06-28 12:11:57.308096: Epoch time: 64.65 s
2024-06-28 12:11:57.308479: Yayy! New best R2: 0.3269
2024-06-28 12:11:59.625540: 
2024-06-28 12:11:59.626223: Epoch 3
2024-06-28 12:11:59.626734: Current learning rate: 0.00097
2024-06-28 12:13:04.097622: meanmse:       0.06520648
2024-06-28 12:13:04.098831: meanr2:        0.4704920623338985
2024-06-28 12:13:04.099560: train_loss 1.7305
2024-06-28 12:13:04.100176: val_loss 1.5588
2024-06-28 12:13:04.100628: Pseudo dice [0.5]
2024-06-28 12:13:04.101074: Epoch time: 64.48 s
2024-06-28 12:13:04.101557: Yayy! New best R2: 0.4705
2024-06-28 12:13:06.315799: 
2024-06-28 12:13:06.316483: Epoch 4
2024-06-28 12:13:06.316898: Current learning rate: 0.00096
2024-06-28 12:14:10.505515: meanmse:       0.06395175
2024-06-28 12:14:10.506613: meanr2:        0.49163479920709663
2024-06-28 12:14:10.507125: train_loss 1.4336
2024-06-28 12:14:10.507607: val_loss 1.5815
2024-06-28 12:14:10.508055: Pseudo dice [0.5]
2024-06-28 12:14:10.508505: Epoch time: 64.2 s
2024-06-28 12:14:10.508926: Yayy! New best R2: 0.4916
2024-06-28 12:14:13.226144: 
2024-06-28 12:14:13.226917: Epoch 5
2024-06-28 12:14:13.227501: Current learning rate: 0.00095
2024-06-28 12:15:17.563002: meanmse:       0.04237753
2024-06-28 12:15:17.564298: meanr2:        0.6479924054428614
2024-06-28 12:15:17.564829: train_loss 1.2906
2024-06-28 12:15:17.565289: val_loss 1.1833
2024-06-28 12:15:17.565657: Pseudo dice [0.5]
2024-06-28 12:15:17.566035: Epoch time: 64.35 s
2024-06-28 12:15:17.566436: Yayy! New best R2: 0.648
2024-06-28 12:15:19.888469: 
2024-06-28 12:15:19.889145: Epoch 6
2024-06-28 12:15:19.889573: Current learning rate: 0.00095
2024-06-28 12:16:24.394649: meanmse:       0.034758594
2024-06-28 12:16:24.395688: meanr2:        0.7188620816685876
2024-06-28 12:16:24.396142: train_loss 1.1908
2024-06-28 12:16:24.396518: val_loss 1.0236
2024-06-28 12:16:24.396885: Pseudo dice [0.5]
2024-06-28 12:16:24.397275: Epoch time: 64.51 s
2024-06-28 12:16:24.397650: Yayy! New best R2: 0.7189
2024-06-28 12:16:27.076054: 
2024-06-28 12:16:27.076747: Epoch 7
2024-06-28 12:16:27.077221: Current learning rate: 0.00094
2024-06-28 12:17:31.509125: meanmse:       0.03793169
2024-06-28 12:17:31.509945: meanr2:        0.6903068160927567
2024-06-28 12:17:31.510355: train_loss 1.1143
2024-06-28 12:17:31.510686: val_loss 1.0982
2024-06-28 12:17:31.511018: Pseudo dice [0.5]
2024-06-28 12:17:31.511334: Epoch time: 64.44 s
2024-06-28 12:17:33.452642: 
2024-06-28 12:17:33.453389: Epoch 8
2024-06-28 12:17:33.454135: Current learning rate: 0.00093
2024-06-28 12:18:37.972713: meanmse:       0.035585478
2024-06-28 12:18:37.973681: meanr2:        0.7126214310908444
2024-06-28 12:18:37.974107: train_loss 1.0092
2024-06-28 12:18:37.974512: val_loss 0.9919
2024-06-28 12:18:37.974862: Pseudo dice [0.5]
2024-06-28 12:18:37.975206: Epoch time: 64.53 s
2024-06-28 12:18:39.959842: 
2024-06-28 12:18:39.960597: Epoch 9
2024-06-28 12:18:39.961061: Current learning rate: 0.00092
2024-06-28 12:19:44.187581: meanmse:       0.038201507
2024-06-28 12:19:44.188561: meanr2:        0.6867985166458095
2024-06-28 12:19:44.188988: train_loss 0.9568
2024-06-28 12:19:44.189338: val_loss 1.0198
2024-06-28 12:19:44.189656: Pseudo dice [0.5]
2024-06-28 12:19:44.189993: Epoch time: 64.24 s
2024-06-28 12:19:46.797357: 
2024-06-28 12:19:46.798161: Epoch 10
2024-06-28 12:19:46.798646: Current learning rate: 0.00091
2024-06-28 12:20:50.948776: meanmse:       0.03206377
2024-06-28 12:20:50.949885: meanr2:        0.7431606238493994
2024-06-28 12:20:50.950366: train_loss 1.0412
2024-06-28 12:20:50.950791: val_loss 0.9422
2024-06-28 12:20:50.951305: Pseudo dice [0.5]
2024-06-28 12:20:50.951801: Epoch time: 64.16 s
2024-06-28 12:20:50.952276: Yayy! New best R2: 0.7432
2024-06-28 12:20:53.107273: 
2024-06-28 12:20:53.107834: Epoch 11
2024-06-28 12:20:53.108214: Current learning rate: 0.0009
2024-06-28 12:21:57.307518: meanmse:       0.036558807
2024-06-28 12:21:57.308350: meanr2:        0.7046905185516613
2024-06-28 12:21:57.308744: train_loss 0.986
2024-06-28 12:21:57.309088: val_loss 1.0045
2024-06-28 12:21:57.309410: Pseudo dice [0.5]
2024-06-28 12:21:57.309764: Epoch time: 64.21 s
2024-06-28 12:21:59.149403: 
2024-06-28 12:21:59.150167: Epoch 12
2024-06-28 12:21:59.150658: Current learning rate: 0.00089
2024-06-28 12:23:03.402437: meanmse:       0.039159175
2024-06-28 12:23:03.403721: meanr2:        0.6829868084510612
2024-06-28 12:23:03.404250: train_loss 0.9607
2024-06-28 12:23:03.404634: val_loss 1.0453
2024-06-28 12:23:03.404983: Pseudo dice [0.5]
2024-06-28 12:23:03.405341: Epoch time: 64.26 s
2024-06-28 12:23:05.716988: 
2024-06-28 12:23:05.718871: Epoch 13
2024-06-28 12:23:05.725369: Current learning rate: 0.00088
2024-06-28 12:24:09.923450: meanmse:       0.03408018
2024-06-28 12:24:09.924425: meanr2:        0.7299431359157249
2024-06-28 12:24:09.924840: train_loss 0.9749
2024-06-28 12:24:09.925187: val_loss 0.9255
2024-06-28 12:24:09.925519: Pseudo dice [0.5]
2024-06-28 12:24:09.925943: Epoch time: 64.21 s
2024-06-28 12:24:11.976501: 
2024-06-28 12:24:11.977249: Epoch 14
2024-06-28 12:24:11.977671: Current learning rate: 0.00087
2024-06-28 12:25:16.477614: meanmse:       0.02964464
2024-06-28 12:25:16.478542: meanr2:        0.7569673020012094
2024-06-28 12:25:16.478955: train_loss 0.8638
2024-06-28 12:25:16.479299: val_loss 0.8642
2024-06-28 12:25:16.479624: Pseudo dice [0.5]
2024-06-28 12:25:16.479952: Epoch time: 64.51 s
2024-06-28 12:25:16.480307: Yayy! New best R2: 0.757
2024-06-28 12:25:18.844556: 
2024-06-28 12:25:18.847938: Epoch 15
2024-06-28 12:25:18.848670: Current learning rate: 0.00086
2024-06-28 12:26:22.703370: meanmse:       0.025549924
2024-06-28 12:26:22.704515: meanr2:        0.7941771465304999
2024-06-28 12:26:22.704972: train_loss 0.8728
2024-06-28 12:26:22.705319: val_loss 0.776
2024-06-28 12:26:22.705635: Pseudo dice [0.5]
2024-06-28 12:26:22.705984: Epoch time: 63.87 s
2024-06-28 12:26:22.706330: Yayy! New best R2: 0.7942
2024-06-28 12:26:24.918894: 
2024-06-28 12:26:24.919573: Epoch 16
2024-06-28 12:26:24.920135: Current learning rate: 0.00085
2024-06-28 12:27:29.325270: meanmse:       0.030309707
2024-06-28 12:27:29.327673: meanr2:        0.7542159027273464
2024-06-28 12:27:29.328424: train_loss 0.778
2024-06-28 12:27:29.328989: val_loss 0.8352
2024-06-28 12:27:29.329420: Pseudo dice [0.5]
2024-06-28 12:27:29.329927: Epoch time: 64.42 s
2024-06-28 12:27:31.335371: 
2024-06-28 12:27:31.336160: Epoch 17
2024-06-28 12:27:31.336850: Current learning rate: 0.00085
2024-06-28 12:28:35.971308: meanmse:       0.026270898
2024-06-28 12:28:35.972534: meanr2:        0.7868706159704033
2024-06-28 12:28:35.973006: train_loss 0.7778
2024-06-28 12:28:35.973432: val_loss 0.7822
2024-06-28 12:28:35.973799: Pseudo dice [0.5]
2024-06-28 12:28:35.974164: Epoch time: 64.65 s
2024-06-28 12:28:38.272797: 
2024-06-28 12:28:38.273613: Epoch 18
2024-06-28 12:28:38.274008: Current learning rate: 0.00084
2024-06-28 12:29:42.183761: meanmse:       0.02820304
2024-06-28 12:29:42.184823: meanr2:        0.7702154024373855
2024-06-28 12:29:42.185295: train_loss 0.7187
2024-06-28 12:29:42.185652: val_loss 0.818
2024-06-28 12:29:42.185989: Pseudo dice [0.5]
2024-06-28 12:29:42.186346: Epoch time: 63.92 s
2024-06-28 12:29:44.091465: 
2024-06-28 12:29:44.092097: Epoch 19
2024-06-28 12:29:44.092533: Current learning rate: 0.00083
2024-06-28 12:30:48.133935: meanmse:       0.021549098
2024-06-28 12:30:48.134807: meanr2:        0.8280825291170096
2024-06-28 12:30:48.135270: train_loss 0.757
2024-06-28 12:30:48.135626: val_loss 0.6685
2024-06-28 12:30:48.135964: Pseudo dice [0.5]
2024-06-28 12:30:48.136343: Epoch time: 64.05 s
2024-06-28 12:30:48.452796: Yayy! New best R2: 0.8281
2024-06-28 12:30:50.773259: 
2024-06-28 12:30:50.774027: Epoch 20
2024-06-28 12:30:50.774559: Current learning rate: 0.00082
2024-06-28 12:31:55.044733: meanmse:       0.026001018
2024-06-28 12:31:55.046019: meanr2:        0.789534378703192
2024-06-28 12:31:55.046750: train_loss 0.7803
2024-06-28 12:31:55.047257: val_loss 0.7734
2024-06-28 12:31:55.047708: Pseudo dice [0.5]
2024-06-28 12:31:55.048207: Epoch time: 64.28 s
2024-06-28 12:31:57.020922: 
2024-06-28 12:31:57.021780: Epoch 21
2024-06-28 12:31:57.022380: Current learning rate: 0.00081
2024-06-28 12:33:01.117966: meanmse:       0.023570197
2024-06-28 12:33:01.209990: meanr2:        0.8103323060886144
2024-06-28 12:33:01.210685: train_loss 0.7443
2024-06-28 12:33:01.211104: val_loss 0.7091
2024-06-28 12:33:01.211527: Pseudo dice [0.5]
2024-06-28 12:33:01.211944: Epoch time: 64.2 s
2024-06-28 12:33:03.175923: 
2024-06-28 12:33:03.176725: Epoch 22
2024-06-28 12:33:03.177297: Current learning rate: 0.0008
2024-06-28 12:34:07.017623: meanmse:       0.023397015
2024-06-28 12:34:07.018781: meanr2:        0.8088652440177843
2024-06-28 12:34:07.019323: train_loss 0.726
2024-06-28 12:34:07.019808: val_loss 0.711
2024-06-28 12:34:07.020287: Pseudo dice [0.5]
2024-06-28 12:34:07.020757: Epoch time: 63.85 s
2024-06-28 12:34:09.004726: 
2024-06-28 12:34:09.005528: Epoch 23
2024-06-28 12:34:09.007767: Current learning rate: 0.00079
2024-06-28 12:35:13.170354: meanmse:       0.022631735
2024-06-28 12:35:13.171587: meanr2:        0.8137006260222197
2024-06-28 12:35:13.172253: train_loss 0.732
2024-06-28 12:35:13.172757: val_loss 0.689
2024-06-28 12:35:13.173191: Pseudo dice [0.5]
2024-06-28 12:35:13.173660: Epoch time: 64.17 s
2024-06-28 12:35:15.126497: 
2024-06-28 12:35:15.127492: Epoch 24
2024-06-28 12:35:15.128084: Current learning rate: 0.00078
2024-06-28 12:36:19.412752: meanmse:       0.025252296
2024-06-28 12:36:19.413953: meanr2:        0.7956577022110657
2024-06-28 12:36:19.414515: train_loss 0.6884
2024-06-28 12:36:19.414943: val_loss 0.7409
2024-06-28 12:36:19.415286: Pseudo dice [0.5]
2024-06-28 12:36:19.415657: Epoch time: 64.3 s
2024-06-28 12:36:21.380222: 
2024-06-28 12:36:21.381050: Epoch 25
2024-06-28 12:36:21.381542: Current learning rate: 0.00077
2024-06-28 12:37:25.503559: meanmse:       0.024522739
2024-06-28 12:37:25.504744: meanr2:        0.8023890389434036
2024-06-28 12:37:25.505156: train_loss 0.6752
2024-06-28 12:37:25.505503: val_loss 0.7174
2024-06-28 12:37:25.505872: Pseudo dice [0.5]
2024-06-28 12:37:25.506268: Epoch time: 64.13 s
2024-06-28 12:37:27.780288: 
2024-06-28 12:37:27.781002: Epoch 26
2024-06-28 12:37:27.781410: Current learning rate: 0.00076
2024-06-28 12:38:32.109631: meanmse:       0.024193592
2024-06-28 12:38:32.110663: meanr2:        0.8040005904913539
2024-06-28 12:38:32.111145: train_loss 0.7432
2024-06-28 12:38:32.111543: val_loss 0.7168
2024-06-28 12:38:32.111992: Pseudo dice [0.5]
2024-06-28 12:38:32.112424: Epoch time: 64.34 s
2024-06-28 12:38:34.082078: 
2024-06-28 12:38:34.082935: Epoch 27
2024-06-28 12:38:34.083520: Current learning rate: 0.00075
2024-06-28 12:39:38.274694: meanmse:       0.023087896
2024-06-28 12:39:38.275604: meanr2:        0.8143181834708334
2024-06-28 12:39:38.276120: train_loss 0.7146
2024-06-28 12:39:38.276476: val_loss 0.7107
2024-06-28 12:39:38.276795: Pseudo dice [0.5]
2024-06-28 12:39:38.277150: Epoch time: 64.2 s
2024-06-28 12:39:40.115728: 
2024-06-28 12:39:40.116350: Epoch 28
2024-06-28 12:39:40.116759: Current learning rate: 0.00074
2024-06-28 12:40:44.678939: meanmse:       0.02106118
2024-06-28 12:40:44.679917: meanr2:        0.8298581376271812
2024-06-28 12:40:44.680405: train_loss 0.6277
2024-06-28 12:40:44.680752: val_loss 0.6527
2024-06-28 12:40:44.681051: Pseudo dice [0.5]
2024-06-28 12:40:44.681365: Epoch time: 64.57 s
2024-06-28 12:40:44.681658: Yayy! New best R2: 0.8299
2024-06-28 12:40:47.201846: 
2024-06-28 12:40:47.202529: Epoch 29
2024-06-28 12:40:47.202964: Current learning rate: 0.00073
2024-06-28 12:41:51.173547: meanmse:       0.021583013
2024-06-28 12:41:51.174653: meanr2:        0.828678147939364
2024-06-28 12:41:51.175133: train_loss 0.6481
2024-06-28 12:41:51.175549: val_loss 0.6529
2024-06-28 12:41:51.175951: Pseudo dice [0.5]
2024-06-28 12:41:51.176377: Epoch time: 63.98 s
2024-06-28 12:41:53.457132: 
2024-06-28 12:41:53.457861: Epoch 30
2024-06-28 12:41:53.458323: Current learning rate: 0.00073
2024-06-28 12:42:57.892215: meanmse:       0.018801538
2024-06-28 12:42:57.893073: meanr2:        0.8498829927987255
2024-06-28 12:42:57.893517: train_loss 0.6569
2024-06-28 12:42:57.893879: val_loss 0.6424
2024-06-28 12:42:57.894182: Pseudo dice [0.5]
2024-06-28 12:42:57.894496: Epoch time: 64.45 s
2024-06-28 12:42:57.894818: Yayy! New best R2: 0.8499
2024-06-28 12:43:00.172932: 
2024-06-28 12:43:00.173797: Epoch 31
2024-06-28 12:43:00.174317: Current learning rate: 0.00072
2024-06-28 12:44:04.354570: meanmse:       0.017944353
2024-06-28 12:44:04.355827: meanr2:        0.8540403299144089
2024-06-28 12:44:04.356397: train_loss 0.6094
2024-06-28 12:44:04.356794: val_loss 0.5728
2024-06-28 12:44:04.357176: Pseudo dice [0.5]
2024-06-28 12:44:04.357574: Epoch time: 64.19 s
2024-06-28 12:44:04.357999: Yayy! New best R2: 0.854
2024-06-28 12:44:06.582742: 
2024-06-28 12:44:06.583526: Epoch 32
2024-06-28 12:44:06.584029: Current learning rate: 0.00071
2024-06-28 12:45:10.897106: meanmse:       0.018141493
2024-06-28 12:45:10.898199: meanr2:        0.850635444053285
2024-06-28 12:45:10.898752: train_loss 0.6131
2024-06-28 12:45:10.899232: val_loss 0.5648
2024-06-28 12:45:10.899661: Pseudo dice [0.5]
2024-06-28 12:45:10.900072: Epoch time: 64.32 s
2024-06-28 12:45:12.967457: 
2024-06-28 12:45:12.968335: Epoch 33
2024-06-28 12:45:12.968879: Current learning rate: 0.0007
2024-06-28 12:46:17.184902: meanmse:       0.022340404
2024-06-28 12:46:17.185916: meanr2:        0.8161562082826266
2024-06-28 12:46:17.186404: train_loss 0.6352
2024-06-28 12:46:17.186839: val_loss 0.6597
2024-06-28 12:46:17.187186: Pseudo dice [0.5]
2024-06-28 12:46:17.187560: Epoch time: 64.23 s
2024-06-28 12:46:19.115839: 
2024-06-28 12:46:19.116948: Epoch 34
2024-06-28 12:46:19.117550: Current learning rate: 0.00069
2024-06-28 12:47:23.435500: meanmse:       0.017978357
2024-06-28 12:47:23.436455: meanr2:        0.8535075712411188
2024-06-28 12:47:23.436925: train_loss 0.5855
2024-06-28 12:47:23.437294: val_loss 0.5791
2024-06-28 12:47:23.437651: Pseudo dice [0.5]
2024-06-28 12:47:23.438049: Epoch time: 64.33 s
2024-06-28 12:47:25.575943: 
2024-06-28 12:47:25.580062: Epoch 35
2024-06-28 12:47:25.580991: Current learning rate: 0.00068
2024-06-28 12:48:30.014949: meanmse:       0.024859184
2024-06-28 12:48:30.015866: meanr2:        0.7999374450264315
2024-06-28 12:48:30.016312: train_loss 0.6336
2024-06-28 12:48:30.016667: val_loss 0.6851
2024-06-28 12:48:30.017064: Pseudo dice [0.5]
2024-06-28 12:48:30.017530: Epoch time: 64.45 s
2024-06-28 12:48:31.981294: 
2024-06-28 12:48:31.982386: Epoch 36
2024-06-28 12:48:31.983055: Current learning rate: 0.00067
2024-06-28 12:49:36.314403: meanmse:       0.02477751
2024-06-28 12:49:36.315579: meanr2:        0.7936205289116365
2024-06-28 12:49:36.316093: train_loss 0.5839
2024-06-28 12:49:36.316536: val_loss 0.6994
2024-06-28 12:49:36.316973: Pseudo dice [0.5]
2024-06-28 12:49:36.317556: Epoch time: 64.34 s
2024-06-28 12:49:38.230514: 
2024-06-28 12:49:38.231307: Epoch 37
2024-06-28 12:49:38.232018: Current learning rate: 0.00066
2024-06-28 12:50:42.361497: meanmse:       0.022768049
2024-06-28 12:50:42.362863: meanr2:        0.8146539689788745
2024-06-28 12:50:42.363488: train_loss 0.5473
2024-06-28 12:50:42.364084: val_loss 0.6722
2024-06-28 12:50:42.364517: Pseudo dice [0.5]
2024-06-28 12:50:42.365041: Epoch time: 64.14 s
2024-06-28 12:50:44.526408: 
2024-06-28 12:50:44.527279: Epoch 38
2024-06-28 12:50:44.527788: Current learning rate: 0.00065
2024-06-28 12:51:48.751102: meanmse:       0.022377998
2024-06-28 12:51:48.752791: meanr2:        0.8231620610545242
2024-06-28 12:51:48.753648: train_loss 0.5849
2024-06-28 12:51:48.754235: val_loss 0.6314
2024-06-28 12:51:48.754967: Pseudo dice [0.5]
2024-06-28 12:51:48.755647: Epoch time: 64.23 s
2024-06-28 12:51:50.662449: 
2024-06-28 12:51:50.663321: Epoch 39
2024-06-28 12:51:50.663835: Current learning rate: 0.00064
2024-06-28 12:52:54.745651: meanmse:       0.032664925
2024-06-28 12:52:54.747163: meanr2:        0.7345691182718603
2024-06-28 12:52:54.747735: train_loss 0.5455
2024-06-28 12:52:54.748182: val_loss 0.8013
2024-06-28 12:52:54.748592: Pseudo dice [0.5]
2024-06-28 12:52:54.749043: Epoch time: 64.09 s
2024-06-28 12:52:57.304130: 
2024-06-28 12:52:57.305025: Epoch 40
2024-06-28 12:52:57.305632: Current learning rate: 0.00063
2024-06-28 12:54:01.441546: meanmse:       0.022758603
2024-06-28 12:54:01.448258: meanr2:        0.8163991305178243
2024-06-28 12:54:01.449131: train_loss 0.5765
2024-06-28 12:54:01.449698: val_loss 0.6522
2024-06-28 12:54:01.450207: Pseudo dice [0.5]
2024-06-28 12:54:01.450722: Epoch time: 64.15 s
2024-06-28 12:54:03.800704: 
2024-06-28 12:54:03.801296: Epoch 41
2024-06-28 12:54:03.801707: Current learning rate: 0.00062
2024-06-28 12:55:07.877666: meanmse:       0.020431159
2024-06-28 12:55:07.878569: meanr2:        0.8347559572532909
2024-06-28 12:55:07.879038: train_loss 0.5827
2024-06-28 12:55:07.879362: val_loss 0.6197
2024-06-28 12:55:07.879692: Pseudo dice [0.5]
2024-06-28 12:55:07.880040: Epoch time: 64.09 s
2024-06-28 12:55:09.769520: 
2024-06-28 12:55:09.770371: Epoch 42
2024-06-28 12:55:09.770892: Current learning rate: 0.00061
2024-06-28 12:56:13.829031: meanmse:       0.022032205
2024-06-28 12:56:13.830287: meanr2:        0.8182478012132357
2024-06-28 12:56:13.830914: train_loss 0.596
2024-06-28 12:56:13.831435: val_loss 0.6452
2024-06-28 12:56:13.831941: Pseudo dice [0.5]
2024-06-28 12:56:13.832472: Epoch time: 64.07 s
2024-06-28 12:56:15.719182: 
2024-06-28 12:56:15.725952: Epoch 43
2024-06-28 12:56:15.726790: Current learning rate: 0.0006
2024-06-28 12:57:19.791585: meanmse:       0.020591341
2024-06-28 12:57:19.792536: meanr2:        0.8358495500845479
2024-06-28 12:57:19.793011: train_loss 0.598
2024-06-28 12:57:19.793376: val_loss 0.6002
2024-06-28 12:57:19.793711: Pseudo dice [0.5]
2024-06-28 12:57:19.794061: Epoch time: 64.08 s
2024-06-28 12:57:21.697564: 
2024-06-28 12:57:21.698274: Epoch 44
2024-06-28 12:57:21.698750: Current learning rate: 0.00059
2024-06-28 12:58:26.029615: meanmse:       0.023097806
2024-06-28 12:58:26.031144: meanr2:        0.8102406942824807
2024-06-28 12:58:26.033541: train_loss 0.5294
2024-06-28 12:58:26.037865: val_loss 0.6435
2024-06-28 12:58:26.038611: Pseudo dice [0.5]
2024-06-28 12:58:26.039191: Epoch time: 64.34 s
2024-06-28 12:58:28.100856: 
2024-06-28 12:58:28.101471: Epoch 45
2024-06-28 12:58:28.101898: Current learning rate: 0.00058
2024-06-28 12:59:32.830933: meanmse:       0.017558033
2024-06-28 12:59:32.832102: meanr2:        0.855438852587189
2024-06-28 12:59:32.832810: train_loss 0.5314
2024-06-28 12:59:32.833503: val_loss 0.5322
2024-06-28 12:59:32.834205: Pseudo dice [0.5]
2024-06-28 12:59:32.834712: Epoch time: 64.74 s
2024-06-28 12:59:32.835176: Yayy! New best R2: 0.8554
2024-06-28 12:59:35.077179: 
2024-06-28 12:59:35.078003: Epoch 46
2024-06-28 12:59:35.078587: Current learning rate: 0.00057
2024-06-28 13:00:39.389906: meanmse:       0.029020967
2024-06-28 13:00:39.392111: meanr2:        0.7688160432131287
2024-06-28 13:00:39.392718: train_loss 0.5287
2024-06-28 13:00:39.393165: val_loss 0.7637
2024-06-28 13:00:39.393619: Pseudo dice [0.5]
2024-06-28 13:00:39.394095: Epoch time: 64.32 s
2024-06-28 13:00:41.555365: 
2024-06-28 13:00:41.556433: Epoch 47
2024-06-28 13:00:41.557277: Current learning rate: 0.00056
2024-06-28 13:01:45.808137: meanmse:       0.017833304
2024-06-28 13:01:45.809114: meanr2:        0.8580302501906563
2024-06-28 13:01:45.809541: train_loss 0.5414
2024-06-28 13:01:45.809944: val_loss 0.5507
2024-06-28 13:01:45.810329: Pseudo dice [0.5]
2024-06-28 13:01:45.810694: Epoch time: 64.26 s
2024-06-28 13:01:45.811042: Yayy! New best R2: 0.858
2024-06-28 13:01:47.887973: 
2024-06-28 13:01:47.888550: Epoch 48
2024-06-28 13:01:47.889024: Current learning rate: 0.00056
2024-06-28 13:02:52.134022: meanmse:       0.019531121
2024-06-28 13:02:52.135159: meanr2:        0.8411818078191561
2024-06-28 13:02:52.137593: train_loss 0.5307
2024-06-28 13:02:52.138022: val_loss 0.5996
2024-06-28 13:02:52.138432: Pseudo dice [0.5]
2024-06-28 13:02:52.138819: Epoch time: 64.26 s
2024-06-28 13:02:54.172615: 
2024-06-28 13:02:54.173256: Epoch 49
2024-06-28 13:02:54.173877: Current learning rate: 0.00055
2024-06-28 13:03:58.534062: meanmse:       0.018710766
2024-06-28 13:03:58.540625: meanr2:        0.846790305387593
2024-06-28 13:03:58.541248: train_loss 0.5673
2024-06-28 13:03:58.541635: val_loss 0.5792
2024-06-28 13:03:58.542106: Pseudo dice [0.5]
2024-06-28 13:03:58.542507: Epoch time: 64.38 s
2024-06-28 13:04:01.147894: 
2024-06-28 13:04:01.149033: Epoch 50
2024-06-28 13:04:01.149814: Current learning rate: 0.00054
2024-06-28 13:05:05.283800: meanmse:       0.023251634
2024-06-28 13:05:05.285000: meanr2:        0.8125796955045491
2024-06-28 13:05:05.285570: train_loss 0.5192
2024-06-28 13:05:05.286006: val_loss 0.655
2024-06-28 13:05:05.286411: Pseudo dice [0.5]
2024-06-28 13:05:05.286835: Epoch time: 64.15 s
2024-06-28 13:05:07.246176: 
2024-06-28 13:05:07.246957: Epoch 51
2024-06-28 13:05:07.247507: Current learning rate: 0.00053
2024-06-28 13:06:11.566689: meanmse:       0.01847782
2024-06-28 13:06:11.567948: meanr2:        0.8496804434744618
2024-06-28 13:06:11.568676: train_loss 0.516
2024-06-28 13:06:11.569206: val_loss 0.5519
2024-06-28 13:06:11.569727: Pseudo dice [0.5]
2024-06-28 13:06:11.570172: Epoch time: 64.33 s
2024-06-28 13:06:13.651350: 
2024-06-28 13:06:13.652307: Epoch 52
2024-06-28 13:06:13.652946: Current learning rate: 0.00052
2024-06-28 13:07:17.988636: meanmse:       0.020063147
2024-06-28 13:07:17.989670: meanr2:        0.8412928472871734
2024-06-28 13:07:17.990164: train_loss 0.5234
2024-06-28 13:07:17.990588: val_loss 0.5969
2024-06-28 13:07:17.990930: Pseudo dice [0.5]
2024-06-28 13:07:17.991305: Epoch time: 64.35 s
2024-06-28 13:07:19.858949: 
2024-06-28 13:07:19.859578: Epoch 53
2024-06-28 13:07:19.860017: Current learning rate: 0.00051
2024-06-28 13:08:24.093000: meanmse:       0.020403098
2024-06-28 13:08:24.093976: meanr2:        0.8331661458525541
2024-06-28 13:08:24.094409: train_loss 0.5246
2024-06-28 13:08:24.094730: val_loss 0.5964
2024-06-28 13:08:24.095102: Pseudo dice [0.5]
2024-06-28 13:08:24.095436: Epoch time: 64.24 s
2024-06-28 13:08:26.165226: 
2024-06-28 13:08:26.166102: Epoch 54
2024-06-28 13:08:26.166666: Current learning rate: 0.0005
2024-06-28 13:09:30.554672: meanmse:       0.02088157
2024-06-28 13:09:30.555776: meanr2:        0.8333371340773112
2024-06-28 13:09:30.556275: train_loss 0.5205
2024-06-28 13:09:30.556606: val_loss 0.622
2024-06-28 13:09:30.556948: Pseudo dice [0.5]
2024-06-28 13:09:30.557349: Epoch time: 64.4 s
2024-06-28 13:09:32.394554: 
2024-06-28 13:09:32.395322: Epoch 55
2024-06-28 13:09:32.395850: Current learning rate: 0.00049
2024-06-28 13:10:36.996782: meanmse:       0.019529201
2024-06-28 13:10:36.997777: meanr2:        0.8426941687423171
2024-06-28 13:10:36.998223: train_loss 0.5144
2024-06-28 13:10:36.998578: val_loss 0.5759
2024-06-28 13:10:36.998914: Pseudo dice [0.5]
2024-06-28 13:10:36.999288: Epoch time: 64.61 s
2024-06-28 13:10:39.061654: 
2024-06-28 13:10:39.062744: Epoch 56
2024-06-28 13:10:39.063224: Current learning rate: 0.00048
2024-06-28 13:11:43.174169: meanmse:       0.027931685
2024-06-28 13:11:43.175342: meanr2:        0.7734820734827622
2024-06-28 13:11:43.175853: train_loss 0.5013
2024-06-28 13:11:43.176252: val_loss 0.7312
2024-06-28 13:11:43.176706: Pseudo dice [0.5]
2024-06-28 13:11:43.177176: Epoch time: 64.12 s
2024-06-28 13:11:45.138717: 
2024-06-28 13:11:45.139303: Epoch 57
2024-06-28 13:11:45.139676: Current learning rate: 0.00047
2024-06-28 13:12:49.837682: meanmse:       0.01859187
2024-06-28 13:12:49.838960: meanr2:        0.8501578154002254
2024-06-28 13:12:49.839519: train_loss 0.5281
2024-06-28 13:12:49.840016: val_loss 0.565
2024-06-28 13:12:49.840461: Pseudo dice [0.5]
2024-06-28 13:12:49.840882: Epoch time: 64.71 s
2024-06-28 13:12:51.700786: 
2024-06-28 13:12:51.701436: Epoch 58
2024-06-28 13:12:51.701814: Current learning rate: 0.00046
2024-06-28 13:13:56.159636: meanmse:       0.016420824
2024-06-28 13:13:56.161191: meanr2:        0.8668879410759739
2024-06-28 13:13:56.162032: train_loss 0.475
2024-06-28 13:13:56.162621: val_loss 0.5138
2024-06-28 13:13:56.163065: Pseudo dice [0.5]
2024-06-28 13:13:56.163525: Epoch time: 64.47 s
2024-06-28 13:13:56.164046: Yayy! New best R2: 0.8669
2024-06-28 13:13:58.622122: 
2024-06-28 13:13:58.622790: Epoch 59
2024-06-28 13:13:58.625140: Current learning rate: 0.00045
2024-06-28 13:15:02.887719: meanmse:       0.016676534
2024-06-28 13:15:02.888893: meanr2:        0.8635616839773396
2024-06-28 13:15:02.889379: train_loss 0.4642
2024-06-28 13:15:02.889774: val_loss 0.5226
2024-06-28 13:15:02.890212: Pseudo dice [0.5]
2024-06-28 13:15:02.890623: Epoch time: 64.27 s
2024-06-28 13:15:05.067311: 
2024-06-28 13:15:05.067904: Epoch 60
2024-06-28 13:15:05.068365: Current learning rate: 0.00044
2024-06-28 13:16:09.299565: meanmse:       0.019665353
2024-06-28 13:16:09.300489: meanr2:        0.841516507032686
2024-06-28 13:16:09.300923: train_loss 0.4777
2024-06-28 13:16:09.301247: val_loss 0.573
2024-06-28 13:16:09.301578: Pseudo dice [0.5]
2024-06-28 13:16:09.301940: Epoch time: 64.24 s
2024-06-28 13:16:11.198193: 
2024-06-28 13:16:11.198885: Epoch 61
2024-06-28 13:16:11.199409: Current learning rate: 0.00043
2024-06-28 13:17:15.522106: meanmse:       0.014267843
2024-06-28 13:17:15.530735: meanr2:        0.8803695135591666
2024-06-28 13:17:15.531538: train_loss 0.4576
2024-06-28 13:17:15.532265: val_loss 0.4737
2024-06-28 13:17:15.532793: Pseudo dice [0.5]
2024-06-28 13:17:15.533292: Epoch time: 64.34 s
2024-06-28 13:17:15.533794: Yayy! New best R2: 0.8804
2024-06-28 13:17:17.791045: 
2024-06-28 13:17:17.791857: Epoch 62
2024-06-28 13:17:17.792323: Current learning rate: 0.00042
2024-06-28 13:18:22.079365: meanmse:       0.02390799
2024-06-28 13:18:22.080323: meanr2:        0.8112727479412858
2024-06-28 13:18:22.080753: train_loss 0.4283
2024-06-28 13:18:22.081139: val_loss 0.6375
2024-06-28 13:18:22.081515: Pseudo dice [0.5]
2024-06-28 13:18:22.081867: Epoch time: 64.3 s
2024-06-28 13:18:24.057375: 
2024-06-28 13:18:24.058169: Epoch 63
2024-06-28 13:18:24.058641: Current learning rate: 0.00041
2024-06-28 13:19:28.400176: meanmse:       0.013803587
2024-06-28 13:19:28.401230: meanr2:        0.8887625767057578
2024-06-28 13:19:28.401733: train_loss 0.4771
2024-06-28 13:19:28.402189: val_loss 0.463
2024-06-28 13:19:28.402646: Pseudo dice [0.5]
2024-06-28 13:19:28.403111: Epoch time: 64.35 s
2024-06-28 13:19:28.403578: Yayy! New best R2: 0.8888
2024-06-28 13:19:31.082962: 
2024-06-28 13:19:31.083549: Epoch 64
2024-06-28 13:19:31.083973: Current learning rate: 0.0004
2024-06-28 13:20:35.268305: meanmse:       0.01788364
2024-06-28 13:20:35.269580: meanr2:        0.8548265421036412
2024-06-28 13:20:35.270250: train_loss 0.4713
2024-06-28 13:20:35.270729: val_loss 0.5393
2024-06-28 13:20:35.271130: Pseudo dice [0.5]
2024-06-28 13:20:35.271575: Epoch time: 64.19 s
2024-06-28 13:20:37.191451: 
2024-06-28 13:20:37.192325: Epoch 65
2024-06-28 13:20:37.192878: Current learning rate: 0.00039
2024-06-28 13:21:41.391833: meanmse:       0.01444805
2024-06-28 13:21:41.393054: meanr2:        0.8832418653034471
2024-06-28 13:21:41.393608: train_loss 0.4959
2024-06-28 13:21:41.394071: val_loss 0.4841
2024-06-28 13:21:41.394495: Pseudo dice [0.5]
2024-06-28 13:21:41.394900: Epoch time: 64.21 s
2024-06-28 13:21:43.307397: 
2024-06-28 13:21:43.308173: Epoch 66
2024-06-28 13:21:43.308664: Current learning rate: 0.00038
2024-06-28 13:22:47.288641: meanmse:       0.01839834
2024-06-28 13:22:47.289866: meanr2:        0.851470033814773
2024-06-28 13:22:47.290419: train_loss 0.4689
2024-06-28 13:22:47.290925: val_loss 0.5498
2024-06-28 13:22:47.291413: Pseudo dice [0.5]
2024-06-28 13:22:47.291869: Epoch time: 63.99 s
2024-06-28 13:22:49.348578: 
2024-06-28 13:22:49.351552: Epoch 67
2024-06-28 13:22:49.352067: Current learning rate: 0.00037
2024-06-28 13:23:53.692900: meanmse:       0.017186383
2024-06-28 13:23:53.694078: meanr2:        0.8626800378892393
2024-06-28 13:23:53.694638: train_loss 0.4516
2024-06-28 13:23:53.695096: val_loss 0.5181
2024-06-28 13:23:53.695530: Pseudo dice [0.5]
2024-06-28 13:23:53.695973: Epoch time: 64.36 s
2024-06-28 13:23:55.918512: 
2024-06-28 13:23:55.920254: Epoch 68
2024-06-28 13:23:55.920938: Current learning rate: 0.00036
2024-06-28 13:24:59.994782: meanmse:       0.026365457
2024-06-28 13:24:59.996102: meanr2:        0.7909581342701599
2024-06-28 13:24:59.996738: train_loss 0.4566
2024-06-28 13:24:59.997296: val_loss 0.6495
2024-06-28 13:24:59.997855: Pseudo dice [0.5]
2024-06-28 13:24:59.998412: Epoch time: 64.08 s
2024-06-28 13:25:02.119310: 
2024-06-28 13:25:02.120112: Epoch 69
2024-06-28 13:25:02.120683: Current learning rate: 0.00035
2024-06-28 13:26:06.389115: meanmse:       0.016260779
2024-06-28 13:26:06.390133: meanr2:        0.8680261864578551
2024-06-28 13:26:06.390501: train_loss 0.4659
2024-06-28 13:26:06.390805: val_loss 0.4966
2024-06-28 13:26:06.391129: Pseudo dice [0.5]
2024-06-28 13:26:06.391449: Epoch time: 64.28 s
2024-06-28 13:26:08.582876: 
2024-06-28 13:26:08.583429: Epoch 70
2024-06-28 13:26:08.583835: Current learning rate: 0.00034
2024-06-28 13:27:13.047784: meanmse:       0.012574825
2024-06-28 13:27:13.049106: meanr2:        0.89908326558165
2024-06-28 13:27:13.049796: train_loss 0.4251
2024-06-28 13:27:13.050258: val_loss 0.4219
2024-06-28 13:27:13.050706: Pseudo dice [0.5]
2024-06-28 13:27:13.051217: Epoch time: 64.47 s
2024-06-28 13:27:13.051835: Yayy! New best R2: 0.8991
2024-06-28 13:27:15.385032: 
2024-06-28 13:27:15.385859: Epoch 71
2024-06-28 13:27:15.386323: Current learning rate: 0.00033
2024-06-28 13:28:19.382521: meanmse:       0.017793672
2024-06-28 13:28:19.383965: meanr2:        0.8573763941758089
2024-06-28 13:28:19.384761: train_loss 0.4449
2024-06-28 13:28:19.385363: val_loss 0.5372
2024-06-28 13:28:19.385874: Pseudo dice [0.5]
2024-06-28 13:28:19.386402: Epoch time: 64.01 s
2024-06-28 13:28:21.669381: 
2024-06-28 13:28:21.670042: Epoch 72
2024-06-28 13:28:21.670652: Current learning rate: 0.00032
2024-06-28 13:29:26.141485: meanmse:       0.016153423
2024-06-28 13:29:26.142581: meanr2:        0.8675341728922631
2024-06-28 13:29:26.143089: train_loss 0.4565
2024-06-28 13:29:26.143483: val_loss 0.5055
2024-06-28 13:29:26.143862: Pseudo dice [0.5]
2024-06-28 13:29:26.144274: Epoch time: 64.48 s
2024-06-28 13:29:28.140398: 
2024-06-28 13:29:28.141258: Epoch 73
2024-06-28 13:29:28.141807: Current learning rate: 0.00031
2024-06-28 13:30:32.085506: meanmse:       0.018528305
2024-06-28 13:30:32.086415: meanr2:        0.8488271422462753
2024-06-28 13:30:32.086834: train_loss 0.443
2024-06-28 13:30:32.087187: val_loss 0.5385
2024-06-28 13:30:32.087528: Pseudo dice [0.5]
2024-06-28 13:30:32.087888: Epoch time: 63.95 s
2024-06-28 13:30:34.121519: 
2024-06-28 13:30:34.122269: Epoch 74
2024-06-28 13:30:34.122830: Current learning rate: 0.0003
2024-06-28 13:31:38.352019: meanmse:       0.019309051
2024-06-28 13:31:38.353529: meanr2:        0.8470768356713112
2024-06-28 13:31:38.354174: train_loss 0.4382
2024-06-28 13:31:38.354594: val_loss 0.55
2024-06-28 13:31:38.355525: Pseudo dice [0.5]
2024-06-28 13:31:38.356170: Epoch time: 64.24 s
2024-06-28 13:31:40.564227: 
2024-06-28 13:31:40.565024: Epoch 75
2024-06-28 13:31:40.565505: Current learning rate: 0.00029
2024-06-28 13:32:44.888242: meanmse:       0.017428057
2024-06-28 13:32:44.889200: meanr2:        0.8579868857943255
2024-06-28 13:32:44.889670: train_loss 0.4152
2024-06-28 13:32:44.890064: val_loss 0.5258
2024-06-28 13:32:44.890453: Pseudo dice [0.5]
2024-06-28 13:32:44.890881: Epoch time: 64.33 s
2024-06-28 13:32:46.877796: 
2024-06-28 13:32:46.878520: Epoch 76
2024-06-28 13:32:46.878975: Current learning rate: 0.00028
2024-06-28 13:33:51.396591: meanmse:       0.021161865
2024-06-28 13:33:51.397610: meanr2:        0.827905116194589
2024-06-28 13:33:51.398041: train_loss 0.4163
2024-06-28 13:33:51.398390: val_loss 0.5911
2024-06-28 13:33:51.398730: Pseudo dice [0.5]
2024-06-28 13:33:51.399090: Epoch time: 64.53 s
2024-06-28 13:33:53.347617: 
2024-06-28 13:33:53.348339: Epoch 77
2024-06-28 13:33:53.348783: Current learning rate: 0.00027
2024-06-28 13:34:57.684587: meanmse:       0.01980662
2024-06-28 13:34:57.685602: meanr2:        0.8414675849674449
2024-06-28 13:34:57.686072: train_loss 0.4222
2024-06-28 13:34:57.686448: val_loss 0.5672
2024-06-28 13:34:57.686859: Pseudo dice [0.5]
2024-06-28 13:34:57.687242: Epoch time: 64.35 s
2024-06-28 13:34:59.577084: 
2024-06-28 13:34:59.577881: Epoch 78
2024-06-28 13:34:59.578370: Current learning rate: 0.00026
2024-06-28 13:36:04.143499: meanmse:       0.018298225
2024-06-28 13:36:04.146602: meanr2:        0.8513008825564076
2024-06-28 13:36:04.147148: train_loss 0.3844
2024-06-28 13:36:04.147494: val_loss 0.5471
2024-06-28 13:36:04.147822: Pseudo dice [0.5]
2024-06-28 13:36:04.148179: Epoch time: 64.58 s
2024-06-28 13:36:06.134191: 
2024-06-28 13:36:06.135221: Epoch 79
2024-06-28 13:36:06.136216: Current learning rate: 0.00025
2024-06-28 13:37:10.280891: meanmse:       0.020628467
2024-06-28 13:37:10.282872: meanr2:        0.8325271821161961
2024-06-28 13:37:10.283271: train_loss 0.4131
2024-06-28 13:37:10.283610: val_loss 0.5964
2024-06-28 13:37:10.283942: Pseudo dice [0.5]
2024-06-28 13:37:10.284300: Epoch time: 64.17 s
2024-06-28 13:37:12.581280: 
2024-06-28 13:37:12.581820: Epoch 80
2024-06-28 13:37:12.582221: Current learning rate: 0.00023
2024-06-28 13:38:16.984637: meanmse:       0.022371449
2024-06-28 13:38:16.985722: meanr2:        0.8195442706235316
2024-06-28 13:38:16.986252: train_loss 0.3909
2024-06-28 13:38:16.986706: val_loss 0.6209
2024-06-28 13:38:16.987104: Pseudo dice [0.5]
2024-06-28 13:38:16.987494: Epoch time: 64.41 s
2024-06-28 13:38:18.964392: 
2024-06-28 13:38:18.965051: Epoch 81
2024-06-28 13:38:18.965480: Current learning rate: 0.00022
2024-06-28 13:39:23.302499: meanmse:       0.0146692125
2024-06-28 13:39:23.303352: meanr2:        0.8791003745837341
2024-06-28 13:39:23.303741: train_loss 0.3868
2024-06-28 13:39:23.304055: val_loss 0.4559
2024-06-28 13:39:23.304357: Pseudo dice [0.5]
2024-06-28 13:39:23.304671: Epoch time: 64.35 s
2024-06-28 13:39:25.374146: 
2024-06-28 13:39:25.374846: Epoch 82
2024-06-28 13:39:25.375310: Current learning rate: 0.00021
2024-06-28 13:40:29.720340: meanmse:       0.01851322
2024-06-28 13:40:29.721842: meanr2:        0.8546477751125562
2024-06-28 13:40:29.722463: train_loss 0.4002
2024-06-28 13:40:29.722931: val_loss 0.5369
2024-06-28 13:40:29.723398: Pseudo dice [0.5]
2024-06-28 13:40:29.723950: Epoch time: 64.35 s
2024-06-28 13:40:31.744284: 
2024-06-28 13:40:31.745081: Epoch 83
2024-06-28 13:40:31.745589: Current learning rate: 0.0002
2024-06-28 13:41:35.988863: meanmse:       0.012724101
2024-06-28 13:41:35.989703: meanr2:        0.8958525918869232
2024-06-28 13:41:35.990116: train_loss 0.3882
2024-06-28 13:41:35.990463: val_loss 0.438
2024-06-28 13:41:35.990789: Pseudo dice [0.5]
2024-06-28 13:41:35.991104: Epoch time: 64.25 s
2024-06-28 13:41:37.909843: 
2024-06-28 13:41:37.910480: Epoch 84
2024-06-28 13:41:37.910954: Current learning rate: 0.00019
2024-06-28 13:42:42.281610: meanmse:       0.016735468
2024-06-28 13:42:42.282852: meanr2:        0.8655285582226817
2024-06-28 13:42:42.283397: train_loss 0.3886
2024-06-28 13:42:42.283892: val_loss 0.514
2024-06-28 13:42:42.284337: Pseudo dice [0.5]
2024-06-28 13:42:42.284770: Epoch time: 64.38 s
2024-06-28 13:42:44.219325: 
2024-06-28 13:42:44.220021: Epoch 85
2024-06-28 13:42:44.220476: Current learning rate: 0.00018
2024-06-28 13:43:48.427559: meanmse:       0.02329777
2024-06-28 13:43:48.428906: meanr2:        0.8119088449425231
2024-06-28 13:43:48.429453: train_loss 0.3879
2024-06-28 13:43:48.429962: val_loss 0.6085
2024-06-28 13:43:48.430396: Pseudo dice [0.5]
2024-06-28 13:43:48.430845: Epoch time: 64.22 s
2024-06-28 13:43:50.354823: 
2024-06-28 13:43:50.355527: Epoch 86
2024-06-28 13:43:50.355992: Current learning rate: 0.00017
2024-06-28 13:44:54.568249: meanmse:       0.019339936
2024-06-28 13:44:54.569267: meanr2:        0.8421564807572556
2024-06-28 13:44:54.569722: train_loss 0.3981
2024-06-28 13:44:54.570148: val_loss 0.5471
2024-06-28 13:44:54.570481: Pseudo dice [0.5]
2024-06-28 13:44:54.570876: Epoch time: 64.22 s
2024-06-28 13:44:56.752727: 
2024-06-28 13:44:56.753745: Epoch 87
2024-06-28 13:44:56.754453: Current learning rate: 0.00016
2024-06-28 13:46:00.929850: meanmse:       0.014830392
2024-06-28 13:46:00.931205: meanr2:        0.8809327586895855
2024-06-28 13:46:00.931866: train_loss 0.4099
2024-06-28 13:46:00.932394: val_loss 0.4746
2024-06-28 13:46:00.932893: Pseudo dice [0.5]
2024-06-28 13:46:00.933368: Epoch time: 64.19 s
2024-06-28 13:46:02.772053: 
2024-06-28 13:46:02.772753: Epoch 88
2024-06-28 13:46:02.773261: Current learning rate: 0.00015
2024-06-28 13:47:07.222688: meanmse:       0.01987504
2024-06-28 13:47:07.230554: meanr2:        0.8389677427837252
2024-06-28 13:47:07.231141: train_loss 0.3924
2024-06-28 13:47:07.231527: val_loss 0.565
2024-06-28 13:47:07.231929: Pseudo dice [0.5]
2024-06-28 13:47:07.232324: Epoch time: 64.47 s
2024-06-28 13:47:09.063799: 
2024-06-28 13:47:09.064761: Epoch 89
2024-06-28 13:47:09.065405: Current learning rate: 0.00014
2024-06-28 13:48:13.396240: meanmse:       0.017893402
2024-06-28 13:48:13.397327: meanr2:        0.8542161756520126
2024-06-28 13:48:13.397846: train_loss 0.4012
2024-06-28 13:48:13.398305: val_loss 0.524
2024-06-28 13:48:13.398731: Pseudo dice [0.5]
2024-06-28 13:48:13.399184: Epoch time: 64.34 s
2024-06-28 13:48:15.613272: 
2024-06-28 13:48:15.614026: Epoch 90
2024-06-28 13:48:15.614532: Current learning rate: 0.00013
2024-06-28 13:49:19.890064: meanmse:       0.017480677
2024-06-28 13:49:19.890986: meanr2:        0.8584599971904704
2024-06-28 13:49:19.891598: train_loss 0.3712
2024-06-28 13:49:19.892001: val_loss 0.5119
2024-06-28 13:49:19.892405: Pseudo dice [0.5]
2024-06-28 13:49:19.892822: Epoch time: 64.28 s
2024-06-28 13:49:21.855195: 
2024-06-28 13:49:21.855962: Epoch 91
2024-06-28 13:49:21.856437: Current learning rate: 0.00011
2024-06-28 13:50:25.875873: meanmse:       0.01592143
2024-06-28 13:50:25.876997: meanr2:        0.8720524936929185
2024-06-28 13:50:25.877519: train_loss 0.3977
2024-06-28 13:50:25.877927: val_loss 0.4871
2024-06-28 13:50:25.878321: Pseudo dice [0.5]
2024-06-28 13:50:25.878750: Epoch time: 64.03 s
2024-06-28 13:50:27.789425: 
2024-06-28 13:50:27.790250: Epoch 92
2024-06-28 13:50:27.790729: Current learning rate: 0.0001
2024-06-28 13:51:32.048251: meanmse:       0.019701105
2024-06-28 13:51:32.049369: meanr2:        0.8428490700418443
2024-06-28 13:51:32.049839: train_loss 0.3699
2024-06-28 13:51:32.050421: val_loss 0.5568
2024-06-28 13:51:32.050927: Pseudo dice [0.5]
2024-06-28 13:51:32.051332: Epoch time: 64.27 s
2024-06-28 13:51:33.900848: 
2024-06-28 13:51:33.901611: Epoch 93
2024-06-28 13:51:33.902110: Current learning rate: 9e-05
2024-06-28 13:52:38.295168: meanmse:       0.017091522
2024-06-28 13:52:38.296341: meanr2:        0.8629722411807618
2024-06-28 13:52:38.296838: train_loss 0.3529
2024-06-28 13:52:38.297263: val_loss 0.5004
2024-06-28 13:52:38.297659: Pseudo dice [0.5]
2024-06-28 13:52:38.298065: Epoch time: 64.4 s
2024-06-28 13:52:40.111107: 
2024-06-28 13:52:40.111775: Epoch 94
2024-06-28 13:52:40.112208: Current learning rate: 8e-05
2024-06-28 13:53:44.341638: meanmse:       0.018239154
2024-06-28 13:53:44.342859: meanr2:        0.8540493953370455
2024-06-28 13:53:44.343476: train_loss 0.3519
2024-06-28 13:53:44.343940: val_loss 0.5302
2024-06-28 13:53:44.344385: Pseudo dice [0.5]
2024-06-28 13:53:44.344855: Epoch time: 64.24 s
2024-06-28 13:53:46.212606: 
2024-06-28 13:53:46.213585: Epoch 95
2024-06-28 13:53:46.214155: Current learning rate: 7e-05
2024-06-28 13:54:50.575412: meanmse:       0.014775354
2024-06-28 13:54:50.576488: meanr2:        0.8797985873743759
2024-06-28 13:54:50.576969: train_loss 0.3721
2024-06-28 13:54:50.577415: val_loss 0.4573
2024-06-28 13:54:50.577828: Pseudo dice [0.5]
2024-06-28 13:54:50.578269: Epoch time: 64.37 s
2024-06-28 13:54:52.583713: 
2024-06-28 13:54:52.584438: Epoch 96
2024-06-28 13:54:52.584905: Current learning rate: 6e-05
2024-06-28 13:55:56.671468: meanmse:       0.019409394
2024-06-28 13:55:56.672409: meanr2:        0.8399418457895359
2024-06-28 13:55:56.672822: train_loss 0.363
2024-06-28 13:55:56.673196: val_loss 0.5594
2024-06-28 13:55:56.673536: Pseudo dice [0.5]
2024-06-28 13:55:56.673895: Epoch time: 64.1 s
2024-06-28 13:55:58.950952: 
2024-06-28 13:55:58.952042: Epoch 97
2024-06-28 13:55:58.952647: Current learning rate: 4e-05
2024-06-28 13:57:03.480985: meanmse:       0.020071093
2024-06-28 13:57:03.482093: meanr2:        0.8313582679890806
2024-06-28 13:57:03.482635: train_loss 0.3632
2024-06-28 13:57:03.483011: val_loss 0.5606
2024-06-28 13:57:03.483404: Pseudo dice [0.5]
2024-06-28 13:57:03.483761: Epoch time: 64.54 s
2024-06-28 13:57:05.365822: 
2024-06-28 13:57:05.366629: Epoch 98
2024-06-28 13:57:05.367143: Current learning rate: 3e-05
2024-06-28 13:58:09.603618: meanmse:       0.021352999
2024-06-28 13:58:09.604554: meanr2:        0.8288409870707973
2024-06-28 13:58:09.604950: train_loss 0.371
2024-06-28 13:58:09.605318: val_loss 0.5909
2024-06-28 13:58:09.605702: Pseudo dice [0.5]
2024-06-28 13:58:09.606043: Epoch time: 64.25 s
2024-06-28 13:58:11.679121: 
2024-06-28 13:58:11.679875: Epoch 99
2024-06-28 13:58:11.680459: Current learning rate: 2e-05
2024-06-28 13:59:16.014455: meanmse:       0.014869398
2024-06-28 13:59:16.015374: meanr2:        0.8821190470355031
2024-06-28 13:59:16.015815: train_loss 0.3871
2024-06-28 13:59:16.016174: val_loss 0.4608
2024-06-28 13:59:16.016511: Pseudo dice [0.5]
2024-06-28 13:59:16.016875: Epoch time: 64.34 s
2024-06-28 13:59:18.323864: Training done.
