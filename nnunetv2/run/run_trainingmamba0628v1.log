nohup: ignoring input
Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

UMambaEnc: UMambaEnc(
  (encoder): ResidualMambaEncoder(
    (stem): StackedConvBlocks(
      (convs): Sequential(
        (0): ConvDropoutNormReLU(
          (conv): Conv3d(3, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
          (all_modules): Sequential(
            (0): Conv3d(3, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
            (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (2): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (5): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (mamba_layers): ModuleList(
      (0): MambaLayer(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=32, out_features=128, bias=False)
          (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
          (act): SiLU()
          (x_proj): Linear(in_features=64, out_features=34, bias=False)
          (dt_proj): Linear(in_features=2, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=32, bias=False)
        )
      )
      (1): MambaLayer(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=64, out_features=256, bias=False)
          (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
          (act): SiLU()
          (x_proj): Linear(in_features=128, out_features=36, bias=False)
          (dt_proj): Linear(in_features=4, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=64, bias=False)
        )
      )
      (2-3): 2 x MambaLayer(
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=128, out_features=512, bias=False)
          (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
          (act): SiLU()
          (x_proj): Linear(in_features=256, out_features=40, bias=False)
          (dt_proj): Linear(in_features=8, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=128, bias=False)
        )
      )
      (4): MambaLayer(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=64, out_features=256, bias=False)
          (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
          (act): SiLU()
          (x_proj): Linear(in_features=128, out_features=36, bias=False)
          (dt_proj): Linear(in_features=4, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=64, bias=False)
        )
      )
      (5): MambaLayer(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=32, out_features=128, bias=False)
          (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
          (act): SiLU()
          (x_proj): Linear(in_features=64, out_features=34, bias=False)
          (dt_proj): Linear(in_features=2, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=32, bias=False)
        )
      )
    )
  )
  (decoder): UNetResDecoder(
    (encoder): ResidualMambaEncoder(
      (stem): StackedConvBlocks(
        (convs): Sequential(
          (0): ConvDropoutNormReLU(
            (conv): Conv3d(3, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
            (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
            (all_modules): Sequential(
              (0): Conv3d(3, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (stages): ModuleList(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (4): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (5): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (mamba_layers): ModuleList(
        (0): MambaLayer(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=32, out_features=128, bias=False)
            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
            (act): SiLU()
            (x_proj): Linear(in_features=64, out_features=34, bias=False)
            (dt_proj): Linear(in_features=2, out_features=64, bias=True)
            (out_proj): Linear(in_features=64, out_features=32, bias=False)
          )
        )
        (1): MambaLayer(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (2-3): 2 x MambaLayer(
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (4): MambaLayer(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (5): MambaLayer(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=32, out_features=128, bias=False)
            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
            (act): SiLU()
            (x_proj): Linear(in_features=64, out_features=34, bias=False)
            (dt_proj): Linear(in_features=2, out_features=64, bias=True)
            (out_proj): Linear(in_features=64, out_features=32, bias=False)
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1-2): 2 x StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (transpconvs): ModuleList(
      (0): ConvTranspose3d(32, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (1): ConvTranspose3d(64, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (2): ConvTranspose3d(128, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (3): ConvTranspose3d(128, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (4): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2))
    )
    (lzz_layers): ModuleList(
      (0): ModuleList(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1-2): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (1): ModuleList(
        (0-1): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2-3): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (2): ModuleList(
        (0-2): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (3): ModuleList(
        (0-2): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (4): ModuleList(
        (0-1): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2-4): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
    )
    (lzz_fc): ModuleList(
      (0-4): 5 x ModuleList(
        (0): Sequential(
          (0): Linear(in_features=288, out_features=72, bias=True)
          (1): Tanh()
        )
        (1): Sequential(
          (0): Linear(in_features=72, out_features=18, bias=True)
          (1): Tanh()
        )
        (2): Sequential(
          (0): Linear(in_features=18, out_features=1, bias=True)
          (1): Tanh()
        )
      )
    )
  )
)

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [40, 192, 192], 'median_image_size_in_voxels': [97.0, 512.0, 512.0], 'spacing': [2.5, 0.7958984971046448, 0.7958984971046448], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [3, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset701_AbdomenCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.7958984971046448, 0.7958984971046448], 'original_median_shape_after_transp': [97, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3071.0, 'mean': 97.29716491699219, 'median': 118.0, 'min': -1024.0, 'percentile_00_5': -958.0, 'percentile_99_5': 270.0, 'std': 137.8484649658203}}} 

2024-06-28 19:08:37.241722: unpacking dataset...
2024-06-28 19:08:37.242132: unpacking done...
2024-06-28 19:08:37.242925: do_dummy_2d_data_aug: False
2024-06-28 19:08:37.258201: Unable to plot network architecture:
2024-06-28 19:08:37.258764: No module named 'hiddenlayer'
2024-06-28 19:08:37.273345: 
2024-06-28 19:08:37.274140: Epoch 0
2024-06-28 19:08:37.274680: Current learning rate: 0.001
using pin_memory on device 0
using pin_memory on device 0
2024-06-28 19:09:51.527938: meanmse:       0.12625228
2024-06-28 19:09:51.529609: meanr2:        -0.024342364844796724
2024-06-28 19:09:51.530663: train_loss 3.0092
2024-06-28 19:09:51.531269: val_loss 2.5914
2024-06-28 19:09:51.531773: Pseudo dice [0.5]
2024-06-28 19:09:51.532300: Epoch time: 74.27 s
2024-06-28 19:09:51.532778: Yayy! New best R2: -0.0243
2024-06-28 19:09:53.671762: 
2024-06-28 19:09:53.672552: Epoch 1
2024-06-28 19:09:53.673037: Current learning rate: 0.00099
2024-06-28 19:10:55.130285: meanmse:       0.124446444
2024-06-28 19:10:55.131732: meanr2:        -0.01725075745772988
2024-06-28 19:10:55.132428: train_loss 2.5617
2024-06-28 19:10:55.132973: val_loss 2.5387
2024-06-28 19:10:55.133457: Pseudo dice [0.5]
2024-06-28 19:10:55.133962: Epoch time: 61.47 s
2024-06-28 19:10:55.134466: Yayy! New best R2: -0.0173
2024-06-28 19:10:57.271241: 
2024-06-28 19:10:57.272101: Epoch 2
2024-06-28 19:10:57.272596: Current learning rate: 0.00098
2024-06-28 19:11:59.138385: meanmse:       0.121332414
2024-06-28 19:11:59.139708: meanr2:        0.024990333972705437
2024-06-28 19:11:59.140445: train_loss 2.5035
2024-06-28 19:11:59.141007: val_loss 2.5326
2024-06-28 19:11:59.141500: Pseudo dice [0.5]
2024-06-28 19:11:59.141968: Epoch time: 61.88 s
2024-06-28 19:11:59.142456: Yayy! New best R2: 0.025
2024-06-28 19:12:02.005677: 
2024-06-28 19:12:02.006322: Epoch 3
2024-06-28 19:12:02.006818: Current learning rate: 0.00097
2024-06-28 19:13:03.899521: meanmse:       0.10164633
2024-06-28 19:13:03.900736: meanr2:        0.1715200492762393
2024-06-28 19:13:03.901368: train_loss 2.421
2024-06-28 19:13:03.901786: val_loss 2.3574
2024-06-28 19:13:03.902153: Pseudo dice [0.5]
2024-06-28 19:13:03.902573: Epoch time: 61.9 s
2024-06-28 19:13:03.902936: Yayy! New best R2: 0.1715
2024-06-28 19:13:06.450344: 
2024-06-28 19:13:06.451416: Epoch 4
2024-06-28 19:13:06.452072: Current learning rate: 0.00096
2024-06-28 19:14:08.289151: meanmse:       0.07547278
2024-06-28 19:14:08.290447: meanr2:        0.3899398663290151
2024-06-28 19:14:08.290989: train_loss 2.1347
2024-06-28 19:14:08.291409: val_loss 1.9867
2024-06-28 19:14:08.291818: Pseudo dice [0.5]
2024-06-28 19:14:08.292267: Epoch time: 61.85 s
2024-06-28 19:14:08.292661: Yayy! New best R2: 0.3899
2024-06-28 19:14:11.155753: 
2024-06-28 19:14:11.156766: Epoch 5
2024-06-28 19:14:11.157334: Current learning rate: 0.00095
2024-06-28 19:15:13.075344: meanmse:       0.06244622
2024-06-28 19:15:13.076486: meanr2:        0.49628771368546654
2024-06-28 19:15:13.077070: train_loss 1.9329
2024-06-28 19:15:13.077490: val_loss 1.7886
2024-06-28 19:15:13.077867: Pseudo dice [0.5]
2024-06-28 19:15:13.078239: Epoch time: 61.93 s
2024-06-28 19:15:13.078596: Yayy! New best R2: 0.4963
2024-06-28 19:15:15.362202: 
2024-06-28 19:15:15.363075: Epoch 6
2024-06-28 19:15:15.363562: Current learning rate: 0.00095
2024-06-28 19:16:17.319475: meanmse:       0.05107014
2024-06-28 19:16:17.320676: meanr2:        0.5828831907462513
2024-06-28 19:16:17.321228: train_loss 1.6884
2024-06-28 19:16:17.321727: val_loss 1.6328
2024-06-28 19:16:17.322234: Pseudo dice [0.5]
2024-06-28 19:16:17.322706: Epoch time: 61.97 s
2024-06-28 19:16:17.323136: Yayy! New best R2: 0.5829
2024-06-28 19:16:19.695217: 
2024-06-28 19:16:19.695845: Epoch 7
2024-06-28 19:16:19.696304: Current learning rate: 0.00094
2024-06-28 19:17:21.763943: meanmse:       0.052932467
2024-06-28 19:17:21.765583: meanr2:        0.5655265050990185
2024-06-28 19:17:21.766272: train_loss 1.5838
2024-06-28 19:17:21.766740: val_loss 1.5165
2024-06-28 19:17:21.767126: Pseudo dice [0.5]
2024-06-28 19:17:21.767516: Epoch time: 62.08 s
2024-06-28 19:17:23.886132: 
2024-06-28 19:17:23.886863: Epoch 8
2024-06-28 19:17:23.887344: Current learning rate: 0.00093
2024-06-28 19:18:25.901341: meanmse:       0.047440745
2024-06-28 19:18:25.902390: meanr2:        0.6151893522549134
2024-06-28 19:18:25.902793: train_loss 1.3446
2024-06-28 19:18:25.903153: val_loss 1.4523
2024-06-28 19:18:25.903533: Pseudo dice [0.5]
2024-06-28 19:18:25.904001: Epoch time: 62.02 s
2024-06-28 19:18:25.904327: Yayy! New best R2: 0.6152
2024-06-28 19:18:28.319417: 
2024-06-28 19:18:28.320339: Epoch 9
2024-06-28 19:18:28.321005: Current learning rate: 0.00092
2024-06-28 19:19:30.390860: meanmse:       0.039612975
2024-06-28 19:19:30.391993: meanr2:        0.6781376950169803
2024-06-28 19:19:30.392503: train_loss 1.1809
2024-06-28 19:19:30.392911: val_loss 1.1381
2024-06-28 19:19:30.393276: Pseudo dice [0.5]
2024-06-28 19:19:30.393663: Epoch time: 62.08 s
2024-06-28 19:19:30.707774: Yayy! New best R2: 0.6781
2024-06-28 19:19:32.965263: 
2024-06-28 19:19:32.965856: Epoch 10
2024-06-28 19:19:32.966239: Current learning rate: 0.00091
2024-06-28 19:20:34.986718: meanmse:       0.045304596
2024-06-28 19:20:34.987815: meanr2:        0.6424820267905785
2024-06-28 19:20:34.988288: train_loss 1.1279
2024-06-28 19:20:34.988694: val_loss 1.2304
2024-06-28 19:20:34.989086: Pseudo dice [0.5]
2024-06-28 19:20:34.989476: Epoch time: 62.03 s
2024-06-28 19:20:36.976768: 
2024-06-28 19:20:36.977602: Epoch 11
2024-06-28 19:20:36.978180: Current learning rate: 0.0009
2024-06-28 19:21:38.946322: meanmse:       0.033687063
2024-06-28 19:21:38.948053: meanr2:        0.7321521131116242
2024-06-28 19:21:38.948916: train_loss 1.0775
2024-06-28 19:21:38.949498: val_loss 1.0055
2024-06-28 19:21:38.950159: Pseudo dice [0.5]
2024-06-28 19:21:38.950745: Epoch time: 61.98 s
2024-06-28 19:21:38.953441: Yayy! New best R2: 0.7322
2024-06-28 19:21:41.420964: 
2024-06-28 19:21:41.422600: Epoch 12
2024-06-28 19:21:41.423251: Current learning rate: 0.00089
2024-06-28 19:22:43.384184: meanmse:       0.03866064
2024-06-28 19:22:43.385329: meanr2:        0.6900317580349966
2024-06-28 19:22:43.385844: train_loss 0.9587
2024-06-28 19:22:43.386241: val_loss 1.0563
2024-06-28 19:22:43.386632: Pseudo dice [0.5]
2024-06-28 19:22:43.387033: Epoch time: 61.97 s
2024-06-28 19:22:45.461569: 
2024-06-28 19:22:45.462426: Epoch 13
2024-06-28 19:22:45.462931: Current learning rate: 0.00088
2024-06-28 19:23:47.454581: meanmse:       0.029426927
2024-06-28 19:23:47.456033: meanr2:        0.7617120569685891
2024-06-28 19:23:47.456582: train_loss 0.9126
2024-06-28 19:23:47.457057: val_loss 0.8925
2024-06-28 19:23:47.457683: Pseudo dice [0.5]
2024-06-28 19:23:47.463999: Epoch time: 62.01 s
2024-06-28 19:23:47.464828: Yayy! New best R2: 0.7617
2024-06-28 19:23:49.796471: 
2024-06-28 19:23:49.797014: Epoch 14
2024-06-28 19:23:49.797397: Current learning rate: 0.00087
2024-06-28 19:24:51.786399: meanmse:       0.040998906
2024-06-28 19:24:51.787314: meanr2:        0.6726540957594564
2024-06-28 19:24:51.787714: train_loss 0.8654
2024-06-28 19:24:51.788158: val_loss 1.0409
2024-06-28 19:24:51.788521: Pseudo dice [0.5]
2024-06-28 19:24:51.788903: Epoch time: 62.0 s
2024-06-28 19:24:53.759284: 
2024-06-28 19:24:53.760167: Epoch 15
2024-06-28 19:24:53.760633: Current learning rate: 0.00086
2024-06-28 19:25:55.706782: meanmse:       0.02777308
2024-06-28 19:25:55.708139: meanr2:        0.7725965399908528
2024-06-28 19:25:55.708757: train_loss 0.8835
2024-06-28 19:25:55.709491: val_loss 0.826
2024-06-28 19:25:55.710019: Pseudo dice [0.5]
2024-06-28 19:25:55.710471: Epoch time: 61.96 s
2024-06-28 19:25:55.711043: Yayy! New best R2: 0.7726
2024-06-28 19:25:58.085997: 
2024-06-28 19:25:58.086645: Epoch 16
2024-06-28 19:25:58.087090: Current learning rate: 0.00085
2024-06-28 19:27:00.140166: meanmse:       0.038530275
2024-06-28 19:27:00.141296: meanr2:        0.691255073707202
2024-06-28 19:27:00.141787: train_loss 0.8556
2024-06-28 19:27:00.142382: val_loss 1.0205
2024-06-28 19:27:00.142886: Pseudo dice [0.5]
2024-06-28 19:27:00.143426: Epoch time: 62.06 s
2024-06-28 19:27:02.459742: 
2024-06-28 19:27:02.460522: Epoch 17
2024-06-28 19:27:02.461045: Current learning rate: 0.00085
2024-06-28 19:28:04.527961: meanmse:       0.031105442
2024-06-28 19:28:04.529591: meanr2:        0.7474414794139806
2024-06-28 19:28:04.530404: train_loss 0.8021
2024-06-28 19:28:04.530962: val_loss 0.8822
2024-06-28 19:28:04.531449: Pseudo dice [0.5]
2024-06-28 19:28:04.532049: Epoch time: 62.08 s
2024-06-28 19:28:06.745759: 
2024-06-28 19:28:06.746668: Epoch 18
2024-06-28 19:28:06.747478: Current learning rate: 0.00084
2024-06-28 19:29:08.865713: meanmse:       0.026785636
2024-06-28 19:29:08.866774: meanr2:        0.7845394515190388
2024-06-28 19:29:08.867400: train_loss 0.7742
2024-06-28 19:29:08.867890: val_loss 0.7806
2024-06-28 19:29:08.868380: Pseudo dice [0.5]
2024-06-28 19:29:08.868895: Epoch time: 62.13 s
2024-06-28 19:29:08.869393: Yayy! New best R2: 0.7845
2024-06-28 19:29:11.209513: 
2024-06-28 19:29:11.210176: Epoch 19
2024-06-28 19:29:11.210615: Current learning rate: 0.00083
2024-06-28 19:30:13.446046: meanmse:       0.029091991
2024-06-28 19:30:13.447280: meanr2:        0.7589188502040319
2024-06-28 19:30:13.447855: train_loss 0.767
2024-06-28 19:30:13.448457: val_loss 0.829
2024-06-28 19:30:13.448951: Pseudo dice [0.5]
2024-06-28 19:30:13.449547: Epoch time: 62.24 s
2024-06-28 19:30:15.981241: 
2024-06-28 19:30:15.982705: Epoch 20
2024-06-28 19:30:15.983234: Current learning rate: 0.00082
2024-06-28 19:31:17.966832: meanmse:       0.025945397
2024-06-28 19:31:17.967974: meanr2:        0.7907324325469265
2024-06-28 19:31:17.968437: train_loss 0.7647
2024-06-28 19:31:17.968801: val_loss 0.7774
2024-06-28 19:31:17.969141: Pseudo dice [0.5]
2024-06-28 19:31:17.969469: Epoch time: 62.0 s
2024-06-28 19:31:17.969782: Yayy! New best R2: 0.7907
2024-06-28 19:31:20.239755: 
2024-06-28 19:31:20.240504: Epoch 21
2024-06-28 19:31:20.240949: Current learning rate: 0.00081
2024-06-28 19:32:22.137968: meanmse:       0.03127301
2024-06-28 19:32:22.139134: meanr2:        0.7409519502777325
2024-06-28 19:32:22.139684: train_loss 0.7331
2024-06-28 19:32:22.140186: val_loss 0.8554
2024-06-28 19:32:22.140679: Pseudo dice [0.5]
2024-06-28 19:32:22.141141: Epoch time: 61.91 s
2024-06-28 19:32:23.954633: 
2024-06-28 19:32:23.955534: Epoch 22
2024-06-28 19:32:23.956157: Current learning rate: 0.0008
2024-06-28 19:33:25.865194: meanmse:       0.034328643
2024-06-28 19:33:25.866471: meanr2:        0.72121640211276
2024-06-28 19:33:25.866986: train_loss 0.7228
2024-06-28 19:33:25.867378: val_loss 0.8978
2024-06-28 19:33:25.867716: Pseudo dice [0.5]
2024-06-28 19:33:25.868083: Epoch time: 61.92 s
2024-06-28 19:33:27.785954: 
2024-06-28 19:33:27.786655: Epoch 23
2024-06-28 19:33:27.787172: Current learning rate: 0.00079
2024-06-28 19:34:29.663690: meanmse:       0.029185407
2024-06-28 19:34:29.664885: meanr2:        0.7596278831199803
2024-06-28 19:34:29.665399: train_loss 0.6948
2024-06-28 19:34:29.665888: val_loss 0.8183
2024-06-28 19:34:29.666316: Pseudo dice [0.5]
2024-06-28 19:34:29.666704: Epoch time: 61.89 s
2024-06-28 19:34:31.664563: 
2024-06-28 19:34:31.665362: Epoch 24
2024-06-28 19:34:31.666020: Current learning rate: 0.00078
2024-06-28 19:35:33.601404: meanmse:       0.036375437
2024-06-28 19:35:33.602409: meanr2:        0.708504274097565
2024-06-28 19:35:33.602874: train_loss 0.7219
2024-06-28 19:35:33.603263: val_loss 0.9219
2024-06-28 19:35:33.603717: Pseudo dice [0.5]
2024-06-28 19:35:33.604107: Epoch time: 61.95 s
2024-06-28 19:35:35.576598: 
2024-06-28 19:35:35.577195: Epoch 25
2024-06-28 19:35:35.577656: Current learning rate: 0.00077
2024-06-28 19:36:37.582009: meanmse:       0.031322517
2024-06-28 19:36:37.583069: meanr2:        0.7453000616201724
2024-06-28 19:36:37.583639: train_loss 0.6493
2024-06-28 19:36:37.584086: val_loss 0.8287
2024-06-28 19:36:37.584493: Pseudo dice [0.5]
2024-06-28 19:36:37.584936: Epoch time: 62.01 s
2024-06-28 19:36:39.619848: 
2024-06-28 19:36:39.620507: Epoch 26
2024-06-28 19:36:39.620973: Current learning rate: 0.00076
2024-06-28 19:37:41.670535: meanmse:       0.029333152
2024-06-28 19:37:41.671550: meanr2:        0.7633023879451297
2024-06-28 19:37:41.672057: train_loss 0.6517
2024-06-28 19:37:41.672446: val_loss 0.8025
2024-06-28 19:37:41.672805: Pseudo dice [0.5]
2024-06-28 19:37:41.673182: Epoch time: 62.06 s
2024-06-28 19:37:43.681124: 
2024-06-28 19:37:43.681892: Epoch 27
2024-06-28 19:37:43.682428: Current learning rate: 0.00075
2024-06-28 19:38:45.732778: meanmse:       0.026677495
2024-06-28 19:38:45.733921: meanr2:        0.7824969332152801
2024-06-28 19:38:45.734423: train_loss 0.7129
2024-06-28 19:38:45.734910: val_loss 0.7537
2024-06-28 19:38:45.735301: Pseudo dice [0.5]
2024-06-28 19:38:45.735696: Epoch time: 62.06 s
2024-06-28 19:38:47.585563: 
2024-06-28 19:38:47.586110: Epoch 28
2024-06-28 19:38:47.586497: Current learning rate: 0.00074
2024-06-28 19:39:49.782562: meanmse:       0.022527562
2024-06-28 19:39:49.783657: meanr2:        0.8203076473956427
2024-06-28 19:39:49.784192: train_loss 0.667
2024-06-28 19:39:49.784582: val_loss 0.686
2024-06-28 19:39:49.784908: Pseudo dice [0.5]
2024-06-28 19:39:49.785266: Epoch time: 62.2 s
2024-06-28 19:39:49.785634: Yayy! New best R2: 0.8203
2024-06-28 19:39:52.025345: 
2024-06-28 19:39:52.026028: Epoch 29
2024-06-28 19:39:52.026475: Current learning rate: 0.00073
2024-06-28 19:40:54.167335: meanmse:       0.025434416
2024-06-28 19:40:54.168687: meanr2:        0.7949567168427989
2024-06-28 19:40:54.169258: train_loss 0.6538
2024-06-28 19:40:54.169775: val_loss 0.7365
2024-06-28 19:40:54.170262: Pseudo dice [0.5]
2024-06-28 19:40:54.170707: Epoch time: 62.15 s
2024-06-28 19:40:56.648830: 
2024-06-28 19:40:56.649539: Epoch 30
2024-06-28 19:40:56.649985: Current learning rate: 0.00073
2024-06-28 19:41:58.766284: meanmse:       0.022948395
2024-06-28 19:41:58.767513: meanr2:        0.8125847965069468
2024-06-28 19:41:58.768060: train_loss 0.643
2024-06-28 19:41:58.768548: val_loss 0.6823
2024-06-28 19:41:58.768983: Pseudo dice [0.5]
2024-06-28 19:41:58.769466: Epoch time: 62.13 s
2024-06-28 19:42:00.789872: 
2024-06-28 19:42:00.790535: Epoch 31
2024-06-28 19:42:00.790989: Current learning rate: 0.00072
2024-06-28 19:43:03.020496: meanmse:       0.02424326
2024-06-28 19:43:03.021598: meanr2:        0.8045601499468834
2024-06-28 19:43:03.022170: train_loss 0.5888
2024-06-28 19:43:03.022555: val_loss 0.7096
2024-06-28 19:43:03.022916: Pseudo dice [0.5]
2024-06-28 19:43:03.023306: Epoch time: 62.24 s
2024-06-28 19:43:05.187000: 
2024-06-28 19:43:05.187648: Epoch 32
2024-06-28 19:43:05.188087: Current learning rate: 0.00071
2024-06-28 19:44:07.260572: meanmse:       0.0261603
2024-06-28 19:44:07.261788: meanr2:        0.7899714199339921
2024-06-28 19:44:07.262378: train_loss 0.5938
2024-06-28 19:44:07.262841: val_loss 0.752
2024-06-28 19:44:07.263264: Pseudo dice [0.5]
2024-06-28 19:44:07.263762: Epoch time: 62.08 s
2024-06-28 19:44:09.426328: 
2024-06-28 19:44:09.427300: Epoch 33
2024-06-28 19:44:09.428059: Current learning rate: 0.0007
2024-06-28 19:45:11.397133: meanmse:       0.031182526
2024-06-28 19:45:11.398125: meanr2:        0.7525888150226528
2024-06-28 19:45:11.398637: train_loss 0.5507
2024-06-28 19:45:11.399038: val_loss 0.8141
2024-06-28 19:45:11.399410: Pseudo dice [0.5]
2024-06-28 19:45:11.399778: Epoch time: 61.98 s
2024-06-28 19:45:13.369267: 
2024-06-28 19:45:13.369941: Epoch 34
2024-06-28 19:45:13.370475: Current learning rate: 0.00069
2024-06-28 19:46:15.327354: meanmse:       0.025171192
2024-06-28 19:46:15.334267: meanr2:        0.7990472881367793
2024-06-28 19:46:15.335049: train_loss 0.5467
2024-06-28 19:46:15.335629: val_loss 0.7078
2024-06-28 19:46:15.336112: Pseudo dice [0.5]
2024-06-28 19:46:15.336635: Epoch time: 61.97 s
2024-06-28 19:46:17.440052: 
2024-06-28 19:46:17.444677: Epoch 35
2024-06-28 19:46:17.445391: Current learning rate: 0.00068
2024-06-28 19:47:19.511806: meanmse:       0.023965538
2024-06-28 19:47:19.512996: meanr2:        0.8031965150641063
2024-06-28 19:47:19.513584: train_loss 0.5393
2024-06-28 19:47:19.514052: val_loss 0.6825
2024-06-28 19:47:19.514494: Pseudo dice [0.5]
2024-06-28 19:47:19.514948: Epoch time: 62.08 s
2024-06-28 19:47:21.482416: 
2024-06-28 19:47:21.483004: Epoch 36
2024-06-28 19:47:21.483396: Current learning rate: 0.00067
2024-06-28 19:48:23.422965: meanmse:       0.019975353
2024-06-28 19:48:23.424389: meanr2:        0.8379060037789805
2024-06-28 19:48:23.425040: train_loss 0.5247
2024-06-28 19:48:23.425598: val_loss 0.614
2024-06-28 19:48:23.426152: Pseudo dice [0.5]
2024-06-28 19:48:23.427037: Epoch time: 61.95 s
2024-06-28 19:48:23.427635: Yayy! New best R2: 0.8379
2024-06-28 19:48:25.845246: 
2024-06-28 19:48:25.846519: Epoch 37
2024-06-28 19:48:25.847319: Current learning rate: 0.00066
2024-06-28 19:49:27.992383: meanmse:       0.02109322
2024-06-28 19:49:27.993279: meanr2:        0.8274891323678922
2024-06-28 19:49:27.993751: train_loss 0.512
2024-06-28 19:49:27.994115: val_loss 0.6328
2024-06-28 19:49:27.994490: Pseudo dice [0.5]
2024-06-28 19:49:27.994804: Epoch time: 62.16 s
2024-06-28 19:49:30.083678: 
2024-06-28 19:49:30.084694: Epoch 38
2024-06-28 19:49:30.085406: Current learning rate: 0.00065
2024-06-28 19:50:32.380358: meanmse:       0.021139083
2024-06-28 19:50:32.381182: meanr2:        0.8293003312816843
2024-06-28 19:50:32.381602: train_loss 0.5143
2024-06-28 19:50:32.381953: val_loss 0.6212
2024-06-28 19:50:32.382263: Pseudo dice [0.5]
2024-06-28 19:50:32.382573: Epoch time: 62.31 s
2024-06-28 19:50:34.557788: 
2024-06-28 19:50:34.559046: Epoch 39
2024-06-28 19:50:34.559763: Current learning rate: 0.00064
2024-06-28 19:51:36.662309: meanmse:       0.02647233
2024-06-28 19:51:36.664010: meanr2:        0.7906062046727154
2024-06-28 19:51:36.664726: train_loss 0.5892
2024-06-28 19:51:36.665219: val_loss 0.7121
2024-06-28 19:51:36.665700: Pseudo dice [0.5]
2024-06-28 19:51:36.666163: Epoch time: 62.12 s
2024-06-28 19:51:39.348435: 
2024-06-28 19:51:39.349647: Epoch 40
2024-06-28 19:51:39.350230: Current learning rate: 0.00063
2024-06-28 19:52:41.442407: meanmse:       0.020838741
2024-06-28 19:52:41.443746: meanr2:        0.8315876993367491
2024-06-28 19:52:41.444367: train_loss 0.588
2024-06-28 19:52:41.444800: val_loss 0.6452
2024-06-28 19:52:41.445272: Pseudo dice [0.5]
2024-06-28 19:52:41.445698: Epoch time: 62.1 s
2024-06-28 19:52:43.386210: 
2024-06-28 19:52:43.387094: Epoch 41
2024-06-28 19:52:43.387601: Current learning rate: 0.00062
2024-06-28 19:53:45.443783: meanmse:       0.026576502
2024-06-28 19:53:45.447551: meanr2:        0.7878339276090905
2024-06-28 19:53:45.448895: train_loss 0.5519
2024-06-28 19:53:45.453538: val_loss 0.7484
2024-06-28 19:53:45.454453: Pseudo dice [0.5]
2024-06-28 19:53:45.454924: Epoch time: 62.07 s
2024-06-28 19:53:47.501448: 
2024-06-28 19:53:47.502008: Epoch 42
2024-06-28 19:53:47.502404: Current learning rate: 0.00061
2024-06-28 19:54:49.656436: meanmse:       0.023420282
2024-06-28 19:54:49.657727: meanr2:        0.8074564016887987
2024-06-28 19:54:49.658409: train_loss 0.5484
2024-06-28 19:54:49.658917: val_loss 0.6617
2024-06-28 19:54:49.659322: Pseudo dice [0.5]
2024-06-28 19:54:49.659728: Epoch time: 62.16 s
2024-06-28 19:54:51.987457: 
2024-06-28 19:54:51.988072: Epoch 43
2024-06-28 19:54:51.988566: Current learning rate: 0.0006
2024-06-28 19:55:54.147028: meanmse:       0.022317903
2024-06-28 19:55:54.149710: meanr2:        0.8198943950249679
2024-06-28 19:55:54.150312: train_loss 0.5412
2024-06-28 19:55:54.150725: val_loss 0.6452
2024-06-28 19:55:54.151095: Pseudo dice [0.5]
2024-06-28 19:55:54.151623: Epoch time: 62.17 s
2024-06-28 19:55:56.216082: 
2024-06-28 19:55:56.216813: Epoch 44
2024-06-28 19:55:56.217805: Current learning rate: 0.00059
2024-06-28 19:56:58.520320: meanmse:       0.024279373
2024-06-28 19:56:58.521641: meanr2:        0.8041644536908014
2024-06-28 19:56:58.522370: train_loss 0.5215
2024-06-28 19:56:58.525153: val_loss 0.6913
2024-06-28 19:56:58.526046: Pseudo dice [0.5]
2024-06-28 19:56:58.526543: Epoch time: 62.31 s
2024-06-28 19:57:00.403093: 
2024-06-28 19:57:00.403698: Epoch 45
2024-06-28 19:57:00.404121: Current learning rate: 0.00058
2024-06-28 19:58:02.460840: meanmse:       0.020696072
2024-06-28 19:58:02.462076: meanr2:        0.8320618513138357
2024-06-28 19:58:02.462671: train_loss 0.51
2024-06-28 19:58:02.463367: val_loss 0.633
2024-06-28 19:58:02.463798: Pseudo dice [0.5]
2024-06-28 19:58:02.464276: Epoch time: 62.07 s
2024-06-28 19:58:04.378340: 
2024-06-28 19:58:04.379181: Epoch 46
2024-06-28 19:58:04.379797: Current learning rate: 0.00057
2024-06-28 19:59:06.548437: meanmse:       0.023365265
2024-06-28 19:59:06.549537: meanr2:        0.8098221808857297
2024-06-28 19:59:06.550077: train_loss 0.5088
2024-06-28 19:59:06.550488: val_loss 0.6528
2024-06-28 19:59:06.551182: Pseudo dice [0.5]
2024-06-28 19:59:06.551766: Epoch time: 62.18 s
2024-06-28 19:59:08.714598: 
2024-06-28 19:59:08.715332: Epoch 47
2024-06-28 19:59:08.715841: Current learning rate: 0.00056
2024-06-28 20:00:10.769122: meanmse:       0.019703096
2024-06-28 20:00:10.770300: meanr2:        0.8372865881941678
2024-06-28 20:00:10.770783: train_loss 0.4995
2024-06-28 20:00:10.771154: val_loss 0.5764
2024-06-28 20:00:10.771526: Pseudo dice [0.5]
2024-06-28 20:00:10.771915: Epoch time: 62.06 s
2024-06-28 20:00:12.700170: 
2024-06-28 20:00:12.700810: Epoch 48
2024-06-28 20:00:12.701227: Current learning rate: 0.00056
2024-06-28 20:01:14.761439: meanmse:       0.01882756
2024-06-28 20:01:14.762607: meanr2:        0.8475381806623364
2024-06-28 20:01:14.763118: train_loss 0.5189
2024-06-28 20:01:14.764470: val_loss 0.5712
2024-06-28 20:01:14.765062: Pseudo dice [0.5]
2024-06-28 20:01:14.766920: Epoch time: 62.07 s
2024-06-28 20:01:14.767477: Yayy! New best R2: 0.8475
2024-06-28 20:01:17.001251: 
2024-06-28 20:01:17.002054: Epoch 49
2024-06-28 20:01:17.002606: Current learning rate: 0.00055
2024-06-28 20:02:19.054691: meanmse:       0.01965951
2024-06-28 20:02:19.056021: meanr2:        0.8408363833987579
2024-06-28 20:02:19.056635: train_loss 0.4902
2024-06-28 20:02:19.057112: val_loss 0.5827
2024-06-28 20:02:19.057558: Pseudo dice [0.5]
2024-06-28 20:02:19.057976: Epoch time: 62.07 s
2024-06-28 20:02:21.717704: 
2024-06-28 20:02:21.718497: Epoch 50
2024-06-28 20:02:21.718944: Current learning rate: 0.00054
2024-06-28 20:03:23.860157: meanmse:       0.02219791
2024-06-28 20:03:23.861364: meanr2:        0.8190300351276831
2024-06-28 20:03:23.861914: train_loss 0.5004
2024-06-28 20:03:23.862364: val_loss 0.6629
2024-06-28 20:03:23.862748: Pseudo dice [0.5]
2024-06-28 20:03:23.863169: Epoch time: 62.15 s
2024-06-28 20:03:25.800069: 
2024-06-28 20:03:25.800713: Epoch 51
2024-06-28 20:03:25.801153: Current learning rate: 0.00053
2024-06-28 20:04:27.907700: meanmse:       0.020471985
2024-06-28 20:04:27.908835: meanr2:        0.8294426245679272
2024-06-28 20:04:27.909327: train_loss 0.4829
2024-06-28 20:04:27.909744: val_loss 0.6207
2024-06-28 20:04:27.910144: Pseudo dice [0.5]
2024-06-28 20:04:27.910584: Epoch time: 62.12 s
2024-06-28 20:04:30.049265: 
2024-06-28 20:04:30.050041: Epoch 52
2024-06-28 20:04:30.050626: Current learning rate: 0.00052
2024-06-28 20:05:32.154310: meanmse:       0.026986213
2024-06-28 20:05:32.155303: meanr2:        0.7781865001659987
2024-06-28 20:05:32.155768: train_loss 0.4826
2024-06-28 20:05:32.156192: val_loss 0.7154
2024-06-28 20:05:32.156549: Pseudo dice [0.5]
2024-06-28 20:05:32.156901: Epoch time: 62.12 s
2024-06-28 20:05:34.244116: 
2024-06-28 20:05:34.244709: Epoch 53
2024-06-28 20:05:34.245095: Current learning rate: 0.00051
2024-06-28 20:06:36.370522: meanmse:       0.018666832
2024-06-28 20:06:36.371803: meanr2:        0.8512953087219182
2024-06-28 20:06:36.372214: train_loss 0.4653
2024-06-28 20:06:36.372609: val_loss 0.5595
2024-06-28 20:06:36.372927: Pseudo dice [0.5]
2024-06-28 20:06:36.373331: Epoch time: 62.13 s
2024-06-28 20:06:36.373690: Yayy! New best R2: 0.8513
2024-06-28 20:06:38.801690: 
2024-06-28 20:06:38.802451: Epoch 54
2024-06-28 20:06:38.802946: Current learning rate: 0.0005
2024-06-28 20:07:40.972862: meanmse:       0.024733894
2024-06-28 20:07:40.973866: meanr2:        0.7925303885175091
2024-06-28 20:07:40.974358: train_loss 0.4847
2024-06-28 20:07:40.974769: val_loss 0.6637
2024-06-28 20:07:40.975174: Pseudo dice [0.5]
2024-06-28 20:07:40.975620: Epoch time: 62.18 s
2024-06-28 20:07:42.987881: 
2024-06-28 20:07:42.988516: Epoch 55
2024-06-28 20:07:42.988994: Current learning rate: 0.00049
2024-06-28 20:08:45.313417: meanmse:       0.022575917
2024-06-28 20:08:45.314383: meanr2:        0.8127863511631557
2024-06-28 20:08:45.314855: train_loss 0.4709
2024-06-28 20:08:45.315228: val_loss 0.6275
2024-06-28 20:08:45.315825: Pseudo dice [0.5]
2024-06-28 20:08:45.316264: Epoch time: 62.33 s
2024-06-28 20:08:47.267999: 
2024-06-28 20:08:47.268726: Epoch 56
2024-06-28 20:08:47.269197: Current learning rate: 0.00048
2024-06-28 20:09:49.455275: meanmse:       0.023259861
2024-06-28 20:09:49.459186: meanr2:        0.8110581954445091
2024-06-28 20:09:49.460068: train_loss 0.4508
2024-06-28 20:09:49.460625: val_loss 0.658
2024-06-28 20:09:49.463319: Pseudo dice [0.5]
2024-06-28 20:09:49.463883: Epoch time: 62.2 s
2024-06-28 20:09:51.474581: 
2024-06-28 20:09:51.475355: Epoch 57
2024-06-28 20:09:51.475884: Current learning rate: 0.00047
2024-06-28 20:10:53.852874: meanmse:       0.015358291
2024-06-28 20:10:53.854146: meanr2:        0.872980693641998
2024-06-28 20:10:53.854827: train_loss 0.4583
2024-06-28 20:10:53.855502: val_loss 0.4988
2024-06-28 20:10:53.855999: Pseudo dice [0.5]
2024-06-28 20:10:53.856538: Epoch time: 62.39 s
2024-06-28 20:10:53.856942: Yayy! New best R2: 0.873
2024-06-28 20:10:56.265019: 
2024-06-28 20:10:56.265794: Epoch 58
2024-06-28 20:10:56.266254: Current learning rate: 0.00046
2024-06-28 20:11:58.233320: meanmse:       0.022354864
2024-06-28 20:11:58.234283: meanr2:        0.8212740570181581
2024-06-28 20:11:58.234746: train_loss 0.4642
2024-06-28 20:11:58.235164: val_loss 0.6274
2024-06-28 20:11:58.235576: Pseudo dice [0.5]
2024-06-28 20:11:58.235971: Epoch time: 61.98 s
2024-06-28 20:12:00.250654: 
2024-06-28 20:12:00.251467: Epoch 59
2024-06-28 20:12:00.252010: Current learning rate: 0.00045
2024-06-28 20:13:02.517175: meanmse:       0.019771757
2024-06-28 20:13:02.518502: meanr2:        0.8434717825973891
2024-06-28 20:13:02.519058: train_loss 0.4593
2024-06-28 20:13:02.519505: val_loss 0.5954
2024-06-28 20:13:02.519926: Pseudo dice [0.5]
2024-06-28 20:13:02.520339: Epoch time: 62.28 s
2024-06-28 20:13:04.965954: 
2024-06-28 20:13:04.966644: Epoch 60
2024-06-28 20:13:04.967103: Current learning rate: 0.00044
2024-06-28 20:14:07.297055: meanmse:       0.01680267
2024-06-28 20:14:07.297988: meanr2:        0.8611040471262817
2024-06-28 20:14:07.298495: train_loss 0.4451
2024-06-28 20:14:07.298871: val_loss 0.5339
2024-06-28 20:14:07.299226: Pseudo dice [0.5]
2024-06-28 20:14:07.299584: Epoch time: 62.34 s
2024-06-28 20:14:09.404528: 
2024-06-28 20:14:09.405261: Epoch 61
2024-06-28 20:14:09.405832: Current learning rate: 0.00043
2024-06-28 20:15:11.382553: meanmse:       0.015893836
2024-06-28 20:15:11.383480: meanr2:        0.8696321898962268
2024-06-28 20:15:11.383900: train_loss 0.4271
2024-06-28 20:15:11.384240: val_loss 0.5133
2024-06-28 20:15:11.384536: Pseudo dice [0.5]
2024-06-28 20:15:11.384846: Epoch time: 61.99 s
2024-06-28 20:15:13.710433: 
2024-06-28 20:15:13.711045: Epoch 62
2024-06-28 20:15:13.711446: Current learning rate: 0.00042
2024-06-28 20:16:15.882901: meanmse:       0.021222722
2024-06-28 20:16:15.884134: meanr2:        0.8271953074039543
2024-06-28 20:16:15.884607: train_loss 0.4609
2024-06-28 20:16:15.885008: val_loss 0.6182
2024-06-28 20:16:15.885375: Pseudo dice [0.5]
2024-06-28 20:16:15.885727: Epoch time: 62.18 s
2024-06-28 20:16:17.827725: 
2024-06-28 20:16:17.828613: Epoch 63
2024-06-28 20:16:17.829232: Current learning rate: 0.00041
2024-06-28 20:17:20.022480: meanmse:       0.01709026
2024-06-28 20:17:20.023623: meanr2:        0.8578617763334496
2024-06-28 20:17:20.024076: train_loss 0.4256
2024-06-28 20:17:20.024429: val_loss 0.5181
2024-06-28 20:17:20.024742: Pseudo dice [0.5]
2024-06-28 20:17:20.025090: Epoch time: 62.21 s
2024-06-28 20:17:21.968624: 
2024-06-28 20:17:21.969381: Epoch 64
2024-06-28 20:17:21.969820: Current learning rate: 0.0004
2024-06-28 20:18:23.969577: meanmse:       0.01668682
2024-06-28 20:18:23.975121: meanr2:        0.8627232566028863
2024-06-28 20:18:23.975584: train_loss 0.42
2024-06-28 20:18:23.975986: val_loss 0.5216
2024-06-28 20:18:23.976359: Pseudo dice [0.5]
2024-06-28 20:18:23.976746: Epoch time: 62.01 s
2024-06-28 20:18:26.312369: 
2024-06-28 20:18:26.312969: Epoch 65
2024-06-28 20:18:26.313406: Current learning rate: 0.00039
2024-06-28 20:19:28.369986: meanmse:       0.021273639
2024-06-28 20:19:28.370990: meanr2:        0.8285425714772117
2024-06-28 20:19:28.371405: train_loss 0.4077
2024-06-28 20:19:28.371764: val_loss 0.6076
2024-06-28 20:19:28.372148: Pseudo dice [0.5]
2024-06-28 20:19:28.376206: Epoch time: 62.07 s
2024-06-28 20:19:30.476109: 
2024-06-28 20:19:30.476780: Epoch 66
2024-06-28 20:19:30.477260: Current learning rate: 0.00038
2024-06-28 20:20:33.137629: meanmse:       0.022889234
2024-06-28 20:20:33.151122: meanr2:        0.8141407209443604
2024-06-28 20:20:33.154626: train_loss 0.4099
2024-06-28 20:20:33.155275: val_loss 0.6391
2024-06-28 20:20:33.155717: Pseudo dice [0.5]
2024-06-28 20:20:33.157153: Epoch time: 62.68 s
2024-06-28 20:20:35.092458: 
2024-06-28 20:20:35.093062: Epoch 67
2024-06-28 20:20:35.093466: Current learning rate: 0.00037
2024-06-28 20:21:37.267132: meanmse:       0.021068754
2024-06-28 20:21:37.267993: meanr2:        0.8300903483436506
2024-06-28 20:21:37.268395: train_loss 0.4154
2024-06-28 20:21:37.268718: val_loss 0.5897
2024-06-28 20:21:37.269037: Pseudo dice [0.5]
2024-06-28 20:21:37.269373: Epoch time: 62.18 s
2024-06-28 20:21:39.650886: 
2024-06-28 20:21:39.651653: Epoch 68
2024-06-28 20:21:39.652222: Current learning rate: 0.00036
2024-06-28 20:22:41.926545: meanmse:       0.015915034
2024-06-28 20:22:41.927633: meanr2:        0.8696950038080683
2024-06-28 20:22:41.928170: train_loss 0.4117
2024-06-28 20:22:41.928710: val_loss 0.4984
2024-06-28 20:22:41.934252: Pseudo dice [0.5]
2024-06-28 20:22:41.934971: Epoch time: 62.29 s
2024-06-28 20:22:44.287339: 
2024-06-28 20:22:44.291420: Epoch 69
2024-06-28 20:22:44.292129: Current learning rate: 0.00035
2024-06-28 20:23:46.556361: meanmse:       0.025291335
2024-06-28 20:23:46.557753: meanr2:        0.7913706093131079
2024-06-28 20:23:46.558372: train_loss 0.3809
2024-06-28 20:23:46.558847: val_loss 0.6775
2024-06-28 20:23:46.559347: Pseudo dice [0.5]
2024-06-28 20:23:46.559870: Epoch time: 62.28 s
2024-06-28 20:23:49.064402: 
2024-06-28 20:23:49.065091: Epoch 70
2024-06-28 20:23:49.065526: Current learning rate: 0.00034
2024-06-28 20:24:51.330011: meanmse:       0.021218328
2024-06-28 20:24:51.331245: meanr2:        0.8291137525610399
2024-06-28 20:24:51.332049: train_loss 0.4052
2024-06-28 20:24:51.332684: val_loss 0.5902
2024-06-28 20:24:51.333096: Pseudo dice [0.5]
2024-06-28 20:24:51.333548: Epoch time: 62.27 s
2024-06-28 20:24:53.696816: 
2024-06-28 20:24:53.697441: Epoch 71
2024-06-28 20:24:53.697851: Current learning rate: 0.00033
2024-06-28 20:25:55.969469: meanmse:       0.021441983
2024-06-28 20:25:55.970785: meanr2:        0.8199423292946684
2024-06-28 20:25:55.971344: train_loss 0.3877
2024-06-28 20:25:55.971787: val_loss 0.609
2024-06-28 20:25:55.972220: Pseudo dice [0.5]
2024-06-28 20:25:55.972625: Epoch time: 62.28 s
2024-06-28 20:25:58.095914: 
2024-06-28 20:25:58.096491: Epoch 72
2024-06-28 20:25:58.096923: Current learning rate: 0.00032
2024-06-28 20:27:00.285867: meanmse:       0.027216893
2024-06-28 20:27:00.286795: meanr2:        0.7861138019183102
2024-06-28 20:27:00.287215: train_loss 0.3534
2024-06-28 20:27:00.287563: val_loss 0.6852
2024-06-28 20:27:00.287895: Pseudo dice [0.5]
2024-06-28 20:27:00.288273: Epoch time: 62.2 s
2024-06-28 20:27:02.214634: 
2024-06-28 20:27:02.215279: Epoch 73
2024-06-28 20:27:02.215786: Current learning rate: 0.00031
2024-06-28 20:28:04.317660: meanmse:       0.025108397
2024-06-28 20:28:04.318708: meanr2:        0.8005778886216098
2024-06-28 20:28:04.319253: train_loss 0.3628
2024-06-28 20:28:04.319706: val_loss 0.6827
2024-06-28 20:28:04.320118: Pseudo dice [0.5]
2024-06-28 20:28:04.320515: Epoch time: 62.11 s
2024-06-28 20:28:06.684357: 
2024-06-28 20:28:06.685210: Epoch 74
2024-06-28 20:28:06.685961: Current learning rate: 0.0003
2024-06-28 20:29:08.765284: meanmse:       0.017588364
2024-06-28 20:29:08.766394: meanr2:        0.8583549951744655
2024-06-28 20:29:08.766933: train_loss 0.3839
2024-06-28 20:29:08.767415: val_loss 0.5439
2024-06-28 20:29:08.767788: Pseudo dice [0.5]
2024-06-28 20:29:08.768415: Epoch time: 62.09 s
2024-06-28 20:29:10.704409: 
2024-06-28 20:29:10.705027: Epoch 75
2024-06-28 20:29:10.705424: Current learning rate: 0.00029
2024-06-28 20:30:12.787082: meanmse:       0.01930802
2024-06-28 20:30:12.787929: meanr2:        0.8449802898639999
2024-06-28 20:30:12.788330: train_loss 0.3446
2024-06-28 20:30:12.788648: val_loss 0.5791
2024-06-28 20:30:12.788951: Pseudo dice [0.5]
2024-06-28 20:30:12.789279: Epoch time: 62.09 s
2024-06-28 20:30:14.687395: 
2024-06-28 20:30:14.687902: Epoch 76
2024-06-28 20:30:14.688286: Current learning rate: 0.00028
2024-06-28 20:31:16.638905: meanmse:       0.025108563
2024-06-28 20:31:16.640204: meanr2:        0.7990623019191406
2024-06-28 20:31:16.640788: train_loss 0.405
2024-06-28 20:31:16.641253: val_loss 0.6667
2024-06-28 20:31:16.641715: Pseudo dice [0.5]
2024-06-28 20:31:16.642171: Epoch time: 61.96 s
2024-06-28 20:31:18.928047: 
2024-06-28 20:31:18.929491: Epoch 77
2024-06-28 20:31:18.930094: Current learning rate: 0.00027
2024-06-28 20:32:21.019619: meanmse:       0.016686222
2024-06-28 20:32:21.032220: meanr2:        0.861630888694162
2024-06-28 20:32:21.037934: train_loss 0.3992
2024-06-28 20:32:21.038710: val_loss 0.5209
2024-06-28 20:32:21.039235: Pseudo dice [0.5]
2024-06-28 20:32:21.039752: Epoch time: 62.12 s
2024-06-28 20:32:22.963576: 
2024-06-28 20:32:22.964371: Epoch 78
2024-06-28 20:32:22.964815: Current learning rate: 0.00026
2024-06-28 20:33:25.174509: meanmse:       0.018740289
2024-06-28 20:33:25.175490: meanr2:        0.8470101093896141
2024-06-28 20:33:25.175943: train_loss 0.3724
2024-06-28 20:33:25.176295: val_loss 0.5464
2024-06-28 20:33:25.176640: Pseudo dice [0.5]
2024-06-28 20:33:25.176991: Epoch time: 62.22 s
2024-06-28 20:33:27.229274: 
2024-06-28 20:33:27.230349: Epoch 79
2024-06-28 20:33:27.230967: Current learning rate: 0.00025
2024-06-28 20:34:29.279259: meanmse:       0.020401243
2024-06-28 20:34:29.280374: meanr2:        0.8364810669806944
2024-06-28 20:34:29.280851: train_loss 0.3567
2024-06-28 20:34:29.281230: val_loss 0.5832
2024-06-28 20:34:29.281569: Pseudo dice [0.5]
2024-06-28 20:34:29.281938: Epoch time: 62.06 s
2024-06-28 20:34:31.619530: 
2024-06-28 20:34:31.620668: Epoch 80
2024-06-28 20:34:31.621152: Current learning rate: 0.00023
2024-06-28 20:35:33.730467: meanmse:       0.021252848
2024-06-28 20:35:33.731730: meanr2:        0.8264348571377274
2024-06-28 20:35:33.732371: train_loss 0.387
2024-06-28 20:35:33.732869: val_loss 0.5958
2024-06-28 20:35:33.733291: Pseudo dice [0.5]
2024-06-28 20:35:33.733740: Epoch time: 62.12 s
2024-06-28 20:35:35.753777: 
2024-06-28 20:35:35.754658: Epoch 81
2024-06-28 20:35:35.755337: Current learning rate: 0.00022
2024-06-28 20:36:37.881785: meanmse:       0.020211639
2024-06-28 20:36:37.882699: meanr2:        0.8394013074635939
2024-06-28 20:36:37.883177: train_loss 0.3688
2024-06-28 20:36:37.883539: val_loss 0.5656
2024-06-28 20:36:37.883871: Pseudo dice [0.5]
2024-06-28 20:36:37.884236: Epoch time: 62.14 s
2024-06-28 20:36:39.957141: 
2024-06-28 20:36:39.957994: Epoch 82
2024-06-28 20:36:39.958502: Current learning rate: 0.00021
2024-06-28 20:37:42.065885: meanmse:       0.015290599
2024-06-28 20:37:42.066891: meanr2:        0.8728502909146815
2024-06-28 20:37:42.067364: train_loss 0.3507
2024-06-28 20:37:42.067866: val_loss 0.4842
2024-06-28 20:37:42.068262: Pseudo dice [0.5]
2024-06-28 20:37:42.068689: Epoch time: 62.12 s
2024-06-28 20:37:44.203765: 
2024-06-28 20:37:44.204538: Epoch 83
2024-06-28 20:37:44.205045: Current learning rate: 0.0002
2024-06-28 20:38:46.122450: meanmse:       0.022097282
2024-06-28 20:38:46.127175: meanr2:        0.8196651625887769
2024-06-28 20:38:46.128015: train_loss 0.3542
2024-06-28 20:38:46.128469: val_loss 0.6
2024-06-28 20:38:46.128849: Pseudo dice [0.5]
2024-06-28 20:38:46.129299: Epoch time: 61.93 s
2024-06-28 20:38:47.969199: 
2024-06-28 20:38:47.969754: Epoch 84
2024-06-28 20:38:47.970137: Current learning rate: 0.00019
2024-06-28 20:39:49.943183: meanmse:       0.020799886
2024-06-28 20:39:49.944274: meanr2:        0.8295707076044938
2024-06-28 20:39:49.944758: train_loss 0.3376
2024-06-28 20:39:49.945157: val_loss 0.5941
2024-06-28 20:39:49.945570: Pseudo dice [0.5]
2024-06-28 20:39:49.946098: Epoch time: 61.98 s
2024-06-28 20:39:51.805032: 
2024-06-28 20:39:51.805581: Epoch 85
2024-06-28 20:39:51.805933: Current learning rate: 0.00018
2024-06-28 20:40:53.866420: meanmse:       0.022095105
2024-06-28 20:40:53.867595: meanr2:        0.8226193542582362
2024-06-28 20:40:53.868109: train_loss 0.3366
2024-06-28 20:40:53.868559: val_loss 0.6122
2024-06-28 20:40:53.869004: Pseudo dice [0.5]
2024-06-28 20:40:53.869559: Epoch time: 62.07 s
2024-06-28 20:40:55.894006: 
2024-06-28 20:40:55.894795: Epoch 86
2024-06-28 20:40:55.895217: Current learning rate: 0.00017
2024-06-28 20:41:57.869302: meanmse:       0.018927854
2024-06-28 20:41:57.870275: meanr2:        0.8436302119695135
2024-06-28 20:41:57.870713: train_loss 0.3709
2024-06-28 20:41:57.871098: val_loss 0.5555
2024-06-28 20:41:57.871448: Pseudo dice [0.5]
2024-06-28 20:41:57.871806: Epoch time: 61.98 s
2024-06-28 20:41:59.925338: 
2024-06-28 20:41:59.926333: Epoch 87
2024-06-28 20:41:59.926953: Current learning rate: 0.00016
2024-06-28 20:43:01.773855: meanmse:       0.016597234
2024-06-28 20:43:01.774970: meanr2:        0.8653269254334361
2024-06-28 20:43:01.775476: train_loss 0.3348
2024-06-28 20:43:01.775870: val_loss 0.5104
2024-06-28 20:43:01.776241: Pseudo dice [0.5]
2024-06-28 20:43:01.776624: Epoch time: 61.86 s
2024-06-28 20:43:03.641834: 
2024-06-28 20:43:03.642702: Epoch 88
2024-06-28 20:43:03.643183: Current learning rate: 0.00015
2024-06-28 20:44:05.596161: meanmse:       0.023156462
2024-06-28 20:44:05.597059: meanr2:        0.8131370124828642
2024-06-28 20:44:05.597459: train_loss 0.3073
2024-06-28 20:44:05.597823: val_loss 0.6332
2024-06-28 20:44:05.598248: Pseudo dice [0.5]
2024-06-28 20:44:05.598581: Epoch time: 61.96 s
2024-06-28 20:44:07.566905: 
2024-06-28 20:44:07.567783: Epoch 89
2024-06-28 20:44:07.568266: Current learning rate: 0.00014
2024-06-28 20:45:09.487841: meanmse:       0.018568061
2024-06-28 20:45:09.489133: meanr2:        0.8508983655890522
2024-06-28 20:45:09.489701: train_loss 0.3495
2024-06-28 20:45:09.490137: val_loss 0.549
2024-06-28 20:45:09.490545: Pseudo dice [0.5]
2024-06-28 20:45:09.490938: Epoch time: 61.94 s
2024-06-28 20:45:12.053452: 
2024-06-28 20:45:12.054352: Epoch 90
2024-06-28 20:45:12.054856: Current learning rate: 0.00013
2024-06-28 20:46:13.974021: meanmse:       0.018935144
2024-06-28 20:46:13.975181: meanr2:        0.8466679964360626
2024-06-28 20:46:13.975693: train_loss 0.3218
2024-06-28 20:46:13.976158: val_loss 0.5314
2024-06-28 20:46:13.976619: Pseudo dice [0.5]
2024-06-28 20:46:13.977109: Epoch time: 61.93 s
2024-06-28 20:46:15.892048: 
2024-06-28 20:46:15.892655: Epoch 91
2024-06-28 20:46:15.893093: Current learning rate: 0.00011
2024-06-28 20:47:17.881062: meanmse:       0.019415887
2024-06-28 20:47:17.882114: meanr2:        0.8425305821505892
2024-06-28 20:47:17.882512: train_loss 0.3394
2024-06-28 20:47:17.882834: val_loss 0.5426
2024-06-28 20:47:17.883146: Pseudo dice [0.5]
2024-06-28 20:47:17.883473: Epoch time: 62.0 s
2024-06-28 20:47:19.745133: 
2024-06-28 20:47:19.746017: Epoch 92
2024-06-28 20:47:19.746613: Current learning rate: 0.0001
2024-06-28 20:48:21.675051: meanmse:       0.016666815
2024-06-28 20:48:21.676112: meanr2:        0.864946371345681
2024-06-28 20:48:21.676669: train_loss 0.3064
2024-06-28 20:48:21.677132: val_loss 0.5058
2024-06-28 20:48:21.677518: Pseudo dice [0.5]
2024-06-28 20:48:21.677881: Epoch time: 61.94 s
2024-06-28 20:48:23.564670: 
2024-06-28 20:48:23.565462: Epoch 93
2024-06-28 20:48:23.565939: Current learning rate: 9e-05
2024-06-28 20:49:25.466881: meanmse:       0.018431595
2024-06-28 20:49:25.467910: meanr2:        0.8510365419436016
2024-06-28 20:49:25.468365: train_loss 0.2903
2024-06-28 20:49:25.468789: val_loss 0.5372
2024-06-28 20:49:25.469189: Pseudo dice [0.5]
2024-06-28 20:49:25.469652: Epoch time: 61.91 s
2024-06-28 20:49:27.376811: 
2024-06-28 20:49:27.377715: Epoch 94
2024-06-28 20:49:27.378238: Current learning rate: 8e-05
2024-06-28 20:50:29.264922: meanmse:       0.022450535
2024-06-28 20:50:29.266186: meanr2:        0.8150432982380532
2024-06-28 20:50:29.266804: train_loss 0.3117
2024-06-28 20:50:29.267439: val_loss 0.5908
2024-06-28 20:50:29.267892: Pseudo dice [0.5]
2024-06-28 20:50:29.268402: Epoch time: 61.9 s
2024-06-28 20:50:31.114160: 
2024-06-28 20:50:31.114964: Epoch 95
2024-06-28 20:50:31.115412: Current learning rate: 7e-05
2024-06-28 20:51:33.163380: meanmse:       0.024035852
2024-06-28 20:51:33.164598: meanr2:        0.8078044077405967
2024-06-28 20:51:33.165118: train_loss 0.3383
2024-06-28 20:51:33.165507: val_loss 0.6201
2024-06-28 20:51:33.165885: Pseudo dice [0.5]
2024-06-28 20:51:33.166294: Epoch time: 62.06 s
2024-06-28 20:51:35.090644: 
2024-06-28 20:51:35.091434: Epoch 96
2024-06-28 20:51:35.091900: Current learning rate: 6e-05
2024-06-28 20:52:37.035713: meanmse:       0.020357175
2024-06-28 20:52:37.036975: meanr2:        0.8381807831444454
2024-06-28 20:52:37.037777: train_loss 0.3384
2024-06-28 20:52:37.038259: val_loss 0.5774
2024-06-28 20:52:37.038700: Pseudo dice [0.5]
2024-06-28 20:52:37.039190: Epoch time: 61.95 s
2024-06-28 20:52:38.883364: 
2024-06-28 20:52:38.884019: Epoch 97
2024-06-28 20:52:38.884502: Current learning rate: 4e-05
2024-06-28 20:53:41.174171: meanmse:       0.018401962
2024-06-28 20:53:41.175224: meanr2:        0.8500052200480411
2024-06-28 20:53:41.175697: train_loss 0.3045
2024-06-28 20:53:41.176098: val_loss 0.5469
2024-06-28 20:53:41.177018: Pseudo dice [0.5]
2024-06-28 20:53:41.178160: Epoch time: 62.3 s
2024-06-28 20:53:43.098557: 
2024-06-28 20:53:43.099221: Epoch 98
2024-06-28 20:53:43.099616: Current learning rate: 3e-05
2024-06-28 20:54:45.012010: meanmse:       0.016906124
2024-06-28 20:54:45.013136: meanr2:        0.8624497288490631
2024-06-28 20:54:45.013674: train_loss 0.3129
2024-06-28 20:54:45.014120: val_loss 0.5126
2024-06-28 20:54:45.014533: Pseudo dice [0.5]
2024-06-28 20:54:45.014959: Epoch time: 61.92 s
2024-06-28 20:54:47.032726: 
2024-06-28 20:54:47.033540: Epoch 99
2024-06-28 20:54:47.034107: Current learning rate: 2e-05
2024-06-28 20:55:49.081414: meanmse:       0.020121086
2024-06-28 20:55:49.082448: meanr2:        0.8370421805729467
2024-06-28 20:55:49.082929: train_loss 0.3383
2024-06-28 20:55:49.083331: val_loss 0.5532
2024-06-28 20:55:49.083742: Pseudo dice [0.5]
2024-06-28 20:55:49.084135: Epoch time: 62.06 s
2024-06-28 20:55:51.931332: Training done.
