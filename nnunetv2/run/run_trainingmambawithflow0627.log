nohup: ignoring input
Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

UMambaEnc: UMambaEnc(
  (encoder): ResidualMambaEncoder(
    (stem): StackedConvBlocks(
      (convs): Sequential(
        (0): ConvDropoutNormReLU(
          (conv): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
          (all_modules): Sequential(
            (0): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
            (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (2): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (5): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (mamba_layers): ModuleList(
      (0): MambaLayer(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=32, out_features=128, bias=False)
          (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
          (act): SiLU()
          (x_proj): Linear(in_features=64, out_features=34, bias=False)
          (dt_proj): Linear(in_features=2, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=32, bias=False)
        )
      )
      (1): MambaLayer(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=64, out_features=256, bias=False)
          (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
          (act): SiLU()
          (x_proj): Linear(in_features=128, out_features=36, bias=False)
          (dt_proj): Linear(in_features=4, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=64, bias=False)
        )
      )
      (2-3): 2 x MambaLayer(
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=128, out_features=512, bias=False)
          (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
          (act): SiLU()
          (x_proj): Linear(in_features=256, out_features=40, bias=False)
          (dt_proj): Linear(in_features=8, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=128, bias=False)
        )
      )
      (4): MambaLayer(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=64, out_features=256, bias=False)
          (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
          (act): SiLU()
          (x_proj): Linear(in_features=128, out_features=36, bias=False)
          (dt_proj): Linear(in_features=4, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=64, bias=False)
        )
      )
      (5): MambaLayer(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=32, out_features=128, bias=False)
          (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
          (act): SiLU()
          (x_proj): Linear(in_features=64, out_features=34, bias=False)
          (dt_proj): Linear(in_features=2, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=32, bias=False)
        )
      )
    )
  )
  (decoder): UNetResDecoder(
    (encoder): ResidualMambaEncoder(
      (stem): StackedConvBlocks(
        (convs): Sequential(
          (0): ConvDropoutNormReLU(
            (conv): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
            (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
            (all_modules): Sequential(
              (0): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (stages): ModuleList(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (4): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (5): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (mamba_layers): ModuleList(
        (0): MambaLayer(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=32, out_features=128, bias=False)
            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
            (act): SiLU()
            (x_proj): Linear(in_features=64, out_features=34, bias=False)
            (dt_proj): Linear(in_features=2, out_features=64, bias=True)
            (out_proj): Linear(in_features=64, out_features=32, bias=False)
          )
        )
        (1): MambaLayer(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (2-3): 2 x MambaLayer(
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (4): MambaLayer(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (5): MambaLayer(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=32, out_features=128, bias=False)
            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
            (act): SiLU()
            (x_proj): Linear(in_features=64, out_features=34, bias=False)
            (dt_proj): Linear(in_features=2, out_features=64, bias=True)
            (out_proj): Linear(in_features=64, out_features=32, bias=False)
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1-2): 2 x StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (transpconvs): ModuleList(
      (0): ConvTranspose3d(32, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (1): ConvTranspose3d(64, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (2): ConvTranspose3d(128, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (3): ConvTranspose3d(128, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (4): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2))
    )
    (lzz_layers): ModuleList(
      (0): ModuleList(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1-2): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (1): ModuleList(
        (0-1): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2-3): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (2): ModuleList(
        (0-2): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (3): ModuleList(
        (0-2): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (4): ModuleList(
        (0-1): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2-4): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
    )
    (lzz_fc): ModuleList(
      (0-4): 5 x ModuleList(
        (0): Sequential(
          (0): Linear(in_features=288, out_features=72, bias=True)
          (1): Tanh()
        )
        (1): Sequential(
          (0): Linear(in_features=72, out_features=18, bias=True)
          (1): Tanh()
        )
        (2): Sequential(
          (0): Linear(in_features=18, out_features=1, bias=True)
          (1): Tanh()
        )
      )
    )
  )
)

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [40, 192, 192], 'median_image_size_in_voxels': [97.0, 512.0, 512.0], 'spacing': [2.5, 0.7958984971046448, 0.7958984971046448], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [3, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset701_AbdomenCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.7958984971046448, 0.7958984971046448], 'original_median_shape_after_transp': [97, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3071.0, 'mean': 97.29716491699219, 'median': 118.0, 'min': -1024.0, 'percentile_00_5': -958.0, 'percentile_99_5': 270.0, 'std': 137.8484649658203}}} 

2024-06-27 09:15:12.283615: unpacking dataset...
2024-06-27 09:15:12.284065: unpacking done...
2024-06-27 09:15:12.284854: do_dummy_2d_data_aug: False
2024-06-27 09:15:12.299780: Unable to plot network architecture:
2024-06-27 09:15:12.300269: No module named 'hiddenlayer'
2024-06-27 09:15:12.310764: 
2024-06-27 09:15:12.311389: Epoch 0
2024-06-27 09:15:12.311863: Current learning rate: 0.001
using pin_memory on device 0
using pin_memory on device 0
2024-06-27 09:16:33.245535: meanmse:       0.10560134
2024-06-27 09:16:33.246880: meanr2:        0.156597624043353
2024-06-27 09:16:33.247548: train_loss 2.6426
2024-06-27 09:16:33.248017: val_loss 2.0966
2024-06-27 09:16:33.248433: Pseudo dice [0.5]
2024-06-27 09:16:33.249102: Epoch time: 80.94 s
2024-06-27 09:16:33.249616: Yayy! New best R2: 0.1566
2024-06-27 09:16:35.576164: 
2024-06-27 09:16:35.576839: Epoch 1
2024-06-27 09:16:35.577221: Current learning rate: 0.00099
2024-06-27 09:17:39.563519: meanmse:       0.081432045
2024-06-27 09:17:39.565456: meanr2:        0.3222376496956442
2024-06-27 09:17:39.567986: train_loss 2.0579
2024-06-27 09:17:39.568450: val_loss 1.8547
2024-06-27 09:17:39.568892: Pseudo dice [0.5]
2024-06-27 09:17:39.569352: Epoch time: 64.0 s
2024-06-27 09:17:39.569813: Yayy! New best R2: 0.3222
2024-06-27 09:17:41.863621: 
2024-06-27 09:17:41.864247: Epoch 2
2024-06-27 09:17:41.864650: Current learning rate: 0.00098
2024-06-27 09:18:46.032919: meanmse:       0.057595227
2024-06-27 09:18:46.034216: meanr2:        0.538318483250936
2024-06-27 09:18:46.034835: train_loss 1.7677
2024-06-27 09:18:46.038094: val_loss 1.5034
2024-06-27 09:18:46.038740: Pseudo dice [0.5]
2024-06-27 09:18:46.039271: Epoch time: 64.18 s
2024-06-27 09:18:46.039754: Yayy! New best R2: 0.5383
2024-06-27 09:18:48.339167: 
2024-06-27 09:18:48.339958: Epoch 3
2024-06-27 09:18:48.340661: Current learning rate: 0.00097
2024-06-27 09:19:52.535131: meanmse:       0.044138905
2024-06-27 09:19:52.536476: meanr2:        0.6375441056666554
2024-06-27 09:19:52.537195: train_loss 1.3401
2024-06-27 09:19:52.537660: val_loss 1.2397
2024-06-27 09:19:52.538098: Pseudo dice [0.5]
2024-06-27 09:19:52.538559: Epoch time: 64.21 s
2024-06-27 09:19:52.538954: Yayy! New best R2: 0.6375
2024-06-27 09:19:54.818370: 
2024-06-27 09:19:54.819590: Epoch 4
2024-06-27 09:19:54.820144: Current learning rate: 0.00096
2024-06-27 09:20:59.391791: meanmse:       0.05027692
2024-06-27 09:20:59.393087: meanr2:        0.5877215617920646
2024-06-27 09:20:59.393612: train_loss 1.3111
2024-06-27 09:20:59.394041: val_loss 1.3067
2024-06-27 09:20:59.394460: Pseudo dice [0.5]
2024-06-27 09:20:59.394886: Epoch time: 64.58 s
2024-06-27 09:21:01.776152: 
2024-06-27 09:21:01.776860: Epoch 5
2024-06-27 09:21:01.777335: Current learning rate: 0.00095
2024-06-27 09:22:06.100584: meanmse:       0.033094283
2024-06-27 09:22:06.101700: meanr2:        0.7336999154463352
2024-06-27 09:22:06.102154: train_loss 1.1405
2024-06-27 09:22:06.102561: val_loss 1.0057
2024-06-27 09:22:06.103054: Pseudo dice [0.5]
2024-06-27 09:22:06.103559: Epoch time: 64.33 s
2024-06-27 09:22:06.104049: Yayy! New best R2: 0.7337
2024-06-27 09:22:08.285873: 
2024-06-27 09:22:08.286591: Epoch 6
2024-06-27 09:22:08.287069: Current learning rate: 0.00095
2024-06-27 09:23:12.697577: meanmse:       0.036817525
2024-06-27 09:23:12.698783: meanr2:        0.7037545725098167
2024-06-27 09:23:12.699319: train_loss 1.0429
2024-06-27 09:23:12.700005: val_loss 1.0607
2024-06-27 09:23:12.700511: Pseudo dice [0.5]
2024-06-27 09:23:12.701043: Epoch time: 64.42 s
2024-06-27 09:23:14.665711: 
2024-06-27 09:23:14.666684: Epoch 7
2024-06-27 09:23:14.667390: Current learning rate: 0.00094
2024-06-27 09:24:18.813944: meanmse:       0.033392955
2024-06-27 09:24:18.814938: meanr2:        0.722339484401679
2024-06-27 09:24:18.815404: train_loss 0.9824
2024-06-27 09:24:18.815847: val_loss 0.9815
2024-06-27 09:24:18.816226: Pseudo dice [0.5]
2024-06-27 09:24:18.816614: Epoch time: 64.16 s
2024-06-27 09:24:21.180601: 
2024-06-27 09:24:21.181594: Epoch 8
2024-06-27 09:24:21.182080: Current learning rate: 0.00093
2024-06-27 09:25:25.347076: meanmse:       0.03504673
2024-06-27 09:25:25.348260: meanr2:        0.7184231243018772
2024-06-27 09:25:25.348814: train_loss 0.9727
2024-06-27 09:25:25.349277: val_loss 1.0123
2024-06-27 09:25:25.349708: Pseudo dice [0.5]
2024-06-27 09:25:25.350118: Epoch time: 64.18 s
2024-06-27 09:25:27.416801: 
2024-06-27 09:25:27.417634: Epoch 9
2024-06-27 09:25:27.418270: Current learning rate: 0.00092
2024-06-27 09:26:31.804069: meanmse:       0.03044696
2024-06-27 09:26:31.805010: meanr2:        0.7547041125352368
2024-06-27 09:26:31.805470: train_loss 0.9944
2024-06-27 09:26:31.805824: val_loss 0.9109
2024-06-27 09:26:31.806182: Pseudo dice [0.5]
2024-06-27 09:26:31.806529: Epoch time: 64.4 s
2024-06-27 09:26:32.073697: Yayy! New best R2: 0.7547
2024-06-27 09:26:34.237116: 
2024-06-27 09:26:34.237784: Epoch 10
2024-06-27 09:26:34.238327: Current learning rate: 0.00091
2024-06-27 09:27:38.565825: meanmse:       0.034356672
2024-06-27 09:27:38.567377: meanr2:        0.7301526409148803
2024-06-27 09:27:38.567959: train_loss 0.8722
2024-06-27 09:27:38.568599: val_loss 0.9455
2024-06-27 09:27:38.571351: Pseudo dice [0.5]
2024-06-27 09:27:38.571886: Epoch time: 64.34 s
2024-06-27 09:27:40.549910: 
2024-06-27 09:27:40.550707: Epoch 11
2024-06-27 09:27:40.551244: Current learning rate: 0.0009
2024-06-27 09:28:44.967998: meanmse:       0.025037102
2024-06-27 09:28:44.969290: meanr2:        0.7957839730205312
2024-06-27 09:28:44.969879: train_loss 0.8559
2024-06-27 09:28:44.970344: val_loss 0.7613
2024-06-27 09:28:44.970861: Pseudo dice [0.5]
2024-06-27 09:28:44.971448: Epoch time: 64.43 s
2024-06-27 09:28:44.971881: Yayy! New best R2: 0.7958
2024-06-27 09:28:47.251859: 
2024-06-27 09:28:47.252716: Epoch 12
2024-06-27 09:28:47.253185: Current learning rate: 0.00089
2024-06-27 09:29:51.645673: meanmse:       0.029453862
2024-06-27 09:29:51.646813: meanr2:        0.7580721181374817
2024-06-27 09:29:51.647327: train_loss 0.8458
2024-06-27 09:29:51.647810: val_loss 0.867
2024-06-27 09:29:51.648320: Pseudo dice [0.5]
2024-06-27 09:29:51.649070: Epoch time: 64.4 s
2024-06-27 09:29:54.060199: 
2024-06-27 09:29:54.061235: Epoch 13
2024-06-27 09:29:54.061866: Current learning rate: 0.00088
2024-06-27 09:30:58.194925: meanmse:       0.026636845
2024-06-27 09:30:58.195909: meanr2:        0.7824168029128856
2024-06-27 09:30:58.196339: train_loss 0.8235
2024-06-27 09:30:58.196683: val_loss 0.8012
2024-06-27 09:30:58.197015: Pseudo dice [0.5]
2024-06-27 09:30:58.197402: Epoch time: 64.15 s
2024-06-27 09:31:00.137294: 
2024-06-27 09:31:00.138299: Epoch 14
2024-06-27 09:31:00.138870: Current learning rate: 0.00087
2024-06-27 09:32:04.458099: meanmse:       0.029493278
2024-06-27 09:32:04.459592: meanr2:        0.7605690657911431
2024-06-27 09:32:04.460294: train_loss 0.8092
2024-06-27 09:32:04.460829: val_loss 0.8485
2024-06-27 09:32:04.461310: Pseudo dice [0.5]
2024-06-27 09:32:04.461803: Epoch time: 64.33 s
2024-06-27 09:32:06.484390: 
2024-06-27 09:32:06.485248: Epoch 15
2024-06-27 09:32:06.485851: Current learning rate: 0.00086
2024-06-27 09:33:10.728538: meanmse:       0.026513088
2024-06-27 09:33:10.729769: meanr2:        0.7893450937487209
2024-06-27 09:33:10.730336: train_loss 0.8121
2024-06-27 09:33:10.730784: val_loss 0.7751
2024-06-27 09:33:10.731241: Pseudo dice [0.5]
2024-06-27 09:33:10.731689: Epoch time: 64.25 s
2024-06-27 09:33:13.051662: 
2024-06-27 09:33:13.052798: Epoch 16
2024-06-27 09:33:13.053514: Current learning rate: 0.00085
2024-06-27 09:34:17.182748: meanmse:       0.03831523
2024-06-27 09:34:17.183806: meanr2:        0.6836718213550829
2024-06-27 09:34:17.184318: train_loss 0.7547
2024-06-27 09:34:17.184677: val_loss 0.9946
2024-06-27 09:34:17.185029: Pseudo dice [0.5]
2024-06-27 09:34:17.185367: Epoch time: 64.14 s
2024-06-27 09:34:19.188261: 
2024-06-27 09:34:19.188932: Epoch 17
2024-06-27 09:34:19.189399: Current learning rate: 0.00085
2024-06-27 09:35:23.213638: meanmse:       0.023227481
2024-06-27 09:35:23.214645: meanr2:        0.8148436718970818
2024-06-27 09:35:23.215098: train_loss 0.786
2024-06-27 09:35:23.215481: val_loss 0.7136
2024-06-27 09:35:23.215847: Pseudo dice [0.5]
2024-06-27 09:35:23.216216: Epoch time: 64.03 s
2024-06-27 09:35:23.216542: Yayy! New best R2: 0.8148
2024-06-27 09:35:25.571527: 
2024-06-27 09:35:25.572376: Epoch 18
2024-06-27 09:35:25.572940: Current learning rate: 0.00084
2024-06-27 09:36:29.987344: meanmse:       0.026363315
2024-06-27 09:36:29.988535: meanr2:        0.786858948425174
2024-06-27 09:36:29.989354: train_loss 0.7272
2024-06-27 09:36:29.989942: val_loss 0.7751
2024-06-27 09:36:29.990412: Pseudo dice [0.5]
2024-06-27 09:36:29.990858: Epoch time: 64.43 s
2024-06-27 09:36:32.371258: 
2024-06-27 09:36:32.372059: Epoch 19
2024-06-27 09:36:32.372625: Current learning rate: 0.00083
2024-06-27 09:37:36.505348: meanmse:       0.027312648
2024-06-27 09:37:36.506439: meanr2:        0.7698569352495004
2024-06-27 09:37:36.507019: train_loss 0.7854
2024-06-27 09:37:36.507479: val_loss 0.7866
2024-06-27 09:37:36.507921: Pseudo dice [0.5]
2024-06-27 09:37:36.508358: Epoch time: 64.14 s
2024-06-27 09:37:38.740867: 
2024-06-27 09:37:38.741776: Epoch 20
2024-06-27 09:37:38.742347: Current learning rate: 0.00082
2024-06-27 09:38:42.807637: meanmse:       0.023308786
2024-06-27 09:38:42.808817: meanr2:        0.8098473749347921
2024-06-27 09:38:42.809341: train_loss 0.6864
2024-06-27 09:38:42.809780: val_loss 0.6924
2024-06-27 09:38:42.810211: Pseudo dice [0.5]
2024-06-27 09:38:42.810642: Epoch time: 64.07 s
2024-06-27 09:38:44.765407: 
2024-06-27 09:38:44.766223: Epoch 21
2024-06-27 09:38:44.766786: Current learning rate: 0.00081
2024-06-27 09:39:48.910335: meanmse:       0.01738266
2024-06-27 09:39:48.911493: meanr2:        0.8550372080075916
2024-06-27 09:39:48.911975: train_loss 0.6304
2024-06-27 09:39:48.912391: val_loss 0.5736
2024-06-27 09:39:48.912792: Pseudo dice [0.5]
2024-06-27 09:39:48.913252: Epoch time: 64.15 s
2024-06-27 09:39:48.913665: Yayy! New best R2: 0.855
2024-06-27 09:39:51.382242: 
2024-06-27 09:39:51.382792: Epoch 22
2024-06-27 09:39:51.383190: Current learning rate: 0.0008
2024-06-27 09:40:55.725429: meanmse:       0.023928536
2024-06-27 09:40:55.726823: meanr2:        0.8108368186867037
2024-06-27 09:40:55.727482: train_loss 0.6587
2024-06-27 09:40:55.727969: val_loss 0.7114
2024-06-27 09:40:55.728428: Pseudo dice [0.5]
2024-06-27 09:40:55.728937: Epoch time: 64.35 s
2024-06-27 09:40:57.601819: 
2024-06-27 09:40:57.602495: Epoch 23
2024-06-27 09:40:57.602963: Current learning rate: 0.00079
2024-06-27 09:42:01.807255: meanmse:       0.02541464
2024-06-27 09:42:01.808304: meanr2:        0.7890743726459852
2024-06-27 09:42:01.808789: train_loss 0.6782
2024-06-27 09:42:01.809174: val_loss 0.7551
2024-06-27 09:42:01.809533: Pseudo dice [0.5]
2024-06-27 09:42:01.809903: Epoch time: 64.21 s
2024-06-27 09:42:03.744603: 
2024-06-27 09:42:03.745388: Epoch 24
2024-06-27 09:42:03.745865: Current learning rate: 0.00078
2024-06-27 09:43:08.202272: meanmse:       0.027267171
2024-06-27 09:43:08.203456: meanr2:        0.777009266266618
2024-06-27 09:43:08.203967: train_loss 0.7201
2024-06-27 09:43:08.204390: val_loss 0.7583
2024-06-27 09:43:08.204832: Pseudo dice [0.5]
2024-06-27 09:43:08.205269: Epoch time: 64.47 s
2024-06-27 09:43:10.164903: 
2024-06-27 09:43:10.165937: Epoch 25
2024-06-27 09:43:10.166513: Current learning rate: 0.00077
2024-06-27 09:44:14.434270: meanmse:       0.021930795
2024-06-27 09:44:14.436076: meanr2:        0.8238797414007852
2024-06-27 09:44:14.436931: train_loss 0.6842
2024-06-27 09:44:14.437497: val_loss 0.6566
2024-06-27 09:44:14.438073: Pseudo dice [0.5]
2024-06-27 09:44:14.438609: Epoch time: 64.28 s
2024-06-27 09:44:16.435374: 
2024-06-27 09:44:16.438911: Epoch 26
2024-06-27 09:44:16.439710: Current learning rate: 0.00076
2024-06-27 09:45:20.605830: meanmse:       0.022691278
2024-06-27 09:45:20.606947: meanr2:        0.8147528255395028
2024-06-27 09:45:20.607545: train_loss 0.6068
2024-06-27 09:45:20.607994: val_loss 0.6982
2024-06-27 09:45:20.608445: Pseudo dice [0.5]
2024-06-27 09:45:20.608903: Epoch time: 64.18 s
2024-06-27 09:45:22.718860: 
2024-06-27 09:45:22.719839: Epoch 27
2024-06-27 09:45:22.720552: Current learning rate: 0.00075
2024-06-27 09:46:27.311298: meanmse:       0.03255976
2024-06-27 09:46:27.312360: meanr2:        0.7357568830090133
2024-06-27 09:46:27.312891: train_loss 0.6666
2024-06-27 09:46:27.313339: val_loss 0.8684
2024-06-27 09:46:27.313798: Pseudo dice [0.5]
2024-06-27 09:46:27.314280: Epoch time: 64.6 s
2024-06-27 09:46:29.294908: 
2024-06-27 09:46:29.295702: Epoch 28
2024-06-27 09:46:29.296243: Current learning rate: 0.00074
2024-06-27 09:47:33.915551: meanmse:       0.026198924
2024-06-27 09:47:33.917067: meanr2:        0.7871503906323437
2024-06-27 09:47:33.917896: train_loss 0.6443
2024-06-27 09:47:33.918539: val_loss 0.7473
2024-06-27 09:47:33.919117: Pseudo dice [0.5]
2024-06-27 09:47:33.919661: Epoch time: 64.63 s
2024-06-27 09:47:35.756206: 
2024-06-27 09:47:35.757016: Epoch 29
2024-06-27 09:47:35.757648: Current learning rate: 0.00073
2024-06-27 09:48:40.472542: meanmse:       0.026237767
2024-06-27 09:48:40.474059: meanr2:        0.7926449308143724
2024-06-27 09:48:40.474797: train_loss 0.601
2024-06-27 09:48:40.475362: val_loss 0.7655
2024-06-27 09:48:40.475818: Pseudo dice [0.5]
2024-06-27 09:48:40.476268: Epoch time: 64.73 s
2024-06-27 09:48:42.773806: 
2024-06-27 09:48:42.774416: Epoch 30
2024-06-27 09:48:42.774824: Current learning rate: 0.00073
2024-06-27 09:49:47.226739: meanmse:       0.021835547
2024-06-27 09:49:47.228059: meanr2:        0.8214210140277757
2024-06-27 09:49:47.228748: train_loss 0.5779
2024-06-27 09:49:47.229270: val_loss 0.6525
2024-06-27 09:49:47.229777: Pseudo dice [0.5]
2024-06-27 09:49:47.230343: Epoch time: 64.46 s
2024-06-27 09:49:49.228344: 
2024-06-27 09:49:49.229152: Epoch 31
2024-06-27 09:49:49.229794: Current learning rate: 0.00072
2024-06-27 09:50:53.371389: meanmse:       0.019248324
2024-06-27 09:50:53.372548: meanr2:        0.843285643870712
2024-06-27 09:50:53.373045: train_loss 0.6099
2024-06-27 09:50:53.373449: val_loss 0.6253
2024-06-27 09:50:53.373901: Pseudo dice [0.5]
2024-06-27 09:50:53.374332: Epoch time: 64.15 s
2024-06-27 09:50:55.210891: 
2024-06-27 09:50:55.211648: Epoch 32
2024-06-27 09:50:55.212168: Current learning rate: 0.00071
2024-06-27 09:51:59.370961: meanmse:       0.017639698
2024-06-27 09:51:59.371858: meanr2:        0.8551296607714363
2024-06-27 09:51:59.372291: train_loss 0.5747
2024-06-27 09:51:59.372643: val_loss 0.5828
2024-06-27 09:51:59.372986: Pseudo dice [0.5]
2024-06-27 09:51:59.373314: Epoch time: 64.17 s
2024-06-27 09:51:59.373631: Yayy! New best R2: 0.8551
2024-06-27 09:52:01.714241: 
2024-06-27 09:52:01.714840: Epoch 33
2024-06-27 09:52:01.715287: Current learning rate: 0.0007
2024-06-27 09:53:05.853935: meanmse:       0.01691943
2024-06-27 09:53:05.855041: meanr2:        0.8641684343332624
2024-06-27 09:53:05.855527: train_loss 0.5716
2024-06-27 09:53:05.855917: val_loss 0.5524
2024-06-27 09:53:05.856275: Pseudo dice [0.5]
2024-06-27 09:53:05.856631: Epoch time: 64.15 s
2024-06-27 09:53:05.856991: Yayy! New best R2: 0.8642
2024-06-27 09:53:08.097785: 
2024-06-27 09:53:08.098475: Epoch 34
2024-06-27 09:53:08.098895: Current learning rate: 0.00069
2024-06-27 09:54:12.358144: meanmse:       0.02007112
2024-06-27 09:54:12.359114: meanr2:        0.8353040085025486
2024-06-27 09:54:12.359564: train_loss 0.5772
2024-06-27 09:54:12.359940: val_loss 0.6021
2024-06-27 09:54:12.360289: Pseudo dice [0.5]
2024-06-27 09:54:12.360634: Epoch time: 64.27 s
2024-06-27 09:54:14.258258: 
2024-06-27 09:54:14.259022: Epoch 35
2024-06-27 09:54:14.259537: Current learning rate: 0.00068
2024-06-27 09:55:18.589519: meanmse:       0.020837821
2024-06-27 09:55:18.590597: meanr2:        0.8304554317364944
2024-06-27 09:55:18.591192: train_loss 0.5601
2024-06-27 09:55:18.591651: val_loss 0.6399
2024-06-27 09:55:18.592103: Pseudo dice [0.5]
2024-06-27 09:55:18.592528: Epoch time: 64.34 s
2024-06-27 09:55:20.541412: 
2024-06-27 09:55:20.542457: Epoch 36
2024-06-27 09:55:20.543105: Current learning rate: 0.00067
2024-06-27 09:56:24.976669: meanmse:       0.023444736
2024-06-27 09:56:24.977890: meanr2:        0.8105882914745787
2024-06-27 09:56:24.978424: train_loss 0.5746
2024-06-27 09:56:24.978831: val_loss 0.6622
2024-06-27 09:56:24.979247: Pseudo dice [0.5]
2024-06-27 09:56:24.979624: Epoch time: 64.46 s
2024-06-27 09:56:26.982656: 
2024-06-27 09:56:26.983534: Epoch 37
2024-06-27 09:56:26.984118: Current learning rate: 0.00066
2024-06-27 09:57:31.134568: meanmse:       0.020284442
2024-06-27 09:57:31.136394: meanr2:        0.832621619809142
2024-06-27 09:57:31.137103: train_loss 0.5549
2024-06-27 09:57:31.137700: val_loss 0.6015
2024-06-27 09:57:31.138314: Pseudo dice [0.5]
2024-06-27 09:57:31.138837: Epoch time: 64.16 s
2024-06-27 09:57:33.181123: 
2024-06-27 09:57:33.181849: Epoch 38
2024-06-27 09:57:33.182346: Current learning rate: 0.00065
2024-06-27 09:58:37.573249: meanmse:       0.020275394
2024-06-27 09:58:37.574250: meanr2:        0.8388810212351298
2024-06-27 09:58:37.574805: train_loss 0.5763
2024-06-27 09:58:37.575274: val_loss 0.6138
2024-06-27 09:58:37.575715: Pseudo dice [0.5]
2024-06-27 09:58:37.576160: Epoch time: 64.4 s
2024-06-27 09:58:39.885072: 
2024-06-27 09:58:39.885786: Epoch 39
2024-06-27 09:58:39.886308: Current learning rate: 0.00064
2024-06-27 09:59:44.001086: meanmse:       0.023731232
2024-06-27 09:59:44.002265: meanr2:        0.8045092708821454
2024-06-27 09:59:44.002894: train_loss 0.5696
2024-06-27 09:59:44.003357: val_loss 0.6633
2024-06-27 09:59:44.003817: Pseudo dice [0.5]
2024-06-27 09:59:44.004271: Epoch time: 64.12 s
2024-06-27 09:59:46.317839: 
2024-06-27 09:59:46.318638: Epoch 40
2024-06-27 09:59:46.319253: Current learning rate: 0.00063
2024-06-27 10:00:50.772802: meanmse:       0.024695868
2024-06-27 10:00:50.774209: meanr2:        0.7993613817836046
2024-06-27 10:00:50.774847: train_loss 0.5542
2024-06-27 10:00:50.775509: val_loss 0.7048
2024-06-27 10:00:50.776170: Pseudo dice [0.5]
2024-06-27 10:00:50.776684: Epoch time: 64.46 s
2024-06-27 10:00:52.762443: 
2024-06-27 10:00:52.763693: Epoch 41
2024-06-27 10:00:52.764325: Current learning rate: 0.00062
2024-06-27 10:01:56.711330: meanmse:       0.022283254
2024-06-27 10:01:56.712356: meanr2:        0.819676419345177
2024-06-27 10:01:56.712818: train_loss 0.6007
2024-06-27 10:01:56.713227: val_loss 0.644
2024-06-27 10:01:56.713602: Pseudo dice [0.5]
2024-06-27 10:01:56.714027: Epoch time: 63.96 s
2024-06-27 10:01:58.803184: 
2024-06-27 10:01:58.803778: Epoch 42
2024-06-27 10:01:58.806844: Current learning rate: 0.00061
2024-06-27 10:03:02.607958: meanmse:       0.020875653
2024-06-27 10:03:02.609028: meanr2:        0.8332692628419601
2024-06-27 10:03:02.609564: train_loss 0.5215
2024-06-27 10:03:02.610016: val_loss 0.6272
2024-06-27 10:03:02.610443: Pseudo dice [0.5]
2024-06-27 10:03:02.610874: Epoch time: 63.81 s
2024-06-27 10:03:04.512429: 
2024-06-27 10:03:04.513001: Epoch 43
2024-06-27 10:03:04.513410: Current learning rate: 0.0006
2024-06-27 10:04:08.974676: meanmse:       0.020184228
2024-06-27 10:04:08.975720: meanr2:        0.8396054793206322
2024-06-27 10:04:08.976187: train_loss 0.5519
2024-06-27 10:04:08.976563: val_loss 0.6033
2024-06-27 10:04:08.976894: Pseudo dice [0.5]
2024-06-27 10:04:08.977280: Epoch time: 64.47 s
2024-06-27 10:04:10.876746: 
2024-06-27 10:04:10.877408: Epoch 44
2024-06-27 10:04:10.877902: Current learning rate: 0.00059
2024-06-27 10:05:15.477108: meanmse:       0.022564804
2024-06-27 10:05:15.478277: meanr2:        0.8161663140044105
2024-06-27 10:05:15.478790: train_loss 0.5654
2024-06-27 10:05:15.479239: val_loss 0.6445
2024-06-27 10:05:15.479645: Pseudo dice [0.5]
2024-06-27 10:05:15.480072: Epoch time: 64.61 s
2024-06-27 10:05:17.557086: 
2024-06-27 10:05:17.557956: Epoch 45
2024-06-27 10:05:17.558462: Current learning rate: 0.00058
2024-06-27 10:06:21.820526: meanmse:       0.028596027
2024-06-27 10:06:21.821786: meanr2:        0.7701950190558716
2024-06-27 10:06:21.822371: train_loss 0.525
2024-06-27 10:06:21.822761: val_loss 0.7627
2024-06-27 10:06:21.823129: Pseudo dice [0.5]
2024-06-27 10:06:21.823555: Epoch time: 64.27 s
2024-06-27 10:06:23.764531: 
2024-06-27 10:06:23.765269: Epoch 46
2024-06-27 10:06:23.765780: Current learning rate: 0.00057
2024-06-27 10:07:28.103350: meanmse:       0.022459922
2024-06-27 10:07:28.104666: meanr2:        0.8188368229779662
2024-06-27 10:07:28.105378: train_loss 0.5774
2024-06-27 10:07:28.105921: val_loss 0.6394
2024-06-27 10:07:28.106436: Pseudo dice [0.5]
2024-06-27 10:07:28.107005: Epoch time: 64.35 s
2024-06-27 10:07:29.989466: 
2024-06-27 10:07:29.990332: Epoch 47
2024-06-27 10:07:29.990900: Current learning rate: 0.00056
2024-06-27 10:08:34.480128: meanmse:       0.025395064
2024-06-27 10:08:34.481081: meanr2:        0.7977861392170594
2024-06-27 10:08:34.481559: train_loss 0.5177
2024-06-27 10:08:34.481922: val_loss 0.698
2024-06-27 10:08:34.482253: Pseudo dice [0.5]
2024-06-27 10:08:34.482604: Epoch time: 64.5 s
2024-06-27 10:08:36.439477: 
2024-06-27 10:08:36.440447: Epoch 48
2024-06-27 10:08:36.441029: Current learning rate: 0.00056
2024-06-27 10:09:40.455048: meanmse:       0.020728046
2024-06-27 10:09:40.456583: meanr2:        0.8319263490738961
2024-06-27 10:09:40.457289: train_loss 0.5357
2024-06-27 10:09:40.457823: val_loss 0.6138
2024-06-27 10:09:40.458274: Pseudo dice [0.5]
2024-06-27 10:09:40.458731: Epoch time: 64.02 s
2024-06-27 10:09:42.346295: 
2024-06-27 10:09:42.347026: Epoch 49
2024-06-27 10:09:42.347503: Current learning rate: 0.00055
2024-06-27 10:10:46.409225: meanmse:       0.019340783
2024-06-27 10:10:46.410416: meanr2:        0.8450675973040497
2024-06-27 10:10:46.410999: train_loss 0.5343
2024-06-27 10:10:46.411453: val_loss 0.599
2024-06-27 10:10:46.411885: Pseudo dice [0.5]
2024-06-27 10:10:46.412346: Epoch time: 64.07 s
2024-06-27 10:10:48.725780: 
2024-06-27 10:10:48.726956: Epoch 50
2024-06-27 10:10:48.727660: Current learning rate: 0.00054
2024-06-27 10:11:52.930613: meanmse:       0.023352258
2024-06-27 10:11:52.931973: meanr2:        0.8124861422230484
2024-06-27 10:11:52.932708: train_loss 0.5138
2024-06-27 10:11:52.933248: val_loss 0.6474
2024-06-27 10:11:52.934101: Pseudo dice [0.5]
2024-06-27 10:11:52.934734: Epoch time: 64.22 s
2024-06-27 10:11:55.101820: 
2024-06-27 10:11:55.102588: Epoch 51
2024-06-27 10:11:55.103145: Current learning rate: 0.00053
2024-06-27 10:12:58.989505: meanmse:       0.018666351
2024-06-27 10:12:58.990614: meanr2:        0.8494927944756263
2024-06-27 10:12:58.991082: train_loss 0.5489
2024-06-27 10:12:58.991424: val_loss 0.5654
2024-06-27 10:12:58.991758: Pseudo dice [0.5]
2024-06-27 10:12:58.992229: Epoch time: 63.9 s
2024-06-27 10:13:00.987781: 
2024-06-27 10:13:00.988413: Epoch 52
2024-06-27 10:13:00.988908: Current learning rate: 0.00052
2024-06-27 10:14:04.627921: meanmse:       0.021741485
2024-06-27 10:14:04.629336: meanr2:        0.8242156184580017
2024-06-27 10:14:04.629993: train_loss 0.5214
2024-06-27 10:14:04.630489: val_loss 0.6336
2024-06-27 10:14:04.630937: Pseudo dice [0.5]
2024-06-27 10:14:04.631405: Epoch time: 63.65 s
2024-06-27 10:14:06.561270: 
2024-06-27 10:14:06.562152: Epoch 53
2024-06-27 10:14:06.562635: Current learning rate: 0.00051
2024-06-27 10:15:10.401469: meanmse:       0.018494027
2024-06-27 10:15:10.402407: meanr2:        0.8492022460518768
2024-06-27 10:15:10.402817: train_loss 0.4813
2024-06-27 10:15:10.403168: val_loss 0.5571
2024-06-27 10:15:10.403544: Pseudo dice [0.5]
2024-06-27 10:15:10.403910: Epoch time: 63.85 s
2024-06-27 10:15:12.707541: 
2024-06-27 10:15:12.708486: Epoch 54
2024-06-27 10:15:12.709087: Current learning rate: 0.0005
2024-06-27 10:16:16.808443: meanmse:       0.021347772
2024-06-27 10:16:16.809794: meanr2:        0.8308685781709283
2024-06-27 10:16:16.810390: train_loss 0.4843
2024-06-27 10:16:16.810904: val_loss 0.6148
2024-06-27 10:16:16.811403: Pseudo dice [0.5]
2024-06-27 10:16:16.811938: Epoch time: 64.11 s
2024-06-27 10:16:18.718657: 
2024-06-27 10:16:18.719572: Epoch 55
2024-06-27 10:16:18.720187: Current learning rate: 0.00049
2024-06-27 10:17:22.775052: meanmse:       0.017638426
2024-06-27 10:17:22.776071: meanr2:        0.8584796394424999
2024-06-27 10:17:22.776501: train_loss 0.52
2024-06-27 10:17:22.776885: val_loss 0.5496
2024-06-27 10:17:22.777246: Pseudo dice [0.5]
2024-06-27 10:17:22.777603: Epoch time: 64.06 s
2024-06-27 10:17:24.592945: 
2024-06-27 10:17:24.593576: Epoch 56
2024-06-27 10:17:24.594145: Current learning rate: 0.00048
2024-06-27 10:18:28.509207: meanmse:       0.0236584
2024-06-27 10:18:28.515242: meanr2:        0.8127398053113706
2024-06-27 10:18:28.515823: train_loss 0.506
2024-06-27 10:18:28.516641: val_loss 0.6773
2024-06-27 10:18:28.517143: Pseudo dice [0.5]
2024-06-27 10:18:28.517699: Epoch time: 63.93 s
2024-06-27 10:18:30.454176: 
2024-06-27 10:18:30.455038: Epoch 57
2024-06-27 10:18:30.455618: Current learning rate: 0.00047
2024-06-27 10:19:34.700333: meanmse:       0.0240635
2024-06-27 10:19:34.701403: meanr2:        0.8002605189196997
2024-06-27 10:19:34.701925: train_loss 0.473
2024-06-27 10:19:34.702407: val_loss 0.6707
2024-06-27 10:19:34.702839: Pseudo dice [0.5]
2024-06-27 10:19:34.703338: Epoch time: 64.25 s
2024-06-27 10:19:36.688151: 
2024-06-27 10:19:36.688936: Epoch 58
2024-06-27 10:19:36.689494: Current learning rate: 0.00046
2024-06-27 10:20:40.978338: meanmse:       0.023013592
2024-06-27 10:20:40.979506: meanr2:        0.8131122161943556
2024-06-27 10:20:40.980070: train_loss 0.4712
2024-06-27 10:20:40.980708: val_loss 0.6443
2024-06-27 10:20:40.981170: Pseudo dice [0.5]
2024-06-27 10:20:40.981617: Epoch time: 64.3 s
2024-06-27 10:20:42.946496: 
2024-06-27 10:20:42.947463: Epoch 59
2024-06-27 10:20:42.948132: Current learning rate: 0.00045
2024-06-27 10:21:46.937428: meanmse:       0.0157864
2024-06-27 10:21:46.938627: meanr2:        0.8716732812877127
2024-06-27 10:21:46.939223: train_loss 0.4899
2024-06-27 10:21:46.939698: val_loss 0.4998
2024-06-27 10:21:46.940189: Pseudo dice [0.5]
2024-06-27 10:21:46.940741: Epoch time: 64.01 s
2024-06-27 10:21:47.268085: Yayy! New best R2: 0.8717
2024-06-27 10:21:49.714190: 
2024-06-27 10:21:49.715119: Epoch 60
2024-06-27 10:21:49.715646: Current learning rate: 0.00044
2024-06-27 10:22:54.053147: meanmse:       0.0213405
2024-06-27 10:22:54.054463: meanr2:        0.8247655191945346
2024-06-27 10:22:54.055147: train_loss 0.4559
2024-06-27 10:22:54.055713: val_loss 0.6073
2024-06-27 10:22:54.056242: Pseudo dice [0.5]
2024-06-27 10:22:54.056858: Epoch time: 64.35 s
2024-06-27 10:22:56.016541: 
2024-06-27 10:22:56.017332: Epoch 61
2024-06-27 10:22:56.017898: Current learning rate: 0.00043
2024-06-27 10:24:00.346488: meanmse:       0.021119712
2024-06-27 10:24:00.347678: meanr2:        0.828941152370777
2024-06-27 10:24:00.348508: train_loss 0.5071
2024-06-27 10:24:00.349071: val_loss 0.6122
2024-06-27 10:24:00.349551: Pseudo dice [0.5]
2024-06-27 10:24:00.350118: Epoch time: 64.34 s
2024-06-27 10:24:02.265308: 
2024-06-27 10:24:02.266337: Epoch 62
2024-06-27 10:24:02.266903: Current learning rate: 0.00042
2024-06-27 10:25:06.379307: meanmse:       0.018158792
2024-06-27 10:25:06.380622: meanr2:        0.8503781300837101
2024-06-27 10:25:06.381255: train_loss 0.4923
2024-06-27 10:25:06.381775: val_loss 0.553
2024-06-27 10:25:06.382282: Pseudo dice [0.5]
2024-06-27 10:25:06.382775: Epoch time: 64.13 s
2024-06-27 10:25:08.340266: 
2024-06-27 10:25:08.342295: Epoch 63
2024-06-27 10:25:08.343235: Current learning rate: 0.00041
2024-06-27 10:26:12.277313: meanmse:       0.018877989
2024-06-27 10:26:12.278465: meanr2:        0.8449201183451969
2024-06-27 10:26:12.279019: train_loss 0.4499
2024-06-27 10:26:12.279510: val_loss 0.5604
2024-06-27 10:26:12.280024: Pseudo dice [0.5]
2024-06-27 10:26:12.280519: Epoch time: 63.95 s
2024-06-27 10:26:14.210341: 
2024-06-27 10:26:14.211107: Epoch 64
2024-06-27 10:26:14.211642: Current learning rate: 0.0004
2024-06-27 10:27:18.391695: meanmse:       0.015952349
2024-06-27 10:27:18.393026: meanr2:        0.8690556221240939
2024-06-27 10:27:18.393794: train_loss 0.4556
2024-06-27 10:27:18.394420: val_loss 0.5128
2024-06-27 10:27:18.395000: Pseudo dice [0.5]
2024-06-27 10:27:18.395599: Epoch time: 64.19 s
2024-06-27 10:27:20.349654: 
2024-06-27 10:27:20.350571: Epoch 65
2024-06-27 10:27:20.351193: Current learning rate: 0.00039
2024-06-27 10:28:24.501325: meanmse:       0.020909868
2024-06-27 10:28:24.502340: meanr2:        0.8312160309239672
2024-06-27 10:28:24.502808: train_loss 0.463
2024-06-27 10:28:24.503201: val_loss 0.5856
2024-06-27 10:28:24.503580: Pseudo dice [0.5]
2024-06-27 10:28:24.504097: Epoch time: 64.16 s
2024-06-27 10:28:26.547022: 
2024-06-27 10:28:26.547959: Epoch 66
2024-06-27 10:28:26.548839: Current learning rate: 0.00038
2024-06-27 10:29:31.022868: meanmse:       0.017342364
2024-06-27 10:29:31.024335: meanr2:        0.8611770843118935
2024-06-27 10:29:31.025029: train_loss 0.439
2024-06-27 10:29:31.025624: val_loss 0.5376
2024-06-27 10:29:31.026188: Pseudo dice [0.5]
2024-06-27 10:29:31.026886: Epoch time: 64.49 s
2024-06-27 10:29:33.177279: 
2024-06-27 10:29:33.178282: Epoch 67
2024-06-27 10:29:33.178802: Current learning rate: 0.00037
2024-06-27 10:30:37.450107: meanmse:       0.019311441
2024-06-27 10:30:37.451483: meanr2:        0.8417604161289168
2024-06-27 10:30:37.452207: train_loss 0.4322
2024-06-27 10:30:37.452864: val_loss 0.5788
2024-06-27 10:30:37.453472: Pseudo dice [0.5]
2024-06-27 10:30:37.454045: Epoch time: 64.28 s
2024-06-27 10:30:39.447207: 
2024-06-27 10:30:39.448108: Epoch 68
2024-06-27 10:30:39.448681: Current learning rate: 0.00036
2024-06-27 10:31:43.475470: meanmse:       0.019362794
2024-06-27 10:31:43.476695: meanr2:        0.84491235994312
2024-06-27 10:31:43.477323: train_loss 0.4703
2024-06-27 10:31:43.477893: val_loss 0.5907
2024-06-27 10:31:43.478516: Pseudo dice [0.5]
2024-06-27 10:31:43.479074: Epoch time: 64.04 s
2024-06-27 10:31:45.500632: 
2024-06-27 10:31:45.501363: Epoch 69
2024-06-27 10:31:45.501922: Current learning rate: 0.00035
2024-06-27 10:32:49.831138: meanmse:       0.018985374
2024-06-27 10:32:49.832272: meanr2:        0.8469516785601111
2024-06-27 10:32:49.832738: train_loss 0.4553
2024-06-27 10:32:49.833131: val_loss 0.5409
2024-06-27 10:32:49.833527: Pseudo dice [0.5]
2024-06-27 10:32:49.833957: Epoch time: 64.34 s
2024-06-27 10:32:52.147291: 
2024-06-27 10:32:52.148055: Epoch 70
2024-06-27 10:32:52.148567: Current learning rate: 0.00034
2024-06-27 10:33:56.097828: meanmse:       0.020412657
2024-06-27 10:33:56.098959: meanr2:        0.8361223258776419
2024-06-27 10:33:56.099459: train_loss 0.4334
2024-06-27 10:33:56.099901: val_loss 0.6026
2024-06-27 10:33:56.100340: Pseudo dice [0.5]
2024-06-27 10:33:56.100788: Epoch time: 63.96 s
2024-06-27 10:33:58.150433: 
2024-06-27 10:33:58.151319: Epoch 71
2024-06-27 10:33:58.151871: Current learning rate: 0.00033
2024-06-27 10:35:02.580357: meanmse:       0.021071
2024-06-27 10:35:02.581405: meanr2:        0.829163358308085
2024-06-27 10:35:02.582067: train_loss 0.4549
2024-06-27 10:35:02.582500: val_loss 0.5977
2024-06-27 10:35:02.582893: Pseudo dice [0.5]
2024-06-27 10:35:02.583302: Epoch time: 64.44 s
2024-06-27 10:35:04.821411: 
2024-06-27 10:35:04.822807: Epoch 72
2024-06-27 10:35:04.823411: Current learning rate: 0.00032
2024-06-27 10:36:09.058654: meanmse:       0.018821787
2024-06-27 10:36:09.059742: meanr2:        0.8463344404607213
2024-06-27 10:36:09.060311: train_loss 0.4751
2024-06-27 10:36:09.060741: val_loss 0.5482
2024-06-27 10:36:09.061146: Pseudo dice [0.5]
2024-06-27 10:36:09.061568: Epoch time: 64.25 s
2024-06-27 10:36:11.083033: 
2024-06-27 10:36:11.083688: Epoch 73
2024-06-27 10:36:11.084355: Current learning rate: 0.00031
2024-06-27 10:37:15.499176: meanmse:       0.021569936
2024-06-27 10:37:15.500299: meanr2:        0.8265802478619997
2024-06-27 10:37:15.500847: train_loss 0.4432
2024-06-27 10:37:15.501291: val_loss 0.5992
2024-06-27 10:37:15.501718: Pseudo dice [0.5]
2024-06-27 10:37:15.502183: Epoch time: 64.42 s
2024-06-27 10:37:17.576638: 
2024-06-27 10:37:17.577360: Epoch 74
2024-06-27 10:37:17.577825: Current learning rate: 0.0003
2024-06-27 10:38:22.123874: meanmse:       0.018185789
2024-06-27 10:38:22.125064: meanr2:        0.8552893884055568
2024-06-27 10:38:22.125682: train_loss 0.4127
2024-06-27 10:38:22.126157: val_loss 0.5465
2024-06-27 10:38:22.126641: Pseudo dice [0.5]
2024-06-27 10:38:22.127128: Epoch time: 64.56 s
2024-06-27 10:38:24.475940: 
2024-06-27 10:38:24.476866: Epoch 75
2024-06-27 10:38:24.477438: Current learning rate: 0.00029
2024-06-27 10:39:28.816494: meanmse:       0.022465276
2024-06-27 10:39:28.817702: meanr2:        0.8187854465401402
2024-06-27 10:39:28.818308: train_loss 0.448
2024-06-27 10:39:28.818795: val_loss 0.6261
2024-06-27 10:39:28.819686: Pseudo dice [0.5]
2024-06-27 10:39:28.820298: Epoch time: 64.35 s
2024-06-27 10:39:30.752154: 
2024-06-27 10:39:30.752962: Epoch 76
2024-06-27 10:39:30.753484: Current learning rate: 0.00028
2024-06-27 10:40:35.009698: meanmse:       0.02396143
2024-06-27 10:40:35.010837: meanr2:        0.8038640203379676
2024-06-27 10:40:35.011302: train_loss 0.4196
2024-06-27 10:40:35.011679: val_loss 0.6608
2024-06-27 10:40:35.012052: Pseudo dice [0.5]
2024-06-27 10:40:35.012435: Epoch time: 64.27 s
2024-06-27 10:40:36.997088: 
2024-06-27 10:40:36.997859: Epoch 77
2024-06-27 10:40:36.998415: Current learning rate: 0.00027
2024-06-27 10:41:41.314676: meanmse:       0.013941121
2024-06-27 10:41:41.315702: meanr2:        0.8858260217953738
2024-06-27 10:41:41.316221: train_loss 0.4639
2024-06-27 10:41:41.316559: val_loss 0.4617
2024-06-27 10:41:41.316876: Pseudo dice [0.5]
2024-06-27 10:41:41.317303: Epoch time: 64.33 s
2024-06-27 10:41:41.317791: Yayy! New best R2: 0.8858
2024-06-27 10:41:43.899736: 
2024-06-27 10:41:43.900366: Epoch 78
2024-06-27 10:41:43.900837: Current learning rate: 0.00026
2024-06-27 10:42:48.060648: meanmse:       0.023064096
2024-06-27 10:42:48.061775: meanr2:        0.814296042695569
2024-06-27 10:42:48.062375: train_loss 0.4592
2024-06-27 10:42:48.062803: val_loss 0.6495
2024-06-27 10:42:48.063210: Pseudo dice [0.5]
2024-06-27 10:42:48.063617: Epoch time: 64.17 s
2024-06-27 10:42:50.070333: 
2024-06-27 10:42:50.071091: Epoch 79
2024-06-27 10:42:50.071609: Current learning rate: 0.00025
2024-06-27 10:43:54.554908: meanmse:       0.013762429
2024-06-27 10:43:54.556282: meanr2:        0.8874600160700438
2024-06-27 10:43:54.556935: train_loss 0.4345
2024-06-27 10:43:54.557495: val_loss 0.4591
2024-06-27 10:43:54.558022: Pseudo dice [0.5]
2024-06-27 10:43:54.558614: Epoch time: 64.49 s
2024-06-27 10:43:54.907726: Yayy! New best R2: 0.8875
2024-06-27 10:43:57.448999: 
2024-06-27 10:43:57.450003: Epoch 80
2024-06-27 10:43:57.450722: Current learning rate: 0.00023
2024-06-27 10:45:01.833037: meanmse:       0.029336752
2024-06-27 10:45:01.834327: meanr2:        0.7658549928990332
2024-06-27 10:45:01.834931: train_loss 0.4441
2024-06-27 10:45:01.835421: val_loss 0.7338
2024-06-27 10:45:01.835864: Pseudo dice [0.5]
2024-06-27 10:45:01.836412: Epoch time: 64.39 s
2024-06-27 10:45:03.803997: 
2024-06-27 10:45:03.804739: Epoch 81
2024-06-27 10:45:03.805176: Current learning rate: 0.00022
2024-06-27 10:46:08.204144: meanmse:       0.020992847
2024-06-27 10:46:08.205210: meanr2:        0.8310370861739185
2024-06-27 10:46:08.205747: train_loss 0.4233
2024-06-27 10:46:08.206240: val_loss 0.5947
2024-06-27 10:46:08.206773: Pseudo dice [0.5]
2024-06-27 10:46:08.207298: Epoch time: 64.41 s
2024-06-27 10:46:10.113944: 
2024-06-27 10:46:10.114632: Epoch 82
2024-06-27 10:46:10.115102: Current learning rate: 0.00021
2024-06-27 10:47:14.685662: meanmse:       0.017222557
2024-06-27 10:47:14.686726: meanr2:        0.8616745547260948
2024-06-27 10:47:14.687268: train_loss 0.3804
2024-06-27 10:47:14.687708: val_loss 0.5291
2024-06-27 10:47:14.688142: Pseudo dice [0.5]
2024-06-27 10:47:14.688559: Epoch time: 64.58 s
2024-06-27 10:47:16.575720: 
2024-06-27 10:47:16.576535: Epoch 83
2024-06-27 10:47:16.577087: Current learning rate: 0.0002
2024-06-27 10:48:20.889573: meanmse:       0.01597871
2024-06-27 10:48:20.890785: meanr2:        0.8699823680935431
2024-06-27 10:48:20.891548: train_loss 0.395
2024-06-27 10:48:20.891959: val_loss 0.5016
2024-06-27 10:48:20.892435: Pseudo dice [0.5]
2024-06-27 10:48:20.892964: Epoch time: 64.32 s
2024-06-27 10:48:22.852901: 
2024-06-27 10:48:22.853743: Epoch 84
2024-06-27 10:48:22.854181: Current learning rate: 0.00019
2024-06-27 10:49:27.677520: meanmse:       0.018751651
2024-06-27 10:49:27.678964: meanr2:        0.8510901881645127
2024-06-27 10:49:27.679543: train_loss 0.3801
2024-06-27 10:49:27.680089: val_loss 0.5586
2024-06-27 10:49:27.680573: Pseudo dice [0.5]
2024-06-27 10:49:27.681064: Epoch time: 64.83 s
2024-06-27 10:49:29.651401: 
2024-06-27 10:49:29.652285: Epoch 85
2024-06-27 10:49:29.652890: Current learning rate: 0.00018
2024-06-27 10:50:33.927788: meanmse:       0.020898165
2024-06-27 10:50:33.928776: meanr2:        0.8317717016570954
2024-06-27 10:50:33.929258: train_loss 0.3909
2024-06-27 10:50:33.929605: val_loss 0.5907
2024-06-27 10:50:33.929935: Pseudo dice [0.5]
2024-06-27 10:50:33.930402: Epoch time: 64.29 s
2024-06-27 10:50:35.823014: 
2024-06-27 10:50:35.824198: Epoch 86
2024-06-27 10:50:35.824808: Current learning rate: 0.00017
2024-06-27 10:51:40.257097: meanmse:       0.020824013
2024-06-27 10:51:40.258782: meanr2:        0.8295186353448418
2024-06-27 10:51:40.259561: train_loss 0.3869
2024-06-27 10:51:40.260165: val_loss 0.5683
2024-06-27 10:51:40.260854: Pseudo dice [0.5]
2024-06-27 10:51:40.261441: Epoch time: 64.44 s
2024-06-27 10:51:42.131394: 
2024-06-27 10:51:42.132514: Epoch 87
2024-06-27 10:51:42.133142: Current learning rate: 0.00016
2024-06-27 10:52:46.150331: meanmse:       0.017886885
2024-06-27 10:52:46.151810: meanr2:        0.8564169569450812
2024-06-27 10:52:46.152521: train_loss 0.3929
2024-06-27 10:52:46.153027: val_loss 0.5472
2024-06-27 10:52:46.153559: Pseudo dice [0.5]
2024-06-27 10:52:46.154091: Epoch time: 64.03 s
2024-06-27 10:52:48.191426: 
2024-06-27 10:52:48.192137: Epoch 88
2024-06-27 10:52:48.192657: Current learning rate: 0.00015
2024-06-27 10:53:52.592844: meanmse:       0.015542647
2024-06-27 10:53:52.593957: meanr2:        0.8712126584571152
2024-06-27 10:53:52.594496: train_loss 0.4266
2024-06-27 10:53:52.594971: val_loss 0.4988
2024-06-27 10:53:52.595489: Pseudo dice [0.5]
2024-06-27 10:53:52.595986: Epoch time: 64.41 s
2024-06-27 10:53:54.579254: 
2024-06-27 10:53:54.580527: Epoch 89
2024-06-27 10:53:54.581261: Current learning rate: 0.00014
2024-06-27 10:54:58.749157: meanmse:       0.018634124
2024-06-27 10:54:58.752745: meanr2:        0.8471664138174319
2024-06-27 10:54:58.753305: train_loss 0.3976
2024-06-27 10:54:58.758017: val_loss 0.5628
2024-06-27 10:54:58.758848: Pseudo dice [0.5]
2024-06-27 10:54:58.759445: Epoch time: 64.19 s
2024-06-27 10:55:00.988273: 
2024-06-27 10:55:00.989029: Epoch 90
2024-06-27 10:55:00.989542: Current learning rate: 0.00013
2024-06-27 10:56:05.300276: meanmse:       0.020911627
2024-06-27 10:56:05.301455: meanr2:        0.8287964099962918
2024-06-27 10:56:05.301977: train_loss 0.3856
2024-06-27 10:56:05.302447: val_loss 0.5804
2024-06-27 10:56:05.302881: Pseudo dice [0.5]
2024-06-27 10:56:05.303351: Epoch time: 64.32 s
2024-06-27 10:56:07.247683: 
2024-06-27 10:56:07.248487: Epoch 91
2024-06-27 10:56:07.249012: Current learning rate: 0.00011
2024-06-27 10:57:11.251727: meanmse:       0.017067518
2024-06-27 10:57:11.253146: meanr2:        0.8629464229608086
2024-06-27 10:57:11.253934: train_loss 0.3675
2024-06-27 10:57:11.254557: val_loss 0.5295
2024-06-27 10:57:11.255101: Pseudo dice [0.5]
2024-06-27 10:57:11.255625: Epoch time: 64.01 s
2024-06-27 10:57:13.295714: 
2024-06-27 10:57:13.296430: Epoch 92
2024-06-27 10:57:13.297017: Current learning rate: 0.0001
2024-06-27 10:58:17.228836: meanmse:       0.023123858
2024-06-27 10:58:17.230856: meanr2:        0.8130752316936327
2024-06-27 10:58:17.231493: train_loss 0.3869
2024-06-27 10:58:17.231903: val_loss 0.6234
2024-06-27 10:58:17.232401: Pseudo dice [0.5]
2024-06-27 10:58:17.233006: Epoch time: 63.94 s
2024-06-27 10:58:19.117169: 
2024-06-27 10:58:19.118158: Epoch 93
2024-06-27 10:58:19.118769: Current learning rate: 9e-05
2024-06-27 10:59:23.624347: meanmse:       0.016596347
2024-06-27 10:59:23.625639: meanr2:        0.8682982153472558
2024-06-27 10:59:23.626321: train_loss 0.3575
2024-06-27 10:59:23.626907: val_loss 0.5161
2024-06-27 10:59:23.627623: Pseudo dice [0.5]
2024-06-27 10:59:23.629443: Epoch time: 64.52 s
2024-06-27 10:59:25.594388: 
2024-06-27 10:59:25.598511: Epoch 94
2024-06-27 10:59:25.599308: Current learning rate: 8e-05
2024-06-27 11:00:29.865797: meanmse:       0.016719477
2024-06-27 11:00:29.866933: meanr2:        0.8659556522200947
2024-06-27 11:00:29.867507: train_loss 0.3557
2024-06-27 11:00:29.868304: val_loss 0.5092
2024-06-27 11:00:29.868746: Pseudo dice [0.5]
2024-06-27 11:00:29.869194: Epoch time: 64.28 s
2024-06-27 11:00:31.801836: 
2024-06-27 11:00:31.802518: Epoch 95
2024-06-27 11:00:31.803100: Current learning rate: 7e-05
2024-06-27 11:01:35.835197: meanmse:       0.024818629
2024-06-27 11:01:35.836259: meanr2:        0.8043769469522075
2024-06-27 11:01:35.836761: train_loss 0.3655
2024-06-27 11:01:35.837118: val_loss 0.6504
2024-06-27 11:01:35.837479: Pseudo dice [0.5]
2024-06-27 11:01:35.837954: Epoch time: 64.04 s
2024-06-27 11:01:37.725080: 
2024-06-27 11:01:37.727398: Epoch 96
2024-06-27 11:01:37.727987: Current learning rate: 6e-05
2024-06-27 11:02:42.117010: meanmse:       0.018911771
2024-06-27 11:02:42.118288: meanr2:        0.8473451276975936
2024-06-27 11:02:42.118935: train_loss 0.3743
2024-06-27 11:02:42.119477: val_loss 0.5489
2024-06-27 11:02:42.119963: Pseudo dice [0.5]
2024-06-27 11:02:42.120559: Epoch time: 64.4 s
2024-06-27 11:02:44.070238: 
2024-06-27 11:02:44.071169: Epoch 97
2024-06-27 11:02:44.071781: Current learning rate: 4e-05
2024-06-27 11:03:48.190152: meanmse:       0.019975236
2024-06-27 11:03:48.191263: meanr2:        0.8392385334449883
2024-06-27 11:03:48.191757: train_loss 0.3593
2024-06-27 11:03:48.192229: val_loss 0.5712
2024-06-27 11:03:48.192638: Pseudo dice [0.5]
2024-06-27 11:03:48.193068: Epoch time: 64.13 s
2024-06-27 11:03:50.160701: 
2024-06-27 11:03:50.161565: Epoch 98
2024-06-27 11:03:50.162105: Current learning rate: 3e-05
2024-06-27 11:04:54.505106: meanmse:       0.024176665
2024-06-27 11:04:54.506215: meanr2:        0.8018300482483806
2024-06-27 11:04:54.506760: train_loss 0.3613
2024-06-27 11:04:54.507213: val_loss 0.6545
2024-06-27 11:04:54.507649: Pseudo dice [0.5]
2024-06-27 11:04:54.508089: Epoch time: 64.35 s
2024-06-27 11:04:56.540659: 
2024-06-27 11:04:56.541794: Epoch 99
2024-06-27 11:04:56.542439: Current learning rate: 2e-05
2024-06-27 11:06:01.423348: meanmse:       0.01790254
2024-06-27 11:06:01.424639: meanr2:        0.8558384917576689
2024-06-27 11:06:01.425233: train_loss 0.4059
2024-06-27 11:06:01.425745: val_loss 0.5375
2024-06-27 11:06:01.426202: Pseudo dice [0.5]
2024-06-27 11:06:01.426661: Epoch time: 64.89 s
2024-06-27 11:06:03.992589: Training done.
