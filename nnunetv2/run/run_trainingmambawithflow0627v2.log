nohup: ignoring input
Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

UMambaEnc: UMambaEnc(
  (encoder): ResidualMambaEncoder(
    (stem): StackedConvBlocks(
      (convs): Sequential(
        (0): ConvDropoutNormReLU(
          (conv): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
          (all_modules): Sequential(
            (0): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
            (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (2): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (5): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (mamba_layers): ModuleList(
      (0): MambaLayer(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=32, out_features=128, bias=False)
          (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
          (act): SiLU()
          (x_proj): Linear(in_features=64, out_features=34, bias=False)
          (dt_proj): Linear(in_features=2, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=32, bias=False)
        )
      )
      (1): MambaLayer(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=64, out_features=256, bias=False)
          (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
          (act): SiLU()
          (x_proj): Linear(in_features=128, out_features=36, bias=False)
          (dt_proj): Linear(in_features=4, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=64, bias=False)
        )
      )
      (2-3): 2 x MambaLayer(
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=128, out_features=512, bias=False)
          (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
          (act): SiLU()
          (x_proj): Linear(in_features=256, out_features=40, bias=False)
          (dt_proj): Linear(in_features=8, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=128, bias=False)
        )
      )
      (4): MambaLayer(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=64, out_features=256, bias=False)
          (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
          (act): SiLU()
          (x_proj): Linear(in_features=128, out_features=36, bias=False)
          (dt_proj): Linear(in_features=4, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=64, bias=False)
        )
      )
      (5): MambaLayer(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=32, out_features=128, bias=False)
          (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
          (act): SiLU()
          (x_proj): Linear(in_features=64, out_features=34, bias=False)
          (dt_proj): Linear(in_features=2, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=32, bias=False)
        )
      )
    )
  )
  (decoder): UNetResDecoder(
    (encoder): ResidualMambaEncoder(
      (stem): StackedConvBlocks(
        (convs): Sequential(
          (0): ConvDropoutNormReLU(
            (conv): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
            (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
            (all_modules): Sequential(
              (0): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (stages): ModuleList(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (4): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (5): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (mamba_layers): ModuleList(
        (0): MambaLayer(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=32, out_features=128, bias=False)
            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
            (act): SiLU()
            (x_proj): Linear(in_features=64, out_features=34, bias=False)
            (dt_proj): Linear(in_features=2, out_features=64, bias=True)
            (out_proj): Linear(in_features=64, out_features=32, bias=False)
          )
        )
        (1): MambaLayer(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (2-3): 2 x MambaLayer(
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (4): MambaLayer(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (5): MambaLayer(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=32, out_features=128, bias=False)
            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
            (act): SiLU()
            (x_proj): Linear(in_features=64, out_features=34, bias=False)
            (dt_proj): Linear(in_features=2, out_features=64, bias=True)
            (out_proj): Linear(in_features=64, out_features=32, bias=False)
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1-2): 2 x StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (transpconvs): ModuleList(
      (0): ConvTranspose3d(32, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (1): ConvTranspose3d(64, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (2): ConvTranspose3d(128, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (3): ConvTranspose3d(128, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (4): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2))
    )
    (lzz_layers): ModuleList(
      (0): ModuleList(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1-2): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (1): ModuleList(
        (0-1): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2-3): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (2): ModuleList(
        (0-2): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (3): ModuleList(
        (0-2): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (4): ModuleList(
        (0-1): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2-4): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
    )
    (lzz_fc): ModuleList(
      (0-4): 5 x ModuleList(
        (0): Sequential(
          (0): Linear(in_features=288, out_features=72, bias=True)
          (1): Tanh()
        )
        (1): Sequential(
          (0): Linear(in_features=72, out_features=18, bias=True)
          (1): Tanh()
        )
        (2): Sequential(
          (0): Linear(in_features=18, out_features=1, bias=True)
          (1): Tanh()
        )
      )
    )
  )
)

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [40, 192, 192], 'median_image_size_in_voxels': [97.0, 512.0, 512.0], 'spacing': [2.5, 0.7958984971046448, 0.7958984971046448], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [3, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset701_AbdomenCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.7958984971046448, 0.7958984971046448], 'original_median_shape_after_transp': [97, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3071.0, 'mean': 97.29716491699219, 'median': 118.0, 'min': -1024.0, 'percentile_00_5': -958.0, 'percentile_99_5': 270.0, 'std': 137.8484649658203}}} 

2024-06-27 12:14:51.310943: unpacking dataset...
2024-06-27 12:14:51.311355: unpacking done...
2024-06-27 12:14:51.312076: do_dummy_2d_data_aug: False
2024-06-27 12:14:51.331417: Unable to plot network architecture:
2024-06-27 12:14:51.331826: No module named 'hiddenlayer'
2024-06-27 12:14:51.343445: 
2024-06-27 12:14:51.344133: Epoch 0
2024-06-27 12:14:51.344583: Current learning rate: 0.001
using pin_memory on device 0
using pin_memory on device 0
2024-06-27 12:16:10.850949: meanmse:       0.121476695
2024-06-27 12:16:10.852864: meanr2:        0.02033184469203855
2024-06-27 12:16:10.853868: train_loss 3.063
2024-06-27 12:16:10.854429: val_loss 2.5263
2024-06-27 12:16:10.854998: Pseudo dice [0.5]
2024-06-27 12:16:10.855572: Epoch time: 79.52 s
2024-06-27 12:16:10.856066: Yayy! New best R2: 0.0203
2024-06-27 12:16:13.011797: 
2024-06-27 12:16:13.012696: Epoch 1
2024-06-27 12:16:13.013217: Current learning rate: 0.00099
2024-06-27 12:17:17.013931: meanmse:       0.108959466
2024-06-27 12:17:17.015083: meanr2:        0.12311902235593152
2024-06-27 12:17:17.015550: train_loss 2.421
2024-06-27 12:17:17.015948: val_loss 2.304
2024-06-27 12:17:17.016336: Pseudo dice [0.5]
2024-06-27 12:17:17.016750: Epoch time: 64.01 s
2024-06-27 12:17:17.017276: Yayy! New best R2: 0.1231
2024-06-27 12:17:19.268769: 
2024-06-27 12:17:19.269511: Epoch 2
2024-06-27 12:17:19.269954: Current learning rate: 0.00098
2024-06-27 12:18:24.073502: meanmse:       0.097819455
2024-06-27 12:18:24.075027: meanr2:        0.21097348440792485
2024-06-27 12:18:24.075711: train_loss 2.2011
2024-06-27 12:18:24.076193: val_loss 2.088
2024-06-27 12:18:24.076869: Pseudo dice [0.5]
2024-06-27 12:18:24.077381: Epoch time: 64.81 s
2024-06-27 12:18:24.077880: Yayy! New best R2: 0.211
2024-06-27 12:18:26.315350: 
2024-06-27 12:18:26.316057: Epoch 3
2024-06-27 12:18:26.316503: Current learning rate: 0.00097
2024-06-27 12:19:30.640337: meanmse:       0.08722136
2024-06-27 12:19:30.641366: meanr2:        0.3043737758056721
2024-06-27 12:19:30.642097: train_loss 2.0511
2024-06-27 12:19:30.642494: val_loss 1.9318
2024-06-27 12:19:30.642849: Pseudo dice [0.5]
2024-06-27 12:19:30.643230: Epoch time: 64.33 s
2024-06-27 12:19:30.643608: Yayy! New best R2: 0.3044
2024-06-27 12:19:32.958849: 
2024-06-27 12:19:32.959724: Epoch 4
2024-06-27 12:19:32.960334: Current learning rate: 0.00096
2024-06-27 12:20:37.171381: meanmse:       0.06574016
2024-06-27 12:20:37.172536: meanr2:        0.4697297905447536
2024-06-27 12:20:37.173103: train_loss 1.7726
2024-06-27 12:20:37.173570: val_loss 1.6198
2024-06-27 12:20:37.174155: Pseudo dice [0.5]
2024-06-27 12:20:37.174548: Epoch time: 64.22 s
2024-06-27 12:20:37.174882: Yayy! New best R2: 0.4697
2024-06-27 12:20:39.805698: 
2024-06-27 12:20:39.806424: Epoch 5
2024-06-27 12:20:39.807024: Current learning rate: 0.00095
2024-06-27 12:21:45.099707: meanmse:       0.04923059
2024-06-27 12:21:45.100818: meanr2:        0.6036779942680411
2024-06-27 12:21:45.101433: train_loss 1.4084
2024-06-27 12:21:45.101950: val_loss 1.3361
2024-06-27 12:21:45.102416: Pseudo dice [0.5]
2024-06-27 12:21:45.102914: Epoch time: 65.3 s
2024-06-27 12:21:45.103381: Yayy! New best R2: 0.6037
2024-06-27 12:21:47.478914: 
2024-06-27 12:21:47.479945: Epoch 6
2024-06-27 12:21:47.480540: Current learning rate: 0.00095
2024-06-27 12:22:51.789486: meanmse:       0.035770327
2024-06-27 12:22:51.790659: meanr2:        0.704106635537185
2024-06-27 12:22:51.791193: train_loss 1.2852
2024-06-27 12:22:51.791753: val_loss 1.0755
2024-06-27 12:22:51.792217: Pseudo dice [0.5]
2024-06-27 12:22:51.792713: Epoch time: 64.32 s
2024-06-27 12:22:51.793175: Yayy! New best R2: 0.7041
2024-06-27 12:22:54.513571: 
2024-06-27 12:22:54.514246: Epoch 7
2024-06-27 12:22:54.514690: Current learning rate: 0.00094
2024-06-27 12:23:58.875740: meanmse:       0.039813057
2024-06-27 12:23:58.876885: meanr2:        0.6772148136878713
2024-06-27 12:23:58.877353: train_loss 1.1738
2024-06-27 12:23:58.877712: val_loss 1.1221
2024-06-27 12:23:58.878221: Pseudo dice [0.5]
2024-06-27 12:23:58.878641: Epoch time: 64.37 s
2024-06-27 12:24:00.799950: 
2024-06-27 12:24:00.800786: Epoch 8
2024-06-27 12:24:00.801485: Current learning rate: 0.00093
2024-06-27 12:25:04.652876: meanmse:       0.046937205
2024-06-27 12:25:04.654163: meanr2:        0.6193923064288519
2024-06-27 12:25:04.654771: train_loss 1.1292
2024-06-27 12:25:04.655335: val_loss 1.2255
2024-06-27 12:25:04.655804: Pseudo dice [0.5]
2024-06-27 12:25:04.656292: Epoch time: 63.86 s
2024-06-27 12:25:06.808585: 
2024-06-27 12:25:06.809250: Epoch 9
2024-06-27 12:25:06.809731: Current learning rate: 0.00092
2024-06-27 12:26:11.298969: meanmse:       0.035682734
2024-06-27 12:26:11.300150: meanr2:        0.7116894377141768
2024-06-27 12:26:11.300812: train_loss 1.119
2024-06-27 12:26:11.301280: val_loss 1.0129
2024-06-27 12:26:11.301719: Pseudo dice [0.5]
2024-06-27 12:26:11.302182: Epoch time: 64.5 s
2024-06-27 12:26:11.578204: Yayy! New best R2: 0.7117
2024-06-27 12:26:14.222492: 
2024-06-27 12:26:14.223445: Epoch 10
2024-06-27 12:26:14.224110: Current learning rate: 0.00091
2024-06-27 12:27:18.320495: meanmse:       0.03325855
2024-06-27 12:27:18.322111: meanr2:        0.7218711194478451
2024-06-27 12:27:18.322845: train_loss 1.038
2024-06-27 12:27:18.323444: val_loss 1.0015
2024-06-27 12:27:18.324054: Pseudo dice [0.5]
2024-06-27 12:27:18.324622: Epoch time: 64.11 s
2024-06-27 12:27:18.325276: Yayy! New best R2: 0.7219
2024-06-27 12:27:20.650654: 
2024-06-27 12:27:20.651547: Epoch 11
2024-06-27 12:27:20.652287: Current learning rate: 0.0009
2024-06-27 12:28:24.877029: meanmse:       0.02995226
2024-06-27 12:28:24.878329: meanr2:        0.7555437870065799
2024-06-27 12:28:24.878920: train_loss 0.9697
2024-06-27 12:28:24.879382: val_loss 0.9333
2024-06-27 12:28:24.879812: Pseudo dice [0.5]
2024-06-27 12:28:24.880273: Epoch time: 64.24 s
2024-06-27 12:28:24.880697: Yayy! New best R2: 0.7555
2024-06-27 12:28:27.509510: 
2024-06-27 12:28:27.510148: Epoch 12
2024-06-27 12:28:27.510747: Current learning rate: 0.00089
2024-06-27 12:29:31.591004: meanmse:       0.031100394
2024-06-27 12:29:31.592314: meanr2:        0.7455654644390323
2024-06-27 12:29:31.592856: train_loss 1.0355
2024-06-27 12:29:31.593287: val_loss 0.9169
2024-06-27 12:29:31.593683: Pseudo dice [0.5]
2024-06-27 12:29:31.594176: Epoch time: 64.09 s
2024-06-27 12:29:33.542899: 
2024-06-27 12:29:33.543860: Epoch 13
2024-06-27 12:29:33.544515: Current learning rate: 0.00088
2024-06-27 12:30:37.978468: meanmse:       0.029090965
2024-06-27 12:30:37.979803: meanr2:        0.7624828365496711
2024-06-27 12:30:37.980446: train_loss 0.8972
2024-06-27 12:30:37.980915: val_loss 0.8763
2024-06-27 12:30:37.981390: Pseudo dice [0.5]
2024-06-27 12:30:37.981871: Epoch time: 64.45 s
2024-06-27 12:30:37.982335: Yayy! New best R2: 0.7625
2024-06-27 12:30:40.394049: 
2024-06-27 12:30:40.394956: Epoch 14
2024-06-27 12:30:40.395487: Current learning rate: 0.00087
2024-06-27 12:31:44.575001: meanmse:       0.02896337
2024-06-27 12:31:44.576110: meanr2:        0.7642753250567139
2024-06-27 12:31:44.576653: train_loss 0.8981
2024-06-27 12:31:44.577247: val_loss 0.8853
2024-06-27 12:31:44.577704: Pseudo dice [0.5]
2024-06-27 12:31:44.578186: Epoch time: 64.19 s
2024-06-27 12:31:44.578621: Yayy! New best R2: 0.7643
2024-06-27 12:31:47.261316: 
2024-06-27 12:31:47.262145: Epoch 15
2024-06-27 12:31:47.262770: Current learning rate: 0.00086
2024-06-27 12:32:51.604610: meanmse:       0.03300671
2024-06-27 12:32:51.606039: meanr2:        0.7348501685239519
2024-06-27 12:32:51.606926: train_loss 0.8654
2024-06-27 12:32:51.607610: val_loss 0.9389
2024-06-27 12:32:51.608195: Pseudo dice [0.5]
2024-06-27 12:32:51.608708: Epoch time: 64.35 s
2024-06-27 12:32:53.471272: 
2024-06-27 12:32:53.471916: Epoch 16
2024-06-27 12:32:53.472309: Current learning rate: 0.00085
2024-06-27 12:33:57.824308: meanmse:       0.025761992
2024-06-27 12:33:57.825855: meanr2:        0.7877276420078109
2024-06-27 12:33:57.826630: train_loss 0.8629
2024-06-27 12:33:57.827248: val_loss 0.7981
2024-06-27 12:33:57.836035: Pseudo dice [0.5]
2024-06-27 12:33:57.837039: Epoch time: 64.36 s
2024-06-27 12:33:57.837672: Yayy! New best R2: 0.7877
2024-06-27 12:34:00.284662: 
2024-06-27 12:34:00.285329: Epoch 17
2024-06-27 12:34:00.285784: Current learning rate: 0.00085
2024-06-27 12:35:04.856873: meanmse:       0.026813328
2024-06-27 12:35:04.858047: meanr2:        0.7818658705221412
2024-06-27 12:35:04.858512: train_loss 0.9135
2024-06-27 12:35:04.858913: val_loss 0.8282
2024-06-27 12:35:04.859322: Pseudo dice [0.5]
2024-06-27 12:35:04.859967: Epoch time: 64.58 s
2024-06-27 12:35:06.789852: 
2024-06-27 12:35:06.790796: Epoch 18
2024-06-27 12:35:06.791440: Current learning rate: 0.00084
2024-06-27 12:36:10.874596: meanmse:       0.02832623
2024-06-27 12:36:10.875832: meanr2:        0.7725307767017032
2024-06-27 12:36:10.876413: train_loss 0.8466
2024-06-27 12:36:10.876897: val_loss 0.8609
2024-06-27 12:36:10.877345: Pseudo dice [0.5]
2024-06-27 12:36:10.877821: Epoch time: 64.1 s
2024-06-27 12:36:12.855044: 
2024-06-27 12:36:12.855927: Epoch 19
2024-06-27 12:36:12.856669: Current learning rate: 0.00083
2024-06-27 12:37:16.813920: meanmse:       0.026456
2024-06-27 12:37:16.815085: meanr2:        0.786965867446305
2024-06-27 12:37:16.815587: train_loss 0.8247
2024-06-27 12:37:16.816048: val_loss 0.7888
2024-06-27 12:37:16.816490: Pseudo dice [0.5]
2024-06-27 12:37:16.816951: Epoch time: 63.97 s
2024-06-27 12:37:19.146209: 
2024-06-27 12:37:19.147216: Epoch 20
2024-06-27 12:37:19.147922: Current learning rate: 0.00082
2024-06-27 12:38:23.493709: meanmse:       0.03273492
2024-06-27 12:38:23.494937: meanr2:        0.7386830753813295
2024-06-27 12:38:23.495484: train_loss 0.7974
2024-06-27 12:38:23.496098: val_loss 0.9101
2024-06-27 12:38:23.496696: Pseudo dice [0.5]
2024-06-27 12:38:23.497199: Epoch time: 64.36 s
2024-06-27 12:38:25.510400: 
2024-06-27 12:38:25.511181: Epoch 21
2024-06-27 12:38:25.511675: Current learning rate: 0.00081
2024-06-27 12:39:29.894856: meanmse:       0.02991451
2024-06-27 12:39:29.896225: meanr2:        0.7590848089696108
2024-06-27 12:39:29.896851: train_loss 0.7235
2024-06-27 12:39:29.897324: val_loss 0.8682
2024-06-27 12:39:29.897792: Pseudo dice [0.5]
2024-06-27 12:39:29.898267: Epoch time: 64.39 s
2024-06-27 12:39:31.877287: 
2024-06-27 12:39:31.878005: Epoch 22
2024-06-27 12:39:31.878488: Current learning rate: 0.0008
2024-06-27 12:40:36.252301: meanmse:       0.027433407
2024-06-27 12:40:36.253582: meanr2:        0.7777937595153268
2024-06-27 12:40:36.254223: train_loss 0.808
2024-06-27 12:40:36.254755: val_loss 0.807
2024-06-27 12:40:36.255385: Pseudo dice [0.5]
2024-06-27 12:40:36.255970: Epoch time: 64.38 s
2024-06-27 12:40:38.360762: 
2024-06-27 12:40:38.361675: Epoch 23
2024-06-27 12:40:38.362304: Current learning rate: 0.00079
2024-06-27 12:41:42.512246: meanmse:       0.02418337
2024-06-27 12:41:42.513377: meanr2:        0.8047870417452654
2024-06-27 12:41:42.513912: train_loss 0.8059
2024-06-27 12:41:42.514421: val_loss 0.747
2024-06-27 12:41:42.514885: Pseudo dice [0.5]
2024-06-27 12:41:42.515352: Epoch time: 64.16 s
2024-06-27 12:41:42.515793: Yayy! New best R2: 0.8048
2024-06-27 12:41:45.091548: 
2024-06-27 12:41:45.092311: Epoch 24
2024-06-27 12:41:45.092729: Current learning rate: 0.00078
2024-06-27 12:42:49.495349: meanmse:       0.03254813
2024-06-27 12:42:49.496733: meanr2:        0.7373321765939216
2024-06-27 12:42:49.497326: train_loss 0.8297
2024-06-27 12:42:49.497905: val_loss 0.9057
2024-06-27 12:42:49.507086: Pseudo dice [0.5]
2024-06-27 12:42:49.507907: Epoch time: 64.41 s
2024-06-27 12:42:51.384515: 
2024-06-27 12:42:51.385268: Epoch 25
2024-06-27 12:42:51.385838: Current learning rate: 0.00077
2024-06-27 12:43:55.752226: meanmse:       0.026832564
2024-06-27 12:43:55.753945: meanr2:        0.7876584639865564
2024-06-27 12:43:55.754720: train_loss 0.7621
2024-06-27 12:43:55.755320: val_loss 0.7687
2024-06-27 12:43:55.755987: Pseudo dice [0.5]
2024-06-27 12:43:55.756733: Epoch time: 64.38 s
2024-06-27 12:43:57.777839: 
2024-06-27 12:43:57.778514: Epoch 26
2024-06-27 12:43:57.778915: Current learning rate: 0.00076
2024-06-27 12:45:02.101192: meanmse:       0.021630459
2024-06-27 12:45:02.102461: meanr2:        0.8238418840516576
2024-06-27 12:45:02.103075: train_loss 0.7329
2024-06-27 12:45:02.103546: val_loss 0.7156
2024-06-27 12:45:02.103995: Pseudo dice [0.5]
2024-06-27 12:45:02.104455: Epoch time: 64.33 s
2024-06-27 12:45:02.104911: Yayy! New best R2: 0.8238
2024-06-27 12:45:04.722179: 
2024-06-27 12:45:04.723109: Epoch 27
2024-06-27 12:45:04.723809: Current learning rate: 0.00075
2024-06-27 12:46:09.017638: meanmse:       0.02484936
2024-06-27 12:46:09.019019: meanr2:        0.7971885894081904
2024-06-27 12:46:09.019718: train_loss 0.7441
2024-06-27 12:46:09.020306: val_loss 0.7652
2024-06-27 12:46:09.020860: Pseudo dice [0.5]
2024-06-27 12:46:09.021538: Epoch time: 64.31 s
2024-06-27 12:46:11.181209: 
2024-06-27 12:46:11.182059: Epoch 28
2024-06-27 12:46:11.182699: Current learning rate: 0.00074
2024-06-27 12:47:15.791076: meanmse:       0.02832967
2024-06-27 12:47:15.792220: meanr2:        0.7664130633550991
2024-06-27 12:47:15.792747: train_loss 0.7627
2024-06-27 12:47:15.793190: val_loss 0.8348
2024-06-27 12:47:15.793618: Pseudo dice [0.5]
2024-06-27 12:47:15.794074: Epoch time: 64.62 s
2024-06-27 12:47:17.747484: 
2024-06-27 12:47:17.748356: Epoch 29
2024-06-27 12:47:17.748878: Current learning rate: 0.00073
2024-06-27 12:48:22.119883: meanmse:       0.022960274
2024-06-27 12:48:22.121093: meanr2:        0.8127169281983474
2024-06-27 12:48:22.121553: train_loss 0.7223
2024-06-27 12:48:22.121941: val_loss 0.7092
2024-06-27 12:48:22.122532: Pseudo dice [0.5]
2024-06-27 12:48:22.123076: Epoch time: 64.38 s
2024-06-27 12:48:24.754272: 
2024-06-27 12:48:24.755001: Epoch 30
2024-06-27 12:48:24.755457: Current learning rate: 0.00073
2024-06-27 12:49:28.986668: meanmse:       0.025967302
2024-06-27 12:49:28.987661: meanr2:        0.7923971485985448
2024-06-27 12:49:28.988120: train_loss 0.718
2024-06-27 12:49:28.988500: val_loss 0.78
2024-06-27 12:49:28.989026: Pseudo dice [0.5]
2024-06-27 12:49:28.989460: Epoch time: 64.24 s
2024-06-27 12:49:31.293612: 
2024-06-27 12:49:31.294197: Epoch 31
2024-06-27 12:49:31.294789: Current learning rate: 0.00072
2024-06-27 12:50:36.553331: meanmse:       0.020535568
2024-06-27 12:50:36.555211: meanr2:        0.8320360161221781
2024-06-27 12:50:36.556051: train_loss 0.6897
2024-06-27 12:50:36.556615: val_loss 0.6813
2024-06-27 12:50:36.557256: Pseudo dice [0.5]
2024-06-27 12:50:36.573972: Epoch time: 65.27 s
2024-06-27 12:50:36.574667: Yayy! New best R2: 0.832
2024-06-27 12:50:38.996232: 
2024-06-27 12:50:38.996963: Epoch 32
2024-06-27 12:50:38.997471: Current learning rate: 0.00071
2024-06-27 12:51:43.618366: meanmse:       0.027653905
2024-06-27 12:51:43.619529: meanr2:        0.7759503133478958
2024-06-27 12:51:43.620164: train_loss 0.6683
2024-06-27 12:51:43.620655: val_loss 0.7856
2024-06-27 12:51:43.622863: Pseudo dice [0.5]
2024-06-27 12:51:43.623427: Epoch time: 64.63 s
2024-06-27 12:51:45.565120: 
2024-06-27 12:51:45.565882: Epoch 33
2024-06-27 12:51:45.566356: Current learning rate: 0.0007
2024-06-27 12:52:49.455817: meanmse:       0.021488871
2024-06-27 12:52:49.457469: meanr2:        0.8257741602647092
2024-06-27 12:52:49.458166: train_loss 0.708
2024-06-27 12:52:49.458700: val_loss 0.6923
2024-06-27 12:52:49.459144: Pseudo dice [0.5]
2024-06-27 12:52:49.459597: Epoch time: 63.9 s
2024-06-27 12:52:51.482770: 
2024-06-27 12:52:51.483438: Epoch 34
2024-06-27 12:52:51.483832: Current learning rate: 0.00069
2024-06-27 12:53:55.589945: meanmse:       0.024061082
2024-06-27 12:53:55.591149: meanr2:        0.8057973670158269
2024-06-27 12:53:55.591598: train_loss 0.6711
2024-06-27 12:53:55.591973: val_loss 0.7242
2024-06-27 12:53:55.592341: Pseudo dice [0.5]
2024-06-27 12:53:55.592749: Epoch time: 64.12 s
2024-06-27 12:53:57.485172: 
2024-06-27 12:53:57.485822: Epoch 35
2024-06-27 12:53:57.486282: Current learning rate: 0.00068
2024-06-27 12:55:01.583144: meanmse:       0.022669263
2024-06-27 12:55:01.584139: meanr2:        0.8152135885522251
2024-06-27 12:55:01.584709: train_loss 0.7065
2024-06-27 12:55:01.585171: val_loss 0.6927
2024-06-27 12:55:01.585576: Pseudo dice [0.5]
2024-06-27 12:55:01.585968: Epoch time: 64.11 s
2024-06-27 12:55:03.499847: 
2024-06-27 12:55:03.500496: Epoch 36
2024-06-27 12:55:03.500960: Current learning rate: 0.00067
2024-06-27 12:56:07.867563: meanmse:       0.025088353
2024-06-27 12:56:07.869024: meanr2:        0.8026744152435709
2024-06-27 12:56:07.869692: train_loss 0.684
2024-06-27 12:56:07.875165: val_loss 0.7476
2024-06-27 12:56:07.876051: Pseudo dice [0.5]
2024-06-27 12:56:07.877115: Epoch time: 64.38 s
2024-06-27 12:56:09.765507: 
2024-06-27 12:56:09.766385: Epoch 37
2024-06-27 12:56:09.766996: Current learning rate: 0.00066
2024-06-27 12:57:14.130167: meanmse:       0.020630844
2024-06-27 12:57:14.131382: meanr2:        0.8329233693587491
2024-06-27 12:57:14.131989: train_loss 0.63
2024-06-27 12:57:14.132493: val_loss 0.6476
2024-06-27 12:57:14.133280: Pseudo dice [0.5]
2024-06-27 12:57:14.133873: Epoch time: 64.38 s
2024-06-27 12:57:14.134302: Yayy! New best R2: 0.8329
2024-06-27 12:57:16.469395: 
2024-06-27 12:57:16.470287: Epoch 38
2024-06-27 12:57:16.470773: Current learning rate: 0.00065
2024-06-27 12:58:20.890841: meanmse:       0.027885364
2024-06-27 12:58:20.892176: meanr2:        0.7774580022509364
2024-06-27 12:58:20.892847: train_loss 0.6751
2024-06-27 12:58:20.893348: val_loss 0.8164
2024-06-27 12:58:20.893883: Pseudo dice [0.5]
2024-06-27 12:58:20.894408: Epoch time: 64.43 s
2024-06-27 12:58:23.024073: 
2024-06-27 12:58:23.025145: Epoch 39
2024-06-27 12:58:23.025861: Current learning rate: 0.00064
2024-06-27 12:59:27.527256: meanmse:       0.01978974
2024-06-27 12:59:27.532819: meanr2:        0.8397644721013865
2024-06-27 12:59:27.540730: train_loss 0.6676
2024-06-27 12:59:27.541408: val_loss 0.6357
2024-06-27 12:59:27.541998: Pseudo dice [0.5]
2024-06-27 12:59:27.542573: Epoch time: 64.53 s
2024-06-27 12:59:27.878771: Yayy! New best R2: 0.8398
2024-06-27 12:59:30.310878: 
2024-06-27 12:59:30.311523: Epoch 40
2024-06-27 12:59:30.312133: Current learning rate: 0.00063
2024-06-27 13:00:34.667350: meanmse:       0.028167032
2024-06-27 13:00:34.668681: meanr2:        0.7743126908128157
2024-06-27 13:00:34.669257: train_loss 0.6486
2024-06-27 13:00:34.669733: val_loss 0.798
2024-06-27 13:00:34.670204: Pseudo dice [0.5]
2024-06-27 13:00:34.670712: Epoch time: 64.36 s
2024-06-27 13:00:36.780657: 
2024-06-27 13:00:36.781431: Epoch 41
2024-06-27 13:00:36.781918: Current learning rate: 0.00062
2024-06-27 13:01:41.169583: meanmse:       0.024446182
2024-06-27 13:01:41.170715: meanr2:        0.8021264890762154
2024-06-27 13:01:41.171212: train_loss 0.6476
2024-06-27 13:01:41.171726: val_loss 0.723
2024-06-27 13:01:41.172155: Pseudo dice [0.5]
2024-06-27 13:01:41.172599: Epoch time: 64.4 s
2024-06-27 13:01:42.986901: 
2024-06-27 13:01:42.987684: Epoch 42
2024-06-27 13:01:42.988217: Current learning rate: 0.00061
2024-06-27 13:02:47.493869: meanmse:       0.022677176
2024-06-27 13:02:47.495788: meanr2:        0.8163197520080253
2024-06-27 13:02:47.496331: train_loss 0.6292
2024-06-27 13:02:47.496820: val_loss 0.6929
2024-06-27 13:02:47.497260: Pseudo dice [0.5]
2024-06-27 13:02:47.497729: Epoch time: 64.52 s
2024-06-27 13:02:49.480369: 
2024-06-27 13:02:49.481134: Epoch 43
2024-06-27 13:02:49.481765: Current learning rate: 0.0006
2024-06-27 13:03:53.782287: meanmse:       0.018437667
2024-06-27 13:03:53.783346: meanr2:        0.8548987574850273
2024-06-27 13:03:53.783857: train_loss 0.5961
2024-06-27 13:03:53.784389: val_loss 0.6212
2024-06-27 13:03:53.784852: Pseudo dice [0.5]
2024-06-27 13:03:53.785377: Epoch time: 64.31 s
2024-06-27 13:03:53.785848: Yayy! New best R2: 0.8549
2024-06-27 13:03:56.107908: 
2024-06-27 13:03:56.108655: Epoch 44
2024-06-27 13:03:56.109214: Current learning rate: 0.00059
2024-06-27 13:05:00.198233: meanmse:       0.024455763
2024-06-27 13:05:00.199362: meanr2:        0.8046090409877674
2024-06-27 13:05:00.199899: train_loss 0.6185
2024-06-27 13:05:00.200455: val_loss 0.7049
2024-06-27 13:05:00.201041: Pseudo dice [0.5]
2024-06-27 13:05:00.201539: Epoch time: 64.1 s
2024-06-27 13:05:02.105953: 
2024-06-27 13:05:02.106711: Epoch 45
2024-06-27 13:05:02.107335: Current learning rate: 0.00058
2024-06-27 13:06:06.455606: meanmse:       0.019652562
2024-06-27 13:06:06.456787: meanr2:        0.8434001708422939
2024-06-27 13:06:06.457433: train_loss 0.6195
2024-06-27 13:06:06.457995: val_loss 0.63
2024-06-27 13:06:06.458458: Pseudo dice [0.5]
2024-06-27 13:06:06.458954: Epoch time: 64.36 s
2024-06-27 13:06:08.703697: 
2024-06-27 13:06:08.704467: Epoch 46
2024-06-27 13:06:08.705013: Current learning rate: 0.00057
2024-06-27 13:07:13.140779: meanmse:       0.018471694
2024-06-27 13:07:13.141792: meanr2:        0.8528138732558671
2024-06-27 13:07:13.142641: train_loss 0.6516
2024-06-27 13:07:13.143247: val_loss 0.606
2024-06-27 13:07:13.143827: Pseudo dice [0.5]
2024-06-27 13:07:13.144389: Epoch time: 64.45 s
2024-06-27 13:07:15.013878: 
2024-06-27 13:07:15.014778: Epoch 47
2024-06-27 13:07:15.015311: Current learning rate: 0.00056
2024-06-27 13:08:19.366830: meanmse:       0.017243596
2024-06-27 13:08:19.368120: meanr2:        0.8596693170771342
2024-06-27 13:08:19.368665: train_loss 0.6125
2024-06-27 13:08:19.369138: val_loss 0.5953
2024-06-27 13:08:19.369587: Pseudo dice [0.5]
2024-06-27 13:08:19.370048: Epoch time: 64.36 s
2024-06-27 13:08:19.370505: Yayy! New best R2: 0.8597
2024-06-27 13:08:21.610257: 
2024-06-27 13:08:21.610993: Epoch 48
2024-06-27 13:08:21.611497: Current learning rate: 0.00056
2024-06-27 13:09:26.092800: meanmse:       0.021377604
2024-06-27 13:09:26.094249: meanr2:        0.8270774401563514
2024-06-27 13:09:26.094963: train_loss 0.5415
2024-06-27 13:09:26.095607: val_loss 0.6509
2024-06-27 13:09:26.096303: Pseudo dice [0.5]
2024-06-27 13:09:26.097028: Epoch time: 64.49 s
2024-06-27 13:09:28.150936: 
2024-06-27 13:09:28.152032: Epoch 49
2024-06-27 13:09:28.152801: Current learning rate: 0.00055
2024-06-27 13:10:32.424247: meanmse:       0.017051022
2024-06-27 13:10:32.425531: meanr2:        0.8625880178574854
2024-06-27 13:10:32.426257: train_loss 0.6055
2024-06-27 13:10:32.426819: val_loss 0.5903
2024-06-27 13:10:32.427405: Pseudo dice [0.5]
2024-06-27 13:10:32.427994: Epoch time: 64.29 s
2024-06-27 13:10:32.757207: Yayy! New best R2: 0.8626
2024-06-27 13:10:35.092105: 
2024-06-27 13:10:35.092736: Epoch 50
2024-06-27 13:10:35.093322: Current learning rate: 0.00054
2024-06-27 13:11:39.486888: meanmse:       0.022169258
2024-06-27 13:11:39.487833: meanr2:        0.8202572921532593
2024-06-27 13:11:39.488285: train_loss 0.6183
2024-06-27 13:11:39.488671: val_loss 0.6909
2024-06-27 13:11:39.489028: Pseudo dice [0.5]
2024-06-27 13:11:39.489405: Epoch time: 64.4 s
2024-06-27 13:11:41.816123: 
2024-06-27 13:11:41.816848: Epoch 51
2024-06-27 13:11:41.817490: Current learning rate: 0.00053
2024-06-27 13:12:46.547199: meanmse:       0.02108894
2024-06-27 13:12:46.548726: meanr2:        0.8300353081285675
2024-06-27 13:12:46.549464: train_loss 0.5621
2024-06-27 13:12:46.550014: val_loss 0.6482
2024-06-27 13:12:46.550676: Pseudo dice [0.5]
2024-06-27 13:12:46.551279: Epoch time: 64.74 s
2024-06-27 13:12:48.653198: 
2024-06-27 13:12:48.654155: Epoch 52
2024-06-27 13:12:48.654787: Current learning rate: 0.00052
2024-06-27 13:13:53.130358: meanmse:       0.018598529
2024-06-27 13:13:53.131451: meanr2:        0.8492808543530107
2024-06-27 13:13:53.132094: train_loss 0.5728
2024-06-27 13:13:53.132541: val_loss 0.5965
2024-06-27 13:13:53.132982: Pseudo dice [0.5]
2024-06-27 13:13:53.133476: Epoch time: 64.49 s
2024-06-27 13:13:55.014497: 
2024-06-27 13:13:55.015126: Epoch 53
2024-06-27 13:13:55.015518: Current learning rate: 0.00051
2024-06-27 13:14:59.515316: meanmse:       0.01911565
2024-06-27 13:14:59.516267: meanr2:        0.8468185940090648
2024-06-27 13:14:59.516788: train_loss 0.5903
2024-06-27 13:14:59.517223: val_loss 0.6323
2024-06-27 13:14:59.517665: Pseudo dice [0.5]
2024-06-27 13:14:59.518132: Epoch time: 64.51 s
2024-06-27 13:15:01.483838: 
2024-06-27 13:15:01.484614: Epoch 54
2024-06-27 13:15:01.485089: Current learning rate: 0.0005
2024-06-27 13:16:05.903372: meanmse:       0.021734847
2024-06-27 13:16:05.904565: meanr2:        0.8219236555841575
2024-06-27 13:16:05.905126: train_loss 0.5944
2024-06-27 13:16:05.905502: val_loss 0.6659
2024-06-27 13:16:05.905876: Pseudo dice [0.5]
2024-06-27 13:16:05.906265: Epoch time: 64.43 s
2024-06-27 13:16:08.382370: 
2024-06-27 13:16:08.383123: Epoch 55
2024-06-27 13:16:08.383630: Current learning rate: 0.00049
2024-06-27 13:17:12.751089: meanmse:       0.020375384
2024-06-27 13:17:12.753082: meanr2:        0.8351998746278877
2024-06-27 13:17:12.753939: train_loss 0.5952
2024-06-27 13:17:12.754734: val_loss 0.6385
2024-06-27 13:17:12.755367: Pseudo dice [0.5]
2024-06-27 13:17:12.756085: Epoch time: 64.38 s
2024-06-27 13:17:14.885055: 
2024-06-27 13:17:14.885889: Epoch 56
2024-06-27 13:17:14.886494: Current learning rate: 0.00048
2024-06-27 13:18:19.481225: meanmse:       0.018537167
2024-06-27 13:18:19.482528: meanr2:        0.8485943396942013
2024-06-27 13:18:19.483101: train_loss 0.5812
2024-06-27 13:18:19.483578: val_loss 0.5895
2024-06-27 13:18:19.484009: Pseudo dice [0.5]
2024-06-27 13:18:19.484459: Epoch time: 64.6 s
2024-06-27 13:18:21.457346: 
2024-06-27 13:18:21.458463: Epoch 57
2024-06-27 13:18:21.460268: Current learning rate: 0.00047
2024-06-27 13:19:26.310664: meanmse:       0.020939544
2024-06-27 13:19:26.311860: meanr2:        0.8287639441311904
2024-06-27 13:19:26.312479: train_loss 0.5752
2024-06-27 13:19:26.312941: val_loss 0.6384
2024-06-27 13:19:26.313394: Pseudo dice [0.5]
2024-06-27 13:19:26.313862: Epoch time: 64.87 s
2024-06-27 13:19:28.689708: 
2024-06-27 13:19:28.690370: Epoch 58
2024-06-27 13:19:28.690783: Current learning rate: 0.00046
2024-06-27 13:20:32.981524: meanmse:       0.018749395
2024-06-27 13:20:32.982472: meanr2:        0.8497776095597224
2024-06-27 13:20:32.982936: train_loss 0.6045
2024-06-27 13:20:32.983327: val_loss 0.577
2024-06-27 13:20:32.983692: Pseudo dice [0.5]
2024-06-27 13:20:32.984073: Epoch time: 64.3 s
2024-06-27 13:20:35.128012: 
2024-06-27 13:20:35.129043: Epoch 59
2024-06-27 13:20:35.129777: Current learning rate: 0.00045
2024-06-27 13:21:39.399424: meanmse:       0.022561619
2024-06-27 13:21:39.403443: meanr2:        0.8196091796722779
2024-06-27 13:21:39.409003: train_loss 0.5548
2024-06-27 13:21:39.409646: val_loss 0.6746
2024-06-27 13:21:39.410088: Pseudo dice [0.5]
2024-06-27 13:21:39.410532: Epoch time: 64.29 s
2024-06-27 13:21:41.670620: 
2024-06-27 13:21:41.671371: Epoch 60
2024-06-27 13:21:41.671859: Current learning rate: 0.00044
2024-06-27 13:22:45.894482: meanmse:       0.018284949
2024-06-27 13:22:45.895380: meanr2:        0.8503435589180907
2024-06-27 13:22:45.895829: train_loss 0.5643
2024-06-27 13:22:45.896189: val_loss 0.5967
2024-06-27 13:22:45.896529: Pseudo dice [0.5]
2024-06-27 13:22:45.896879: Epoch time: 64.23 s
2024-06-27 13:22:48.075346: 
2024-06-27 13:22:48.076089: Epoch 61
2024-06-27 13:22:48.076680: Current learning rate: 0.00043
2024-06-27 13:23:52.013859: meanmse:       0.022432478
2024-06-27 13:23:52.014871: meanr2:        0.8170949072743876
2024-06-27 13:23:52.015303: train_loss 0.5822
2024-06-27 13:23:52.015689: val_loss 0.6446
2024-06-27 13:23:52.016040: Pseudo dice [0.5]
2024-06-27 13:23:52.016493: Epoch time: 63.95 s
2024-06-27 13:23:54.090574: 
2024-06-27 13:23:54.091296: Epoch 62
2024-06-27 13:23:54.091742: Current learning rate: 0.00042
2024-06-27 13:24:58.639664: meanmse:       0.018929062
2024-06-27 13:24:58.640857: meanr2:        0.8497165322251661
2024-06-27 13:24:58.641553: train_loss 0.5524
2024-06-27 13:24:58.642085: val_loss 0.6062
2024-06-27 13:24:58.642639: Pseudo dice [0.5]
2024-06-27 13:24:58.643186: Epoch time: 64.56 s
2024-06-27 13:25:00.721271: 
2024-06-27 13:25:00.722422: Epoch 63
2024-06-27 13:25:00.723150: Current learning rate: 0.00041
2024-06-27 13:26:04.867404: meanmse:       0.01799284
2024-06-27 13:26:04.868774: meanr2:        0.8558886068152447
2024-06-27 13:26:04.869478: train_loss 0.5378
2024-06-27 13:26:04.870043: val_loss 0.5782
2024-06-27 13:26:04.870459: Pseudo dice [0.5]
2024-06-27 13:26:04.870855: Epoch time: 64.16 s
2024-06-27 13:26:07.219204: 
2024-06-27 13:26:07.220160: Epoch 64
2024-06-27 13:26:07.220830: Current learning rate: 0.0004
2024-06-27 13:27:11.681023: meanmse:       0.023654861
2024-06-27 13:27:11.688153: meanr2:        0.8094688834295574
2024-06-27 13:27:11.688728: train_loss 0.5375
2024-06-27 13:27:11.689152: val_loss 0.6942
2024-06-27 13:27:11.689679: Pseudo dice [0.5]
2024-06-27 13:27:11.690163: Epoch time: 64.48 s
2024-06-27 13:27:13.646370: 
2024-06-27 13:27:13.647366: Epoch 65
2024-06-27 13:27:13.648062: Current learning rate: 0.00039
2024-06-27 13:28:18.010851: meanmse:       0.02163329
2024-06-27 13:28:18.011959: meanr2:        0.8292742224555932
2024-06-27 13:28:18.012421: train_loss 0.5383
2024-06-27 13:28:18.012824: val_loss 0.6511
2024-06-27 13:28:18.013261: Pseudo dice [0.5]
2024-06-27 13:28:18.013875: Epoch time: 64.38 s
2024-06-27 13:28:20.084321: 
2024-06-27 13:28:20.084939: Epoch 66
2024-06-27 13:28:20.085527: Current learning rate: 0.00038
2024-06-27 13:29:24.337688: meanmse:       0.01723329
2024-06-27 13:29:24.339019: meanr2:        0.860554823940613
2024-06-27 13:29:24.339598: train_loss 0.5361
2024-06-27 13:29:24.340178: val_loss 0.5849
2024-06-27 13:29:24.340789: Pseudo dice [0.5]
2024-06-27 13:29:24.341311: Epoch time: 64.26 s
2024-06-27 13:29:26.335099: 
2024-06-27 13:29:26.335857: Epoch 67
2024-06-27 13:29:26.336327: Current learning rate: 0.00037
2024-06-27 13:30:30.955778: meanmse:       0.01596147
2024-06-27 13:30:30.957194: meanr2:        0.870178252385384
2024-06-27 13:30:30.957896: train_loss 0.4869
2024-06-27 13:30:30.958452: val_loss 0.5093
2024-06-27 13:30:30.958976: Pseudo dice [0.5]
2024-06-27 13:30:30.959517: Epoch time: 64.63 s
2024-06-27 13:30:30.960179: Yayy! New best R2: 0.8702
2024-06-27 13:30:33.439654: 
2024-06-27 13:30:33.440728: Epoch 68
2024-06-27 13:30:33.441441: Current learning rate: 0.00036
2024-06-27 13:31:38.162529: meanmse:       0.016641976
2024-06-27 13:31:38.163941: meanr2:        0.8678319389121281
2024-06-27 13:31:38.164653: train_loss 0.5545
2024-06-27 13:31:38.165198: val_loss 0.5371
2024-06-27 13:31:38.165764: Pseudo dice [0.5]
2024-06-27 13:31:38.166342: Epoch time: 64.73 s
2024-06-27 13:31:40.314212: 
2024-06-27 13:31:40.314951: Epoch 69
2024-06-27 13:31:40.315562: Current learning rate: 0.00035
2024-06-27 13:32:44.572649: meanmse:       0.017588036
2024-06-27 13:32:44.575294: meanr2:        0.8565111717703051
2024-06-27 13:32:44.576074: train_loss 0.5165
2024-06-27 13:32:44.576666: val_loss 0.5649
2024-06-27 13:32:44.578262: Pseudo dice [0.5]
2024-06-27 13:32:44.578842: Epoch time: 64.27 s
2024-06-27 13:32:47.237179: 
2024-06-27 13:32:47.242229: Epoch 70
2024-06-27 13:32:47.243107: Current learning rate: 0.00034
2024-06-27 13:33:51.495159: meanmse:       0.016261062
2024-06-27 13:33:51.496387: meanr2:        0.8683748326292093
2024-06-27 13:33:51.497004: train_loss 0.5361
2024-06-27 13:33:51.497437: val_loss 0.5608
2024-06-27 13:33:51.497886: Pseudo dice [0.5]
2024-06-27 13:33:51.498321: Epoch time: 64.27 s
2024-06-27 13:33:53.593628: 
2024-06-27 13:33:53.594537: Epoch 71
2024-06-27 13:33:53.595245: Current learning rate: 0.00033
2024-06-27 13:34:57.894437: meanmse:       0.018312305
2024-06-27 13:34:57.895643: meanr2:        0.8493139878667081
2024-06-27 13:34:57.896241: train_loss 0.5286
2024-06-27 13:34:57.896711: val_loss 0.5753
2024-06-27 13:34:57.897157: Pseudo dice [0.5]
2024-06-27 13:34:57.897619: Epoch time: 64.31 s
2024-06-27 13:34:59.914269: 
2024-06-27 13:34:59.915083: Epoch 72
2024-06-27 13:34:59.915555: Current learning rate: 0.00032
2024-06-27 13:36:04.105095: meanmse:       0.015746169
2024-06-27 13:36:04.106383: meanr2:        0.8736145262307752
2024-06-27 13:36:04.107033: train_loss 0.5061
2024-06-27 13:36:04.107493: val_loss 0.5305
2024-06-27 13:36:04.107937: Pseudo dice [0.5]
2024-06-27 13:36:04.108390: Epoch time: 64.2 s
2024-06-27 13:36:04.108860: Yayy! New best R2: 0.8736
2024-06-27 13:36:06.915848: 
2024-06-27 13:36:06.916632: Epoch 73
2024-06-27 13:36:06.917213: Current learning rate: 0.00031
2024-06-27 13:37:11.568855: meanmse:       0.015850456
2024-06-27 13:37:11.570143: meanr2:        0.8700203399609658
2024-06-27 13:37:11.570672: train_loss 0.5242
2024-06-27 13:37:11.571122: val_loss 0.5385
2024-06-27 13:37:11.571560: Pseudo dice [0.5]
2024-06-27 13:37:11.572015: Epoch time: 64.66 s
2024-06-27 13:37:13.676091: 
2024-06-27 13:37:13.676858: Epoch 74
2024-06-27 13:37:13.677376: Current learning rate: 0.0003
2024-06-27 13:38:17.759348: meanmse:       0.017592998
2024-06-27 13:38:17.761037: meanr2:        0.8558823650953503
2024-06-27 13:38:17.761748: train_loss 0.5091
2024-06-27 13:38:17.762343: val_loss 0.5721
2024-06-27 13:38:17.762798: Pseudo dice [0.5]
2024-06-27 13:38:17.763247: Epoch time: 64.09 s
2024-06-27 13:38:19.901922: 
2024-06-27 13:38:19.902585: Epoch 75
2024-06-27 13:38:19.903051: Current learning rate: 0.00029
2024-06-27 13:39:24.029555: meanmse:       0.022215683
2024-06-27 13:39:24.030915: meanr2:        0.8194733976002979
2024-06-27 13:39:24.031518: train_loss 0.4979
2024-06-27 13:39:24.032023: val_loss 0.648
2024-06-27 13:39:24.032486: Pseudo dice [0.5]
2024-06-27 13:39:24.032977: Epoch time: 64.14 s
2024-06-27 13:39:26.440859: 
2024-06-27 13:39:26.441807: Epoch 76
2024-06-27 13:39:26.442356: Current learning rate: 0.00028
2024-06-27 13:40:32.155394: meanmse:       0.014257107
2024-06-27 13:40:32.156680: meanr2:        0.8818800734510369
2024-06-27 13:40:32.157305: train_loss 0.5335
2024-06-27 13:40:32.157826: val_loss 0.4933
2024-06-27 13:40:32.158300: Pseudo dice [0.5]
2024-06-27 13:40:32.158792: Epoch time: 65.73 s
2024-06-27 13:40:32.159280: Yayy! New best R2: 0.8819
2024-06-27 13:40:34.464954: 
2024-06-27 13:40:34.465895: Epoch 77
2024-06-27 13:40:34.466506: Current learning rate: 0.00027
2024-06-27 13:41:38.629750: meanmse:       0.01613956
2024-06-27 13:41:38.631288: meanr2:        0.8692602066302424
2024-06-27 13:41:38.631985: train_loss 0.5072
2024-06-27 13:41:38.632532: val_loss 0.5345
2024-06-27 13:41:38.633066: Pseudo dice [0.5]
2024-06-27 13:41:38.633571: Epoch time: 64.17 s
2024-06-27 13:41:40.785434: 
2024-06-27 13:41:40.786077: Epoch 78
2024-06-27 13:41:40.786487: Current learning rate: 0.00026
2024-06-27 13:42:45.481175: meanmse:       0.01943746
2024-06-27 13:42:45.482376: meanr2:        0.8416514260024145
2024-06-27 13:42:45.482939: train_loss 0.508
2024-06-27 13:42:45.483408: val_loss 0.5953
2024-06-27 13:42:45.483885: Pseudo dice [0.5]
2024-06-27 13:42:45.484355: Epoch time: 64.7 s
2024-06-27 13:42:47.695530: 
2024-06-27 13:42:47.696303: Epoch 79
2024-06-27 13:42:47.696818: Current learning rate: 0.00025
2024-06-27 13:43:52.394813: meanmse:       0.01597047
2024-06-27 13:43:52.396255: meanr2:        0.8741495298401344
2024-06-27 13:43:52.396829: train_loss 0.5273
2024-06-27 13:43:52.397365: val_loss 0.5327
2024-06-27 13:43:52.397875: Pseudo dice [0.5]
2024-06-27 13:43:52.398357: Epoch time: 64.71 s
2024-06-27 13:43:54.829411: 
2024-06-27 13:43:54.830411: Epoch 80
2024-06-27 13:43:54.831068: Current learning rate: 0.00023
2024-06-27 13:44:59.441851: meanmse:       0.018989049
2024-06-27 13:44:59.443111: meanr2:        0.8481332742455033
2024-06-27 13:44:59.443757: train_loss 0.444
2024-06-27 13:44:59.444294: val_loss 0.5859
2024-06-27 13:44:59.444779: Pseudo dice [0.5]
2024-06-27 13:44:59.445293: Epoch time: 64.62 s
2024-06-27 13:45:01.893263: 
2024-06-27 13:45:01.893994: Epoch 81
2024-06-27 13:45:01.894404: Current learning rate: 0.00022
2024-06-27 13:46:06.246724: meanmse:       0.018161098
2024-06-27 13:46:06.248049: meanr2:        0.8531095250723791
2024-06-27 13:46:06.248697: train_loss 0.4787
2024-06-27 13:46:06.249205: val_loss 0.5766
2024-06-27 13:46:06.249691: Pseudo dice [0.5]
2024-06-27 13:46:06.250200: Epoch time: 64.36 s
2024-06-27 13:46:08.403060: 
2024-06-27 13:46:08.403838: Epoch 82
2024-06-27 13:46:08.404240: Current learning rate: 0.00021
2024-06-27 13:47:13.080093: meanmse:       0.021380814
2024-06-27 13:47:13.081258: meanr2:        0.8281967721101274
2024-06-27 13:47:13.081784: train_loss 0.4894
2024-06-27 13:47:13.082238: val_loss 0.6215
2024-06-27 13:47:13.082676: Pseudo dice [0.5]
2024-06-27 13:47:13.083147: Epoch time: 64.69 s
2024-06-27 13:47:14.952886: 
2024-06-27 13:47:14.953928: Epoch 83
2024-06-27 13:47:14.954608: Current learning rate: 0.0002
2024-06-27 13:48:19.392883: meanmse:       0.021014594
2024-06-27 13:48:19.393968: meanr2:        0.8294051572317689
2024-06-27 13:48:19.394495: train_loss 0.4533
2024-06-27 13:48:19.394934: val_loss 0.6343
2024-06-27 13:48:19.395391: Pseudo dice [0.5]
2024-06-27 13:48:19.395836: Epoch time: 64.45 s
2024-06-27 13:48:21.455489: 
2024-06-27 13:48:21.456447: Epoch 84
2024-06-27 13:48:21.457087: Current learning rate: 0.00019
2024-06-27 13:49:25.906262: meanmse:       0.021051599
2024-06-27 13:49:25.907617: meanr2:        0.8334126108676302
2024-06-27 13:49:25.908224: train_loss 0.4573
2024-06-27 13:49:25.908724: val_loss 0.6021
2024-06-27 13:49:25.909280: Pseudo dice [0.5]
2024-06-27 13:49:25.909815: Epoch time: 64.46 s
2024-06-27 13:49:27.892242: 
2024-06-27 13:49:27.893200: Epoch 85
2024-06-27 13:49:27.893827: Current learning rate: 0.00018
2024-06-27 13:50:32.277162: meanmse:       0.018607944
2024-06-27 13:50:32.278518: meanr2:        0.8471697908870156
2024-06-27 13:50:32.279155: train_loss 0.4535
2024-06-27 13:50:32.279653: val_loss 0.5698
2024-06-27 13:50:32.280245: Pseudo dice [0.5]
2024-06-27 13:50:32.280837: Epoch time: 64.39 s
2024-06-27 13:50:34.251463: 
2024-06-27 13:50:34.252301: Epoch 86
2024-06-27 13:50:34.252815: Current learning rate: 0.00017
2024-06-27 13:51:38.523838: meanmse:       0.014742333
2024-06-27 13:51:38.525301: meanr2:        0.8792104451013336
2024-06-27 13:51:38.525942: train_loss 0.4358
2024-06-27 13:51:38.526461: val_loss 0.4874
2024-06-27 13:51:38.526959: Pseudo dice [0.5]
2024-06-27 13:51:38.527522: Epoch time: 64.28 s
2024-06-27 13:51:40.660116: 
2024-06-27 13:51:40.661225: Epoch 87
2024-06-27 13:51:40.661960: Current learning rate: 0.00016
2024-06-27 13:52:45.138114: meanmse:       0.016899733
2024-06-27 13:52:45.139312: meanr2:        0.8687353766449537
2024-06-27 13:52:45.139884: train_loss 0.4512
2024-06-27 13:52:45.140481: val_loss 0.5299
2024-06-27 13:52:45.140943: Pseudo dice [0.5]
2024-06-27 13:52:45.141404: Epoch time: 64.49 s
2024-06-27 13:52:47.107628: 
2024-06-27 13:52:47.108345: Epoch 88
2024-06-27 13:52:47.108840: Current learning rate: 0.00015
2024-06-27 13:53:51.454414: meanmse:       0.01855564
2024-06-27 13:53:51.455781: meanr2:        0.8530929320100786
2024-06-27 13:53:51.456424: train_loss 0.4525
2024-06-27 13:53:51.456934: val_loss 0.5662
2024-06-27 13:53:51.457415: Pseudo dice [0.5]
2024-06-27 13:53:51.457922: Epoch time: 64.35 s
2024-06-27 13:53:53.389992: 
2024-06-27 13:53:53.390584: Epoch 89
2024-06-27 13:53:53.391029: Current learning rate: 0.00014
2024-06-27 13:54:57.715372: meanmse:       0.019341588
2024-06-27 13:54:57.716362: meanr2:        0.8424542972238037
2024-06-27 13:54:57.716831: train_loss 0.4626
2024-06-27 13:54:57.717321: val_loss 0.5769
2024-06-27 13:54:57.717855: Pseudo dice [0.5]
2024-06-27 13:54:57.718675: Epoch time: 64.33 s
2024-06-27 13:55:00.481375: 
2024-06-27 13:55:00.482068: Epoch 90
2024-06-27 13:55:00.482537: Current learning rate: 0.00013
2024-06-27 13:56:04.936753: meanmse:       0.021464255
2024-06-27 13:56:04.938037: meanr2:        0.8245046992061635
2024-06-27 13:56:04.938745: train_loss 0.4437
2024-06-27 13:56:04.939333: val_loss 0.5984
2024-06-27 13:56:04.939899: Pseudo dice [0.5]
2024-06-27 13:56:04.940504: Epoch time: 64.46 s
2024-06-27 13:56:07.015931: 
2024-06-27 13:56:07.016804: Epoch 91
2024-06-27 13:56:07.017659: Current learning rate: 0.00011
2024-06-27 13:57:11.251729: meanmse:       0.01873189
2024-06-27 13:57:11.253081: meanr2:        0.8487971414195239
2024-06-27 13:57:11.253783: train_loss 0.4604
2024-06-27 13:57:11.254331: val_loss 0.5504
2024-06-27 13:57:11.254797: Pseudo dice [0.5]
2024-06-27 13:57:11.255309: Epoch time: 64.24 s
2024-06-27 13:57:13.155721: 
2024-06-27 13:57:13.156925: Epoch 92
2024-06-27 13:57:13.157596: Current learning rate: 0.0001
2024-06-27 13:58:17.289311: meanmse:       0.017927771
2024-06-27 13:58:17.290616: meanr2:        0.8544696142296168
2024-06-27 13:58:17.291083: train_loss 0.4083
2024-06-27 13:58:17.291514: val_loss 0.5506
2024-06-27 13:58:17.291903: Pseudo dice [0.5]
2024-06-27 13:58:17.292308: Epoch time: 64.14 s
2024-06-27 13:58:19.155572: 
2024-06-27 13:58:19.156557: Epoch 93
2024-06-27 13:58:19.157186: Current learning rate: 9e-05
2024-06-27 13:59:23.908577: meanmse:       0.019650506
2024-06-27 13:59:23.909868: meanr2:        0.8425142573515391
2024-06-27 13:59:23.910754: train_loss 0.4181
2024-06-27 13:59:23.911317: val_loss 0.5943
2024-06-27 13:59:23.911787: Pseudo dice [0.5]
2024-06-27 13:59:23.912305: Epoch time: 64.76 s
2024-06-27 13:59:25.819159: 
2024-06-27 13:59:25.820260: Epoch 94
2024-06-27 13:59:25.820813: Current learning rate: 8e-05
2024-06-27 14:00:30.235286: meanmse:       0.02295789
2024-06-27 14:00:30.236536: meanr2:        0.8178552234853984
2024-06-27 14:00:30.237118: train_loss 0.4101
2024-06-27 14:00:30.237638: val_loss 0.6414
2024-06-27 14:00:30.238220: Pseudo dice [0.5]
2024-06-27 14:00:30.239003: Epoch time: 64.42 s
2024-06-27 14:00:32.307103: 
2024-06-27 14:00:32.307881: Epoch 95
2024-06-27 14:00:32.308436: Current learning rate: 7e-05
2024-06-27 14:01:36.803621: meanmse:       0.016098687
2024-06-27 14:01:36.804990: meanr2:        0.8684414664166371
2024-06-27 14:01:36.805603: train_loss 0.4461
2024-06-27 14:01:36.806078: val_loss 0.5027
2024-06-27 14:01:36.806540: Pseudo dice [0.5]
2024-06-27 14:01:36.807019: Epoch time: 64.5 s
2024-06-27 14:01:38.777051: 
2024-06-27 14:01:38.777737: Epoch 96
2024-06-27 14:01:38.778255: Current learning rate: 6e-05
2024-06-27 14:02:42.914173: meanmse:       0.020006543
2024-06-27 14:02:42.915304: meanr2:        0.8406324089300995
2024-06-27 14:02:42.915781: train_loss 0.4489
2024-06-27 14:02:42.916202: val_loss 0.577
2024-06-27 14:02:42.916655: Pseudo dice [0.5]
2024-06-27 14:02:42.917113: Epoch time: 64.15 s
2024-06-27 14:02:45.389270: 
2024-06-27 14:02:45.390027: Epoch 97
2024-06-27 14:02:45.390493: Current learning rate: 4e-05
2024-06-27 14:03:49.714082: meanmse:       0.014931737
2024-06-27 14:03:49.715387: meanr2:        0.8784389061959668
2024-06-27 14:03:49.716015: train_loss 0.4288
2024-06-27 14:03:49.716548: val_loss 0.4718
2024-06-27 14:03:49.717118: Pseudo dice [0.5]
2024-06-27 14:03:49.717828: Epoch time: 64.33 s
2024-06-27 14:03:51.764138: 
2024-06-27 14:03:51.765008: Epoch 98
2024-06-27 14:03:51.765549: Current learning rate: 3e-05
2024-06-27 14:04:56.366209: meanmse:       0.014469083
2024-06-27 14:04:56.367513: meanr2:        0.8797098202147283
2024-06-27 14:04:56.368144: train_loss 0.4606
2024-06-27 14:04:56.368660: val_loss 0.4721
2024-06-27 14:04:56.369194: Pseudo dice [0.5]
2024-06-27 14:04:56.369709: Epoch time: 64.62 s
2024-06-27 14:04:58.493491: 
2024-06-27 14:04:58.494555: Epoch 99
2024-06-27 14:04:58.495193: Current learning rate: 2e-05
2024-06-27 14:06:02.969082: meanmse:       0.017570784
2024-06-27 14:06:02.970328: meanr2:        0.8600786846223533
2024-06-27 14:06:02.970908: train_loss 0.4164
2024-06-27 14:06:02.971380: val_loss 0.5408
2024-06-27 14:06:02.971836: Pseudo dice [0.5]
2024-06-27 14:06:02.972322: Epoch time: 64.49 s
2024-06-27 14:06:05.459492: Training done.
