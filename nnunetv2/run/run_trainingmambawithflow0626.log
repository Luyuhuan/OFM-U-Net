nohup: ignoring input
Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

UMambaEnc: UMambaEnc(
  (encoder): ResidualMambaEncoder(
    (stem): StackedConvBlocks(
      (convs): Sequential(
        (0): ConvDropoutNormReLU(
          (conv): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
          (all_modules): Sequential(
            (0): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
            (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (2): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (5): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              (1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (mamba_layers): ModuleList(
      (0): MambaLayer(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=32, out_features=128, bias=False)
          (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
          (act): SiLU()
          (x_proj): Linear(in_features=64, out_features=34, bias=False)
          (dt_proj): Linear(in_features=2, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=32, bias=False)
        )
      )
      (1): MambaLayer(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=64, out_features=256, bias=False)
          (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
          (act): SiLU()
          (x_proj): Linear(in_features=128, out_features=36, bias=False)
          (dt_proj): Linear(in_features=4, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=64, bias=False)
        )
      )
      (2-3): 2 x MambaLayer(
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=128, out_features=512, bias=False)
          (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
          (act): SiLU()
          (x_proj): Linear(in_features=256, out_features=40, bias=False)
          (dt_proj): Linear(in_features=8, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=128, bias=False)
        )
      )
      (4): MambaLayer(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=64, out_features=256, bias=False)
          (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
          (act): SiLU()
          (x_proj): Linear(in_features=128, out_features=36, bias=False)
          (dt_proj): Linear(in_features=4, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=64, bias=False)
        )
      )
      (5): MambaLayer(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=32, out_features=128, bias=False)
          (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
          (act): SiLU()
          (x_proj): Linear(in_features=64, out_features=34, bias=False)
          (dt_proj): Linear(in_features=2, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=32, bias=False)
        )
      )
    )
  )
  (decoder): UNetResDecoder(
    (encoder): ResidualMambaEncoder(
      (stem): StackedConvBlocks(
        (convs): Sequential(
          (0): ConvDropoutNormReLU(
            (conv): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
            (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
            (all_modules): Sequential(
              (0): Conv3d(5, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (stages): ModuleList(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (4): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (5): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2], padding=0)
                (1): ConvDropoutNormReLU(
                  (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (all_modules): Sequential(
                    (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                    (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  )
                )
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (mamba_layers): ModuleList(
        (0): MambaLayer(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=32, out_features=128, bias=False)
            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
            (act): SiLU()
            (x_proj): Linear(in_features=64, out_features=34, bias=False)
            (dt_proj): Linear(in_features=2, out_features=64, bias=True)
            (out_proj): Linear(in_features=64, out_features=32, bias=False)
          )
        )
        (1): MambaLayer(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (2-3): 2 x MambaLayer(
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
        )
        (4): MambaLayer(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (5): MambaLayer(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (mamba): Mamba(
            (in_proj): Linear(in_features=32, out_features=128, bias=False)
            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
            (act): SiLU()
            (x_proj): Linear(in_features=64, out_features=34, bias=False)
            (dt_proj): Linear(in_features=2, out_features=64, bias=True)
            (out_proj): Linear(in_features=64, out_features=32, bias=False)
          )
        )
      )
    )
    (stages): ModuleList(
      (0): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1-2): 2 x StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(256, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (3): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(128, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (4): StackedResidualBlocks(
        (blocks): Sequential(
          (0): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(64, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            (skip): Sequential(
              (0): ConvDropoutNormReLU(
                (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
            )
          )
          (1): BasicBlockD(
            (conv1): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (2): LeakyReLU(negative_slope=0.01, inplace=True)
              )
            )
            (conv2): ConvDropoutNormReLU(
              (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
              (norm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              (all_modules): Sequential(
                (0): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
                (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
              )
            )
            (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (transpconvs): ModuleList(
      (0): ConvTranspose3d(32, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (1): ConvTranspose3d(64, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (2): ConvTranspose3d(128, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (3): ConvTranspose3d(128, 64, kernel_size=(1, 2, 2), stride=(1, 2, 2))
      (4): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2))
    )
    (lzz_layers): ModuleList(
      (0): ModuleList(
        (0): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (1-2): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (1): ModuleList(
        (0-1): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2-3): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (2): ModuleList(
        (0-2): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (3): ModuleList(
        (0-2): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (3): StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
      (4): ModuleList(
        (0-1): 2 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
        (2-4): 3 x StackedResidualBlocks(
          (blocks): Sequential(
            (0): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
              (skip): Sequential(
                (0): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)
              )
            )
            (1): BasicBlockD(
              (conv1): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (nonlin): LeakyReLU(negative_slope=0.01, inplace=True)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                  (2): LeakyReLU(negative_slope=0.01, inplace=True)
                )
              )
              (conv2): ConvDropoutNormReLU(
                (conv): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                (norm): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                (all_modules): Sequential(
                  (0): Conv3d(40, 40, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
                  (1): InstanceNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
                )
              )
              (nonlin2): LeakyReLU(negative_slope=0.01, inplace=True)
            )
          )
        )
      )
    )
    (lzz_fc): ModuleList(
      (0-4): 5 x ModuleList(
        (0): Sequential(
          (0): Linear(in_features=288, out_features=72, bias=True)
          (1): Tanh()
        )
        (1): Sequential(
          (0): Linear(in_features=72, out_features=18, bias=True)
          (1): Tanh()
        )
        (2): Sequential(
          (0): Linear(in_features=18, out_features=1, bias=True)
          (1): Tanh()
        )
      )
    )
  )
)

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [40, 192, 192], 'median_image_size_in_voxels': [97.0, 512.0, 512.0], 'spacing': [2.5, 0.7958984971046448, 0.7958984971046448], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [3, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset701_AbdomenCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [2.5, 0.7958984971046448, 0.7958984971046448], 'original_median_shape_after_transp': [97, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3071.0, 'mean': 97.29716491699219, 'median': 118.0, 'min': -1024.0, 'percentile_00_5': -958.0, 'percentile_99_5': 270.0, 'std': 137.8484649658203}}} 

2024-06-26 21:16:29.808348: unpacking dataset...
2024-06-26 21:16:29.808693: unpacking done...
2024-06-26 21:16:29.809485: do_dummy_2d_data_aug: False
2024-06-26 21:16:29.824229: Unable to plot network architecture:
2024-06-26 21:16:29.824644: No module named 'hiddenlayer'
2024-06-26 21:16:29.839594: 
2024-06-26 21:16:29.840252: Epoch 0
2024-06-26 21:16:29.840692: Current learning rate: 0.001
using pin_memory on device 0
using pin_memory on device 0
2024-06-26 21:17:49.478117: meanmse:       0.11143431
2024-06-26 21:17:49.480067: meanr2:        0.10470444760603365
2024-06-26 21:17:49.480760: train_loss 2.8281
2024-06-26 21:17:49.481354: val_loss 2.3525
2024-06-26 21:17:49.481794: Pseudo dice [0.5]
2024-06-26 21:17:49.482425: Epoch time: 79.65 s
2024-06-26 21:17:49.482919: Yayy! New best R2: 0.1047
2024-06-26 21:17:51.509112: 
2024-06-26 21:17:51.509880: Epoch 1
2024-06-26 21:17:51.510448: Current learning rate: 0.00099
2024-06-26 21:18:55.310570: meanmse:       0.08961714
2024-06-26 21:18:55.311869: meanr2:        0.26755673602593383
2024-06-26 21:18:55.312544: train_loss 2.1359
2024-06-26 21:18:55.313057: val_loss 1.9745
2024-06-26 21:18:55.313514: Pseudo dice [0.5]
2024-06-26 21:18:55.314033: Epoch time: 63.81 s
2024-06-26 21:18:55.314543: Yayy! New best R2: 0.2676
2024-06-26 21:18:57.472794: 
2024-06-26 21:18:57.473541: Epoch 2
2024-06-26 21:18:57.474046: Current learning rate: 0.00098
2024-06-26 21:20:01.162599: meanmse:       0.08133937
2024-06-26 21:20:01.164699: meanr2:        0.3314034440588088
2024-06-26 21:20:01.165403: train_loss 1.9366
2024-06-26 21:20:01.165893: val_loss 1.7804
2024-06-26 21:20:01.166365: Pseudo dice [0.5]
2024-06-26 21:20:01.166855: Epoch time: 63.7 s
2024-06-26 21:20:01.167296: Yayy! New best R2: 0.3314
2024-06-26 21:20:04.086735: 
2024-06-26 21:20:04.087421: Epoch 3
2024-06-26 21:20:04.087927: Current learning rate: 0.00097
2024-06-26 21:21:07.424691: meanmse:       0.051729836
2024-06-26 21:21:07.426178: meanr2:        0.5850405810245648
2024-06-26 21:21:07.426876: train_loss 1.5954
2024-06-26 21:21:07.427392: val_loss 1.3737
2024-06-26 21:21:07.427889: Pseudo dice [0.5]
2024-06-26 21:21:07.428424: Epoch time: 63.35 s
2024-06-26 21:21:07.429020: Yayy! New best R2: 0.585
2024-06-26 21:21:09.500823: 
2024-06-26 21:21:09.501757: Epoch 4
2024-06-26 21:21:09.502568: Current learning rate: 0.00096
2024-06-26 21:22:13.309092: meanmse:       0.0530095
2024-06-26 21:22:13.310590: meanr2:        0.5641019030802712
2024-06-26 21:22:13.311200: train_loss 1.298
2024-06-26 21:22:13.311754: val_loss 1.3608
2024-06-26 21:22:13.312254: Pseudo dice [0.5]
2024-06-26 21:22:13.312797: Epoch time: 63.82 s
2024-06-26 21:22:15.202317: 
2024-06-26 21:22:15.202971: Epoch 5
2024-06-26 21:22:15.203429: Current learning rate: 0.00095
2024-06-26 21:23:18.879957: meanmse:       0.052025344
2024-06-26 21:23:18.881116: meanr2:        0.5819389759003921
2024-06-26 21:23:18.881658: train_loss 1.2368
2024-06-26 21:23:18.882179: val_loss 1.3153
2024-06-26 21:23:18.882622: Pseudo dice [0.5]
2024-06-26 21:23:18.883119: Epoch time: 63.69 s
2024-06-26 21:23:20.706249: 
2024-06-26 21:23:20.706973: Epoch 6
2024-06-26 21:23:20.707466: Current learning rate: 0.00095
2024-06-26 21:24:24.210839: meanmse:       0.036867008
2024-06-26 21:24:24.211829: meanr2:        0.6962603341774296
2024-06-26 21:24:24.212379: train_loss 1.0747
2024-06-26 21:24:24.212792: val_loss 1.0247
2024-06-26 21:24:24.213208: Pseudo dice [0.5]
2024-06-26 21:24:24.213606: Epoch time: 63.51 s
2024-06-26 21:24:24.214005: Yayy! New best R2: 0.6963
2024-06-26 21:24:26.420582: 
2024-06-26 21:24:26.421370: Epoch 7
2024-06-26 21:24:26.421906: Current learning rate: 0.00094
2024-06-26 21:25:30.105808: meanmse:       0.027884107
2024-06-26 21:25:30.107025: meanr2:        0.7738052309374328
2024-06-26 21:25:30.107553: train_loss 0.9853
2024-06-26 21:25:30.107990: val_loss 0.8793
2024-06-26 21:25:30.108430: Pseudo dice [0.5]
2024-06-26 21:25:30.108874: Epoch time: 63.69 s
2024-06-26 21:25:30.109318: Yayy! New best R2: 0.7738
2024-06-26 21:25:32.484696: 
2024-06-26 21:25:32.485543: Epoch 8
2024-06-26 21:25:32.486130: Current learning rate: 0.00093
2024-06-26 21:26:36.301863: meanmse:       0.035670355
2024-06-26 21:26:36.303121: meanr2:        0.7118726267784159
2024-06-26 21:26:36.303715: train_loss 0.9911
2024-06-26 21:26:36.304159: val_loss 0.9938
2024-06-26 21:26:36.304613: Pseudo dice [0.5]
2024-06-26 21:26:36.305055: Epoch time: 63.83 s
2024-06-26 21:26:38.178102: 
2024-06-26 21:26:38.178866: Epoch 9
2024-06-26 21:26:38.179436: Current learning rate: 0.00092
2024-06-26 21:27:42.441233: meanmse:       0.029892208
2024-06-26 21:27:42.442711: meanr2:        0.7591447159900049
2024-06-26 21:27:42.443449: train_loss 0.9046
2024-06-26 21:27:42.443895: val_loss 0.9017
2024-06-26 21:27:42.444316: Pseudo dice [0.5]
2024-06-26 21:27:42.444782: Epoch time: 64.27 s
2024-06-26 21:27:44.602545: 
2024-06-26 21:27:44.603431: Epoch 10
2024-06-26 21:27:44.604114: Current learning rate: 0.00091
2024-06-26 21:28:48.334955: meanmse:       0.028241789
2024-06-26 21:28:48.336556: meanr2:        0.7727853019481811
2024-06-26 21:28:48.337193: train_loss 0.9051
2024-06-26 21:28:48.337689: val_loss 0.8257
2024-06-26 21:28:48.338117: Pseudo dice [0.5]
2024-06-26 21:28:48.338578: Epoch time: 63.74 s
2024-06-26 21:28:50.365448: 
2024-06-26 21:28:50.366184: Epoch 11
2024-06-26 21:28:50.366636: Current learning rate: 0.0009
2024-06-26 21:29:53.899896: meanmse:       0.030043168
2024-06-26 21:29:53.901152: meanr2:        0.7562631289896682
2024-06-26 21:29:53.901722: train_loss 0.8546
2024-06-26 21:29:53.902179: val_loss 0.8761
2024-06-26 21:29:53.902605: Pseudo dice [0.5]
2024-06-26 21:29:53.903083: Epoch time: 63.54 s
2024-06-26 21:29:55.775465: 
2024-06-26 21:29:55.776194: Epoch 12
2024-06-26 21:29:55.776796: Current learning rate: 0.00089
2024-06-26 21:30:59.206192: meanmse:       0.0279118
2024-06-26 21:30:59.207192: meanr2:        0.7774157626878314
2024-06-26 21:30:59.207599: train_loss 0.8154
2024-06-26 21:30:59.207958: val_loss 0.831
2024-06-26 21:30:59.208361: Pseudo dice [0.5]
2024-06-26 21:30:59.208717: Epoch time: 63.44 s
2024-06-26 21:30:59.209062: Yayy! New best R2: 0.7774
2024-06-26 21:31:01.324025: 
2024-06-26 21:31:01.324799: Epoch 13
2024-06-26 21:31:01.325297: Current learning rate: 0.00088
2024-06-26 21:32:04.619436: meanmse:       0.026303362
2024-06-26 21:32:04.620939: meanr2:        0.7891419365744323
2024-06-26 21:32:04.621549: train_loss 0.8336
2024-06-26 21:32:04.622175: val_loss 0.8127
2024-06-26 21:32:04.622679: Pseudo dice [0.5]
2024-06-26 21:32:04.623192: Epoch time: 63.3 s
2024-06-26 21:32:04.623696: Yayy! New best R2: 0.7891
2024-06-26 21:32:06.807734: 
2024-06-26 21:32:06.808424: Epoch 14
2024-06-26 21:32:06.808944: Current learning rate: 0.00087
2024-06-26 21:33:10.306584: meanmse:       0.026655786
2024-06-26 21:33:10.307672: meanr2:        0.7858728352498275
2024-06-26 21:33:10.308169: train_loss 0.7852
2024-06-26 21:33:10.308564: val_loss 0.8217
2024-06-26 21:33:10.308949: Pseudo dice [0.5]
2024-06-26 21:33:10.309351: Epoch time: 63.51 s
2024-06-26 21:33:12.224522: 
2024-06-26 21:33:12.225301: Epoch 15
2024-06-26 21:33:12.225809: Current learning rate: 0.00086
2024-06-26 21:34:16.016432: meanmse:       0.026285795
2024-06-26 21:34:16.017691: meanr2:        0.7874849214728702
2024-06-26 21:34:16.018466: train_loss 0.82
2024-06-26 21:34:16.019028: val_loss 0.7902
2024-06-26 21:34:16.019513: Pseudo dice [0.5]
2024-06-26 21:34:16.019975: Epoch time: 63.8 s
2024-06-26 21:34:18.014821: 
2024-06-26 21:34:18.015449: Epoch 16
2024-06-26 21:34:18.015864: Current learning rate: 0.00085
2024-06-26 21:35:21.850080: meanmse:       0.026061576
2024-06-26 21:35:21.851611: meanr2:        0.7871345785328728
2024-06-26 21:35:21.852392: train_loss 0.8264
2024-06-26 21:35:21.852989: val_loss 0.7971
2024-06-26 21:35:21.853491: Pseudo dice [0.5]
2024-06-26 21:35:21.854013: Epoch time: 63.85 s
2024-06-26 21:35:23.929428: 
2024-06-26 21:35:23.930423: Epoch 17
2024-06-26 21:35:23.931016: Current learning rate: 0.00085
2024-06-26 21:36:27.275759: meanmse:       0.026808752
2024-06-26 21:36:27.276959: meanr2:        0.7839873990271568
2024-06-26 21:36:27.277484: train_loss 0.7222
2024-06-26 21:36:27.278119: val_loss 0.7947
2024-06-26 21:36:27.278550: Pseudo dice [0.5]
2024-06-26 21:36:27.278929: Epoch time: 63.36 s
2024-06-26 21:36:29.200472: 
2024-06-26 21:36:29.201514: Epoch 18
2024-06-26 21:36:29.202074: Current learning rate: 0.00084
2024-06-26 21:37:32.879735: meanmse:       0.035664048
2024-06-26 21:37:32.881022: meanr2:        0.7038192848661495
2024-06-26 21:37:32.881841: train_loss 0.7597
2024-06-26 21:37:32.882276: val_loss 0.927
2024-06-26 21:37:32.882693: Pseudo dice [0.5]
2024-06-26 21:37:32.883127: Epoch time: 63.69 s
2024-06-26 21:37:34.985436: 
2024-06-26 21:37:34.986280: Epoch 19
2024-06-26 21:37:34.986976: Current learning rate: 0.00083
2024-06-26 21:38:38.413227: meanmse:       0.026586343
2024-06-26 21:38:38.414333: meanr2:        0.786654330074898
2024-06-26 21:38:38.414846: train_loss 0.7587
2024-06-26 21:38:38.415307: val_loss 0.8037
2024-06-26 21:38:38.415751: Pseudo dice [0.5]
2024-06-26 21:38:38.416219: Epoch time: 63.44 s
2024-06-26 21:38:40.633443: 
2024-06-26 21:38:40.634160: Epoch 20
2024-06-26 21:38:40.634670: Current learning rate: 0.00082
2024-06-26 21:39:43.806482: meanmse:       0.023793042
2024-06-26 21:39:43.807914: meanr2:        0.80523354890353
2024-06-26 21:39:43.808449: train_loss 0.749
2024-06-26 21:39:43.808915: val_loss 0.7144
2024-06-26 21:39:43.809429: Pseudo dice [0.5]
2024-06-26 21:39:43.810008: Epoch time: 63.18 s
2024-06-26 21:39:43.810597: Yayy! New best R2: 0.8052
2024-06-26 21:39:46.112391: 
2024-06-26 21:39:46.113217: Epoch 21
2024-06-26 21:39:46.113815: Current learning rate: 0.00081
2024-06-26 21:40:49.675066: meanmse:       0.029899329
2024-06-26 21:40:49.676192: meanr2:        0.7617845849213091
2024-06-26 21:40:49.676723: train_loss 0.6607
2024-06-26 21:40:49.677381: val_loss 0.834
2024-06-26 21:40:49.678001: Pseudo dice [0.5]
2024-06-26 21:40:49.678479: Epoch time: 63.57 s
2024-06-26 21:40:51.477688: 
2024-06-26 21:40:51.478374: Epoch 22
2024-06-26 21:40:51.478874: Current learning rate: 0.0008
2024-06-26 21:41:55.367725: meanmse:       0.021580283
2024-06-26 21:41:55.368704: meanr2:        0.8213724324262389
2024-06-26 21:41:55.369160: train_loss 0.6839
2024-06-26 21:41:55.369575: val_loss 0.6673
2024-06-26 21:41:55.369960: Pseudo dice [0.5]
2024-06-26 21:41:55.370390: Epoch time: 63.9 s
2024-06-26 21:41:55.370783: Yayy! New best R2: 0.8214
2024-06-26 21:41:57.358644: 
2024-06-26 21:41:57.359425: Epoch 23
2024-06-26 21:41:57.359893: Current learning rate: 0.00079
2024-06-26 21:43:01.008718: meanmse:       0.028582655
2024-06-26 21:43:01.009821: meanr2:        0.7704080513112301
2024-06-26 21:43:01.010300: train_loss 0.7025
2024-06-26 21:43:01.010756: val_loss 0.7904
2024-06-26 21:43:01.011178: Pseudo dice [0.5]
2024-06-26 21:43:01.011619: Epoch time: 63.66 s
2024-06-26 21:43:02.920310: 
2024-06-26 21:43:02.921133: Epoch 24
2024-06-26 21:43:02.921635: Current learning rate: 0.00078
2024-06-26 21:44:06.161496: meanmse:       0.020112235
2024-06-26 21:44:06.162482: meanr2:        0.8386415694441062
2024-06-26 21:44:06.163030: train_loss 0.6343
2024-06-26 21:44:06.163431: val_loss 0.6509
2024-06-26 21:44:06.163892: Pseudo dice [0.5]
2024-06-26 21:44:06.164393: Epoch time: 63.25 s
2024-06-26 21:44:06.164875: Yayy! New best R2: 0.8386
2024-06-26 21:44:08.367509: 
2024-06-26 21:44:08.368237: Epoch 25
2024-06-26 21:44:08.368901: Current learning rate: 0.00077
2024-06-26 21:45:12.220675: meanmse:       0.024080265
2024-06-26 21:45:12.222013: meanr2:        0.8029938958158106
2024-06-26 21:45:12.222588: train_loss 0.7183
2024-06-26 21:45:12.223096: val_loss 0.7173
2024-06-26 21:45:12.223584: Pseudo dice [0.5]
2024-06-26 21:45:12.224088: Epoch time: 63.86 s
2024-06-26 21:45:14.004655: 
2024-06-26 21:45:14.005251: Epoch 26
2024-06-26 21:45:14.005647: Current learning rate: 0.00076
2024-06-26 21:46:17.493316: meanmse:       0.02377929
2024-06-26 21:46:17.494456: meanr2:        0.8080695990200367
2024-06-26 21:46:17.494959: train_loss 0.6899
2024-06-26 21:46:17.495345: val_loss 0.7084
2024-06-26 21:46:17.495746: Pseudo dice [0.5]
2024-06-26 21:46:17.496172: Epoch time: 63.5 s
2024-06-26 21:46:19.379108: 
2024-06-26 21:46:19.379928: Epoch 27
2024-06-26 21:46:19.380455: Current learning rate: 0.00075
2024-06-26 21:47:22.932807: meanmse:       0.023079973
2024-06-26 21:47:22.934368: meanr2:        0.8099370631731609
2024-06-26 21:47:22.934999: train_loss 0.7178
2024-06-26 21:47:22.935528: val_loss 0.689
2024-06-26 21:47:22.936060: Pseudo dice [0.5]
2024-06-26 21:47:22.936617: Epoch time: 63.56 s
2024-06-26 21:47:24.714390: 
2024-06-26 21:47:24.715096: Epoch 28
2024-06-26 21:47:24.715615: Current learning rate: 0.00074
2024-06-26 21:48:27.890471: meanmse:       0.022985756
2024-06-26 21:48:27.891510: meanr2:        0.8137048318295267
2024-06-26 21:48:27.891995: train_loss 0.6474
2024-06-26 21:48:27.892406: val_loss 0.6838
2024-06-26 21:48:27.892804: Pseudo dice [0.5]
2024-06-26 21:48:27.893235: Epoch time: 63.18 s
2024-06-26 21:48:29.743199: 
2024-06-26 21:48:29.744131: Epoch 29
2024-06-26 21:48:29.744781: Current learning rate: 0.00073
2024-06-26 21:49:33.003644: meanmse:       0.022321204
2024-06-26 21:49:33.004818: meanr2:        0.8214784850934016
2024-06-26 21:49:33.005287: train_loss 0.6129
2024-06-26 21:49:33.005668: val_loss 0.6727
2024-06-26 21:49:33.006044: Pseudo dice [0.5]
2024-06-26 21:49:33.006444: Epoch time: 63.27 s
2024-06-26 21:49:35.057815: 
2024-06-26 21:49:35.058636: Epoch 30
2024-06-26 21:49:35.059238: Current learning rate: 0.00073
2024-06-26 21:50:38.389706: meanmse:       0.016990848
2024-06-26 21:50:38.390961: meanr2:        0.8601101485540643
2024-06-26 21:50:38.391609: train_loss 0.6262
2024-06-26 21:50:38.392090: val_loss 0.5721
2024-06-26 21:50:38.392564: Pseudo dice [0.5]
2024-06-26 21:50:38.393054: Epoch time: 63.34 s
2024-06-26 21:50:38.393538: Yayy! New best R2: 0.8601
2024-06-26 21:50:40.509855: 
2024-06-26 21:50:40.510578: Epoch 31
2024-06-26 21:50:40.511087: Current learning rate: 0.00072
2024-06-26 21:51:44.475849: meanmse:       0.018147169
2024-06-26 21:51:44.476799: meanr2:        0.8523470552842326
2024-06-26 21:51:44.477241: train_loss 0.6099
2024-06-26 21:51:44.477627: val_loss 0.6019
2024-06-26 21:51:44.478030: Pseudo dice [0.5]
2024-06-26 21:51:44.478420: Epoch time: 63.97 s
2024-06-26 21:51:46.505637: 
2024-06-26 21:51:46.506373: Epoch 32
2024-06-26 21:51:46.506932: Current learning rate: 0.00071
2024-06-26 21:52:50.149864: meanmse:       0.019779926
2024-06-26 21:52:50.151336: meanr2:        0.8397878370842528
2024-06-26 21:52:50.151997: train_loss 0.5737
2024-06-26 21:52:50.152523: val_loss 0.62
2024-06-26 21:52:50.153137: Pseudo dice [0.5]
2024-06-26 21:52:50.153737: Epoch time: 63.65 s
2024-06-26 21:52:51.954966: 
2024-06-26 21:52:51.955799: Epoch 33
2024-06-26 21:52:51.956297: Current learning rate: 0.0007
2024-06-26 21:53:55.657935: meanmse:       0.020064302
2024-06-26 21:53:55.659067: meanr2:        0.8382778742537933
2024-06-26 21:53:55.659579: train_loss 0.5819
2024-06-26 21:53:55.660034: val_loss 0.6279
2024-06-26 21:53:55.660495: Pseudo dice [0.5]
2024-06-26 21:53:55.660944: Epoch time: 63.71 s
2024-06-26 21:53:57.456412: 
2024-06-26 21:53:57.457245: Epoch 34
2024-06-26 21:53:57.457753: Current learning rate: 0.00069
2024-06-26 21:55:00.949823: meanmse:       0.023327572
2024-06-26 21:55:00.950814: meanr2:        0.8152671784214104
2024-06-26 21:55:00.951345: train_loss 0.5662
2024-06-26 21:55:00.951727: val_loss 0.6936
2024-06-26 21:55:00.952099: Pseudo dice [0.5]
2024-06-26 21:55:00.952530: Epoch time: 63.5 s
2024-06-26 21:55:02.802032: 
2024-06-26 21:55:02.802571: Epoch 35
2024-06-26 21:55:02.802967: Current learning rate: 0.00068
2024-06-26 21:56:06.012599: meanmse:       0.019537056
2024-06-26 21:56:06.013818: meanr2:        0.8393678916648607
2024-06-26 21:56:06.014314: train_loss 0.5821
2024-06-26 21:56:06.014718: val_loss 0.5914
2024-06-26 21:56:06.015097: Pseudo dice [0.5]
2024-06-26 21:56:06.015472: Epoch time: 63.22 s
2024-06-26 21:56:07.878810: 
2024-06-26 21:56:07.879487: Epoch 36
2024-06-26 21:56:07.880037: Current learning rate: 0.00067
2024-06-26 21:57:10.848520: meanmse:       0.021135626
2024-06-26 21:57:10.849949: meanr2:        0.8293440060678967
2024-06-26 21:57:10.850662: train_loss 0.5847
2024-06-26 21:57:10.851255: val_loss 0.6429
2024-06-26 21:57:10.851828: Pseudo dice [0.5]
2024-06-26 21:57:10.852470: Epoch time: 62.98 s
2024-06-26 21:57:12.656027: 
2024-06-26 21:57:12.656791: Epoch 37
2024-06-26 21:57:12.657332: Current learning rate: 0.00066
2024-06-26 21:58:15.939611: meanmse:       0.028518481
2024-06-26 21:58:15.940768: meanr2:        0.7735983572197026
2024-06-26 21:58:15.941295: train_loss 0.563
2024-06-26 21:58:15.941652: val_loss 0.7432
2024-06-26 21:58:15.942009: Pseudo dice [0.5]
2024-06-26 21:58:15.942428: Epoch time: 63.29 s
2024-06-26 21:58:17.806257: 
2024-06-26 21:58:17.806880: Epoch 38
2024-06-26 21:58:17.807372: Current learning rate: 0.00065
2024-06-26 21:59:21.220083: meanmse:       0.018449157
2024-06-26 21:59:21.220971: meanr2:        0.8521015791705285
2024-06-26 21:59:21.221465: train_loss 0.5573
2024-06-26 21:59:21.221889: val_loss 0.608
2024-06-26 21:59:21.222287: Pseudo dice [0.5]
2024-06-26 21:59:21.222683: Epoch time: 63.42 s
2024-06-26 21:59:23.044023: 
2024-06-26 21:59:23.044805: Epoch 39
2024-06-26 21:59:23.045260: Current learning rate: 0.00064
2024-06-26 22:00:26.623502: meanmse:       0.017576497
2024-06-26 22:00:26.624890: meanr2:        0.857168413263079
2024-06-26 22:00:26.625495: train_loss 0.5313
2024-06-26 22:00:26.626042: val_loss 0.5685
2024-06-26 22:00:26.626541: Pseudo dice [0.5]
2024-06-26 22:00:26.627078: Epoch time: 63.59 s
2024-06-26 22:00:28.810951: 
2024-06-26 22:00:28.811668: Epoch 40
2024-06-26 22:00:28.812165: Current learning rate: 0.00063
2024-06-26 22:01:32.618534: meanmse:       0.01652444
2024-06-26 22:01:32.619788: meanr2:        0.8667992412772633
2024-06-26 22:01:32.620590: train_loss 0.557
2024-06-26 22:01:32.621083: val_loss 0.5543
2024-06-26 22:01:32.621542: Pseudo dice [0.5]
2024-06-26 22:01:32.622029: Epoch time: 63.82 s
2024-06-26 22:01:32.622494: Yayy! New best R2: 0.8668
2024-06-26 22:01:35.030133: 
2024-06-26 22:01:35.031118: Epoch 41
2024-06-26 22:01:35.031658: Current learning rate: 0.00062
2024-06-26 22:02:38.548784: meanmse:       0.02020271
2024-06-26 22:02:38.550026: meanr2:        0.8353215002578885
2024-06-26 22:02:38.550619: train_loss 0.5268
2024-06-26 22:02:38.551109: val_loss 0.6142
2024-06-26 22:02:38.551574: Pseudo dice [0.5]
2024-06-26 22:02:38.552136: Epoch time: 63.53 s
2024-06-26 22:02:40.333910: 
2024-06-26 22:02:40.334846: Epoch 42
2024-06-26 22:02:40.335495: Current learning rate: 0.00061
2024-06-26 22:03:43.810195: meanmse:       0.01797673
2024-06-26 22:03:43.811380: meanr2:        0.8521352053773685
2024-06-26 22:03:43.811859: train_loss 0.5274
2024-06-26 22:03:43.812284: val_loss 0.5937
2024-06-26 22:03:43.812678: Pseudo dice [0.5]
2024-06-26 22:03:43.813045: Epoch time: 63.49 s
2024-06-26 22:03:45.645897: 
2024-06-26 22:03:45.646800: Epoch 43
2024-06-26 22:03:45.647381: Current learning rate: 0.0006
2024-06-26 22:04:49.131264: meanmse:       0.023190076
2024-06-26 22:04:49.133094: meanr2:        0.8109771962712995
2024-06-26 22:04:49.133847: train_loss 0.5398
2024-06-26 22:04:49.134682: val_loss 0.6738
2024-06-26 22:04:49.135513: Pseudo dice [0.5]
2024-06-26 22:04:49.136304: Epoch time: 63.5 s
2024-06-26 22:04:51.117480: 
2024-06-26 22:04:51.118511: Epoch 44
2024-06-26 22:04:51.119202: Current learning rate: 0.00059
2024-06-26 22:05:54.358914: meanmse:       0.018599428
2024-06-26 22:05:54.360320: meanr2:        0.850123544476273
2024-06-26 22:05:54.360907: train_loss 0.5479
2024-06-26 22:05:54.361400: val_loss 0.5852
2024-06-26 22:05:54.361947: Pseudo dice [0.5]
2024-06-26 22:05:54.362494: Epoch time: 63.25 s
2024-06-26 22:05:56.167934: 
2024-06-26 22:05:56.168625: Epoch 45
2024-06-26 22:05:56.169113: Current learning rate: 0.00058
2024-06-26 22:06:59.533340: meanmse:       0.02116202
2024-06-26 22:06:59.534712: meanr2:        0.8280647300948322
2024-06-26 22:06:59.535296: train_loss 0.5567
2024-06-26 22:06:59.535773: val_loss 0.6189
2024-06-26 22:06:59.536988: Pseudo dice [0.5]
2024-06-26 22:06:59.540559: Epoch time: 63.37 s
2024-06-26 22:07:01.364313: 
2024-06-26 22:07:01.365546: Epoch 46
2024-06-26 22:07:01.366172: Current learning rate: 0.00057
2024-06-26 22:08:05.130204: meanmse:       0.020398907
2024-06-26 22:08:05.131747: meanr2:        0.8355313813119923
2024-06-26 22:08:05.132409: train_loss 0.5229
2024-06-26 22:08:05.132970: val_loss 0.5905
2024-06-26 22:08:05.133472: Pseudo dice [0.5]
2024-06-26 22:08:05.133968: Epoch time: 63.78 s
2024-06-26 22:08:06.925183: 
2024-06-26 22:08:06.926074: Epoch 47
2024-06-26 22:08:06.926605: Current learning rate: 0.00056
2024-06-26 22:09:10.439743: meanmse:       0.021505795
2024-06-26 22:09:10.441095: meanr2:        0.8217411231794867
2024-06-26 22:09:10.441709: train_loss 0.5481
2024-06-26 22:09:10.442288: val_loss 0.6307
2024-06-26 22:09:10.442828: Pseudo dice [0.5]
2024-06-26 22:09:10.443381: Epoch time: 63.52 s
2024-06-26 22:09:12.200052: 
2024-06-26 22:09:12.200759: Epoch 48
2024-06-26 22:09:12.201272: Current learning rate: 0.00056
2024-06-26 22:10:15.833791: meanmse:       0.026244462
2024-06-26 22:10:15.835045: meanr2:        0.7883209896072949
2024-06-26 22:10:15.835686: train_loss 0.5485
2024-06-26 22:10:15.836203: val_loss 0.7274
2024-06-26 22:10:15.836708: Pseudo dice [0.5]
2024-06-26 22:10:15.837246: Epoch time: 63.64 s
2024-06-26 22:10:17.553970: 
2024-06-26 22:10:17.554736: Epoch 49
2024-06-26 22:10:17.555218: Current learning rate: 0.00055
2024-06-26 22:11:21.196121: meanmse:       0.027818047
2024-06-26 22:11:21.197151: meanr2:        0.7745353729502732
2024-06-26 22:11:21.197673: train_loss 0.5719
2024-06-26 22:11:21.198091: val_loss 0.7413
2024-06-26 22:11:21.198511: Pseudo dice [0.5]
2024-06-26 22:11:21.198943: Epoch time: 63.65 s
2024-06-26 22:11:23.327363: 
2024-06-26 22:11:23.328375: Epoch 50
2024-06-26 22:11:23.329096: Current learning rate: 0.00054
2024-06-26 22:12:26.551931: meanmse:       0.024153547
2024-06-26 22:12:26.553120: meanr2:        0.8041942549340785
2024-06-26 22:12:26.553671: train_loss 0.5506
2024-06-26 22:12:26.554154: val_loss 0.6881
2024-06-26 22:12:26.554609: Pseudo dice [0.5]
2024-06-26 22:12:26.555105: Epoch time: 63.23 s
2024-06-26 22:12:28.607935: 
2024-06-26 22:12:28.608616: Epoch 51
2024-06-26 22:12:28.609149: Current learning rate: 0.00053
2024-06-26 22:13:32.132389: meanmse:       0.019737476
2024-06-26 22:13:32.133875: meanr2:        0.8445462451923018
2024-06-26 22:13:32.134625: train_loss 0.5349
2024-06-26 22:13:32.135251: val_loss 0.5988
2024-06-26 22:13:32.135947: Pseudo dice [0.5]
2024-06-26 22:13:32.136640: Epoch time: 63.53 s
2024-06-26 22:13:33.963699: 
2024-06-26 22:13:33.964482: Epoch 52
2024-06-26 22:13:33.965002: Current learning rate: 0.00052
2024-06-26 22:14:37.003228: meanmse:       0.020963289
2024-06-26 22:14:37.004410: meanr2:        0.8269768128554993
2024-06-26 22:14:37.004947: train_loss 0.4876
2024-06-26 22:14:37.005418: val_loss 0.6182
2024-06-26 22:14:37.005877: Pseudo dice [0.5]
2024-06-26 22:14:37.006381: Epoch time: 63.05 s
2024-06-26 22:14:38.745484: 
2024-06-26 22:14:38.746260: Epoch 53
2024-06-26 22:14:38.746821: Current learning rate: 0.00051
2024-06-26 22:15:42.293043: meanmse:       0.01737255
2024-06-26 22:15:42.294030: meanr2:        0.8585796683887943
2024-06-26 22:15:42.294528: train_loss 0.5158
2024-06-26 22:15:42.294930: val_loss 0.5502
2024-06-26 22:15:42.295328: Pseudo dice [0.5]
2024-06-26 22:15:42.295740: Epoch time: 63.56 s
2024-06-26 22:15:44.131646: 
2024-06-26 22:15:44.132584: Epoch 54
2024-06-26 22:15:44.133134: Current learning rate: 0.0005
2024-06-26 22:16:47.808874: meanmse:       0.026066983
2024-06-26 22:16:47.809967: meanr2:        0.7906156907172452
2024-06-26 22:16:47.810479: train_loss 0.4621
2024-06-26 22:16:47.810907: val_loss 0.6843
2024-06-26 22:16:47.811330: Pseudo dice [0.5]
2024-06-26 22:16:47.811820: Epoch time: 63.69 s
2024-06-26 22:16:49.526988: 
2024-06-26 22:16:49.527927: Epoch 55
2024-06-26 22:16:49.528469: Current learning rate: 0.00049
2024-06-26 22:17:53.000100: meanmse:       0.023450762
2024-06-26 22:17:53.001274: meanr2:        0.8140294600842288
2024-06-26 22:17:53.001880: train_loss 0.4832
2024-06-26 22:17:53.002341: val_loss 0.662
2024-06-26 22:17:53.002749: Pseudo dice [0.5]
2024-06-26 22:17:53.003197: Epoch time: 63.48 s
2024-06-26 22:17:54.820609: 
2024-06-26 22:17:54.821336: Epoch 56
2024-06-26 22:17:54.821845: Current learning rate: 0.00048
2024-06-26 22:18:58.503557: meanmse:       0.019175876
2024-06-26 22:18:58.504937: meanr2:        0.8440832433781538
2024-06-26 22:18:58.505471: train_loss 0.5388
2024-06-26 22:18:58.505945: val_loss 0.5683
2024-06-26 22:18:58.506366: Pseudo dice [0.5]
2024-06-26 22:18:58.506809: Epoch time: 63.69 s
2024-06-26 22:19:00.304358: 
2024-06-26 22:19:00.305304: Epoch 57
2024-06-26 22:19:00.306029: Current learning rate: 0.00047
2024-06-26 22:20:03.837578: meanmse:       0.019742165
2024-06-26 22:20:03.838817: meanr2:        0.841476995641356
2024-06-26 22:20:03.839400: train_loss 0.4849
2024-06-26 22:20:03.839879: val_loss 0.5885
2024-06-26 22:20:03.840395: Pseudo dice [0.5]
2024-06-26 22:20:03.840901: Epoch time: 63.54 s
2024-06-26 22:20:05.704152: 
2024-06-26 22:20:05.704871: Epoch 58
2024-06-26 22:20:05.705418: Current learning rate: 0.00046
2024-06-26 22:21:09.319068: meanmse:       0.021026291
2024-06-26 22:21:09.320504: meanr2:        0.8285727383157234
2024-06-26 22:21:09.321098: train_loss 0.476
2024-06-26 22:21:09.321784: val_loss 0.6069
2024-06-26 22:21:09.322345: Pseudo dice [0.5]
2024-06-26 22:21:09.322857: Epoch time: 63.62 s
2024-06-26 22:21:11.348406: 
2024-06-26 22:21:11.349164: Epoch 59
2024-06-26 22:21:11.349699: Current learning rate: 0.00045
2024-06-26 22:22:14.601794: meanmse:       0.020745648
2024-06-26 22:22:14.602857: meanr2:        0.8360149549205922
2024-06-26 22:22:14.603381: train_loss 0.5027
2024-06-26 22:22:14.603829: val_loss 0.5883
2024-06-26 22:22:14.604257: Pseudo dice [0.5]
2024-06-26 22:22:14.604700: Epoch time: 63.26 s
2024-06-26 22:22:17.005301: 
2024-06-26 22:22:17.006028: Epoch 60
2024-06-26 22:22:17.007386: Current learning rate: 0.00044
2024-06-26 22:23:20.320169: meanmse:       0.017143881
2024-06-26 22:23:20.321548: meanr2:        0.8584861611195586
2024-06-26 22:23:20.322139: train_loss 0.4722
2024-06-26 22:23:20.322618: val_loss 0.551
2024-06-26 22:23:20.323083: Pseudo dice [0.5]
2024-06-26 22:23:20.323557: Epoch time: 63.32 s
2024-06-26 22:23:22.215941: 
2024-06-26 22:23:22.216863: Epoch 61
2024-06-26 22:23:22.218737: Current learning rate: 0.00043
2024-06-26 22:24:25.639744: meanmse:       0.01715813
2024-06-26 22:24:25.641020: meanr2:        0.860356063013322
2024-06-26 22:24:25.641623: train_loss 0.4628
2024-06-26 22:24:25.642128: val_loss 0.5404
2024-06-26 22:24:25.642596: Pseudo dice [0.5]
2024-06-26 22:24:25.643133: Epoch time: 63.43 s
2024-06-26 22:24:27.619289: 
2024-06-26 22:24:27.620227: Epoch 62
2024-06-26 22:24:27.620856: Current learning rate: 0.00042
2024-06-26 22:25:30.909651: meanmse:       0.015787153
2024-06-26 22:25:30.911033: meanr2:        0.8725493965130081
2024-06-26 22:25:30.911662: train_loss 0.4676
2024-06-26 22:25:30.912168: val_loss 0.5134
2024-06-26 22:25:30.912664: Pseudo dice [0.5]
2024-06-26 22:25:30.913148: Epoch time: 63.3 s
2024-06-26 22:25:30.913655: Yayy! New best R2: 0.8725
2024-06-26 22:25:33.356347: 
2024-06-26 22:25:33.357311: Epoch 63
2024-06-26 22:25:33.357927: Current learning rate: 0.00041
2024-06-26 22:26:37.304645: meanmse:       0.018017447
2024-06-26 22:26:37.305953: meanr2:        0.8543597194573226
2024-06-26 22:26:37.306533: train_loss 0.4682
2024-06-26 22:26:37.306992: val_loss 0.5501
2024-06-26 22:26:37.307420: Pseudo dice [0.5]
2024-06-26 22:26:37.307869: Epoch time: 63.96 s
2024-06-26 22:26:39.130022: 
2024-06-26 22:26:39.131048: Epoch 64
2024-06-26 22:26:39.131756: Current learning rate: 0.0004
2024-06-26 22:27:42.795063: meanmse:       0.0201964
2024-06-26 22:27:42.796132: meanr2:        0.8350143235123478
2024-06-26 22:27:42.796641: train_loss 0.4501
2024-06-26 22:27:42.797084: val_loss 0.5964
2024-06-26 22:27:42.797539: Pseudo dice [0.5]
2024-06-26 22:27:42.797991: Epoch time: 63.67 s
2024-06-26 22:27:44.617579: 
2024-06-26 22:27:44.618425: Epoch 65
2024-06-26 22:27:44.618992: Current learning rate: 0.00039
2024-06-26 22:28:48.566248: meanmse:       0.018106801
2024-06-26 22:28:48.567473: meanr2:        0.8540870165054795
2024-06-26 22:28:48.568044: train_loss 0.4693
2024-06-26 22:28:48.568516: val_loss 0.5604
2024-06-26 22:28:48.568957: Pseudo dice [0.5]
2024-06-26 22:28:48.569476: Epoch time: 63.96 s
2024-06-26 22:28:50.380525: 
2024-06-26 22:28:50.381225: Epoch 66
2024-06-26 22:28:50.381908: Current learning rate: 0.00038
2024-06-26 22:29:54.001232: meanmse:       0.02022181
2024-06-26 22:29:54.002497: meanr2:        0.8387801297255986
2024-06-26 22:29:54.003027: train_loss 0.4421
2024-06-26 22:29:54.003473: val_loss 0.598
2024-06-26 22:29:54.003911: Pseudo dice [0.5]
2024-06-26 22:29:54.004366: Epoch time: 63.63 s
2024-06-26 22:29:55.866214: 
2024-06-26 22:29:55.867114: Epoch 67
2024-06-26 22:29:55.867650: Current learning rate: 0.00037
2024-06-26 22:30:59.386545: meanmse:       0.016397025
2024-06-26 22:30:59.387711: meanr2:        0.8671039017350153
2024-06-26 22:30:59.388280: train_loss 0.4218
2024-06-26 22:30:59.388773: val_loss 0.5105
2024-06-26 22:30:59.389237: Pseudo dice [0.5]
2024-06-26 22:30:59.389695: Epoch time: 63.53 s
2024-06-26 22:31:01.245774: 
2024-06-26 22:31:01.246912: Epoch 68
2024-06-26 22:31:01.247732: Current learning rate: 0.00036
2024-06-26 22:32:04.916470: meanmse:       0.021106074
2024-06-26 22:32:04.922404: meanr2:        0.832450015895865
2024-06-26 22:32:04.923184: train_loss 0.4348
2024-06-26 22:32:04.923736: val_loss 0.6012
2024-06-26 22:32:04.924320: Pseudo dice [0.5]
2024-06-26 22:32:04.924907: Epoch time: 63.69 s
2024-06-26 22:32:06.809021: 
2024-06-26 22:32:06.809659: Epoch 69
2024-06-26 22:32:06.810189: Current learning rate: 0.00035
2024-06-26 22:33:10.488014: meanmse:       0.018653698
2024-06-26 22:33:10.489156: meanr2:        0.8479958736163984
2024-06-26 22:33:10.489871: train_loss 0.4313
2024-06-26 22:33:10.490334: val_loss 0.5662
2024-06-26 22:33:10.490773: Pseudo dice [0.5]
2024-06-26 22:33:10.491226: Epoch time: 63.69 s
2024-06-26 22:33:12.713973: 
2024-06-26 22:33:12.714803: Epoch 70
2024-06-26 22:33:12.715504: Current learning rate: 0.00034
2024-06-26 22:34:16.517774: meanmse:       0.02088212
2024-06-26 22:34:16.519197: meanr2:        0.8268599572328934
2024-06-26 22:34:16.519799: train_loss 0.46
2024-06-26 22:34:16.520308: val_loss 0.6027
2024-06-26 22:34:16.520992: Pseudo dice [0.5]
2024-06-26 22:34:16.521643: Epoch time: 63.81 s
2024-06-26 22:34:18.349010: 
2024-06-26 22:34:18.349818: Epoch 71
2024-06-26 22:34:18.350256: Current learning rate: 0.00033
2024-06-26 22:35:22.100200: meanmse:       0.015081352
2024-06-26 22:35:22.101391: meanr2:        0.8759188089862051
2024-06-26 22:35:22.101934: train_loss 0.4619
2024-06-26 22:35:22.102401: val_loss 0.4843
2024-06-26 22:35:22.102841: Pseudo dice [0.5]
2024-06-26 22:35:22.103300: Epoch time: 63.76 s
2024-06-26 22:35:22.103767: Yayy! New best R2: 0.8759
2024-06-26 22:35:24.514639: 
2024-06-26 22:35:24.515356: Epoch 72
2024-06-26 22:35:24.515974: Current learning rate: 0.00032
2024-06-26 22:36:27.941382: meanmse:       0.018351195
2024-06-26 22:36:27.942876: meanr2:        0.852996954121804
2024-06-26 22:36:27.943489: train_loss 0.442
2024-06-26 22:36:27.944003: val_loss 0.5644
2024-06-26 22:36:27.944525: Pseudo dice [0.5]
2024-06-26 22:36:27.945175: Epoch time: 63.44 s
2024-06-26 22:36:29.799809: 
2024-06-26 22:36:29.800368: Epoch 73
2024-06-26 22:36:29.800770: Current learning rate: 0.00031
2024-06-26 22:37:33.786547: meanmse:       0.020532092
2024-06-26 22:37:33.787682: meanr2:        0.833392634927026
2024-06-26 22:37:33.788171: train_loss 0.412
2024-06-26 22:37:33.788536: val_loss 0.5961
2024-06-26 22:37:33.788886: Pseudo dice [0.5]
2024-06-26 22:37:33.789286: Epoch time: 63.99 s
2024-06-26 22:37:35.646508: 
2024-06-26 22:37:35.647268: Epoch 74
2024-06-26 22:37:35.648320: Current learning rate: 0.0003
2024-06-26 22:38:39.336921: meanmse:       0.01954217
2024-06-26 22:38:39.338548: meanr2:        0.8415478709169782
2024-06-26 22:38:39.339208: train_loss 0.3975
2024-06-26 22:38:39.339700: val_loss 0.5551
2024-06-26 22:38:39.340195: Pseudo dice [0.5]
2024-06-26 22:38:39.340788: Epoch time: 63.7 s
2024-06-26 22:38:41.248393: 
2024-06-26 22:38:41.253247: Epoch 75
2024-06-26 22:38:41.253939: Current learning rate: 0.00029
2024-06-26 22:39:45.096938: meanmse:       0.018576028
2024-06-26 22:39:45.098501: meanr2:        0.8486972139861764
2024-06-26 22:39:45.099061: train_loss 0.4223
2024-06-26 22:39:45.099543: val_loss 0.5378
2024-06-26 22:39:45.099974: Pseudo dice [0.5]
2024-06-26 22:39:45.100442: Epoch time: 63.86 s
2024-06-26 22:39:46.900643: 
2024-06-26 22:39:46.901447: Epoch 76
2024-06-26 22:39:46.902031: Current learning rate: 0.00028
2024-06-26 22:40:50.613191: meanmse:       0.023414755
2024-06-26 22:40:50.614381: meanr2:        0.8078989680617098
2024-06-26 22:40:50.614926: train_loss 0.4606
2024-06-26 22:40:50.615415: val_loss 0.6395
2024-06-26 22:40:50.615894: Pseudo dice [0.5]
2024-06-26 22:40:50.616382: Epoch time: 63.72 s
2024-06-26 22:40:52.454873: 
2024-06-26 22:40:52.455756: Epoch 77
2024-06-26 22:40:52.456327: Current learning rate: 0.00027
2024-06-26 22:41:55.947536: meanmse:       0.018558567
2024-06-26 22:41:55.948886: meanr2:        0.8467160151352185
2024-06-26 22:41:55.949443: train_loss 0.4137
2024-06-26 22:41:55.949868: val_loss 0.5458
2024-06-26 22:41:55.950249: Pseudo dice [0.5]
2024-06-26 22:41:55.950682: Epoch time: 63.5 s
2024-06-26 22:41:57.816021: 
2024-06-26 22:41:57.816647: Epoch 78
2024-06-26 22:41:57.817060: Current learning rate: 0.00026
2024-06-26 22:43:01.673699: meanmse:       0.0200398
2024-06-26 22:43:01.674768: meanr2:        0.8366891674891768
2024-06-26 22:43:01.675281: train_loss 0.4251
2024-06-26 22:43:01.675658: val_loss 0.5607
2024-06-26 22:43:01.676054: Pseudo dice [0.5]
2024-06-26 22:43:01.676499: Epoch time: 63.87 s
2024-06-26 22:43:03.578630: 
2024-06-26 22:43:03.579379: Epoch 79
2024-06-26 22:43:03.579941: Current learning rate: 0.00025
2024-06-26 22:44:07.816308: meanmse:       0.021097109
2024-06-26 22:44:07.817369: meanr2:        0.8311137668851479
2024-06-26 22:44:07.817920: train_loss 0.3716
2024-06-26 22:44:07.818414: val_loss 0.5994
2024-06-26 22:44:07.818819: Pseudo dice [0.5]
2024-06-26 22:44:07.819322: Epoch time: 64.25 s
2024-06-26 22:44:10.184368: 
2024-06-26 22:44:10.185372: Epoch 80
2024-06-26 22:44:10.186014: Current learning rate: 0.00023
2024-06-26 22:45:14.287182: meanmse:       0.016302653
2024-06-26 22:45:14.288504: meanr2:        0.8671804192116448
2024-06-26 22:45:14.289095: train_loss 0.4098
2024-06-26 22:45:14.290490: val_loss 0.498
2024-06-26 22:45:14.290956: Pseudo dice [0.5]
2024-06-26 22:45:14.291411: Epoch time: 64.11 s
2024-06-26 22:45:16.354955: 
2024-06-26 22:45:16.356044: Epoch 81
2024-06-26 22:45:16.356761: Current learning rate: 0.00022
2024-06-26 22:46:20.307204: meanmse:       0.017349077
2024-06-26 22:46:20.308367: meanr2:        0.8600791734940674
2024-06-26 22:46:20.309016: train_loss 0.4429
2024-06-26 22:46:20.309522: val_loss 0.5203
2024-06-26 22:46:20.310036: Pseudo dice [0.5]
2024-06-26 22:46:20.310606: Epoch time: 63.97 s
2024-06-26 22:46:22.360497: 
2024-06-26 22:46:22.361648: Epoch 82
2024-06-26 22:46:22.362389: Current learning rate: 0.00021
2024-06-26 22:47:26.556267: meanmse:       0.021395924
2024-06-26 22:47:26.563895: meanr2:        0.8268560650637663
2024-06-26 22:47:26.564638: train_loss 0.4347
2024-06-26 22:47:26.565070: val_loss 0.6029
2024-06-26 22:47:26.566439: Pseudo dice [0.5]
2024-06-26 22:47:26.567101: Epoch time: 64.21 s
2024-06-26 22:47:28.477961: 
2024-06-26 22:47:28.478632: Epoch 83
2024-06-26 22:47:28.479148: Current learning rate: 0.0002
2024-06-26 22:48:32.990404: meanmse:       0.020681223
2024-06-26 22:48:32.991539: meanr2:        0.8296587771779305
2024-06-26 22:48:32.992093: train_loss 0.3829
2024-06-26 22:48:32.992591: val_loss 0.5851
2024-06-26 22:48:32.993058: Pseudo dice [0.5]
2024-06-26 22:48:32.993522: Epoch time: 64.52 s
2024-06-26 22:48:34.888559: 
2024-06-26 22:48:34.889260: Epoch 84
2024-06-26 22:48:34.889726: Current learning rate: 0.00019
2024-06-26 22:49:39.173124: meanmse:       0.016580457
2024-06-26 22:49:39.174710: meanr2:        0.8641922831266179
2024-06-26 22:49:39.175445: train_loss 0.4104
2024-06-26 22:49:39.175873: val_loss 0.5125
2024-06-26 22:49:39.176471: Pseudo dice [0.5]
2024-06-26 22:49:39.176913: Epoch time: 64.29 s
2024-06-26 22:49:41.103960: 
2024-06-26 22:49:41.104528: Epoch 85
2024-06-26 22:49:41.104942: Current learning rate: 0.00018
2024-06-26 22:50:45.363091: meanmse:       0.015369182
2024-06-26 22:50:45.364311: meanr2:        0.8766254834254631
2024-06-26 22:50:45.364815: train_loss 0.4162
2024-06-26 22:50:45.365246: val_loss 0.4731
2024-06-26 22:50:45.365719: Pseudo dice [0.5]
2024-06-26 22:50:45.366151: Epoch time: 64.27 s
2024-06-26 22:50:45.366644: Yayy! New best R2: 0.8766
2024-06-26 22:50:47.900728: 
2024-06-26 22:50:47.901487: Epoch 86
2024-06-26 22:50:47.902058: Current learning rate: 0.00017
2024-06-26 22:51:51.864135: meanmse:       0.016222056
2024-06-26 22:51:51.865199: meanr2:        0.8666853347277723
2024-06-26 22:51:51.865695: train_loss 0.3965
2024-06-26 22:51:51.866100: val_loss 0.4965
2024-06-26 22:51:51.866450: Pseudo dice [0.5]
2024-06-26 22:51:51.866840: Epoch time: 63.97 s
2024-06-26 22:51:53.658285: 
2024-06-26 22:51:53.659075: Epoch 87
2024-06-26 22:51:53.659481: Current learning rate: 0.00016
2024-06-26 22:52:57.827008: meanmse:       0.015057933
2024-06-26 22:52:57.828104: meanr2:        0.8777428356502589
2024-06-26 22:52:57.828516: train_loss 0.3782
2024-06-26 22:52:57.828842: val_loss 0.4886
2024-06-26 22:52:57.829159: Pseudo dice [0.5]
2024-06-26 22:52:57.829523: Epoch time: 64.18 s
2024-06-26 22:52:57.829901: Yayy! New best R2: 0.8777
2024-06-26 22:53:00.031128: 
2024-06-26 22:53:00.033060: Epoch 88
2024-06-26 22:53:00.033854: Current learning rate: 0.00015
2024-06-26 22:54:04.074556: meanmse:       0.020285329
2024-06-26 22:54:04.075436: meanr2:        0.8344408354355004
2024-06-26 22:54:04.075829: train_loss 0.3624
2024-06-26 22:54:04.076169: val_loss 0.5722
2024-06-26 22:54:04.076483: Pseudo dice [0.5]
2024-06-26 22:54:04.076813: Epoch time: 64.06 s
2024-06-26 22:54:06.057237: 
2024-06-26 22:54:06.058098: Epoch 89
2024-06-26 22:54:06.058804: Current learning rate: 0.00014
2024-06-26 22:55:10.099683: meanmse:       0.0169594
2024-06-26 22:55:10.100846: meanr2:        0.8636180245012318
2024-06-26 22:55:10.101355: train_loss 0.3628
2024-06-26 22:55:10.101794: val_loss 0.5245
2024-06-26 22:55:10.102232: Pseudo dice [0.5]
2024-06-26 22:55:10.102664: Epoch time: 64.05 s
2024-06-26 22:55:12.288226: 
2024-06-26 22:55:12.288935: Epoch 90
2024-06-26 22:55:12.289398: Current learning rate: 0.00013
2024-06-26 22:56:16.443933: meanmse:       0.018342117
2024-06-26 22:56:16.445189: meanr2:        0.853792135461315
2024-06-26 22:56:16.445754: train_loss 0.384
2024-06-26 22:56:16.446271: val_loss 0.5363
2024-06-26 22:56:16.446809: Pseudo dice [0.5]
2024-06-26 22:56:16.447381: Epoch time: 64.16 s
2024-06-26 22:56:18.300674: 
2024-06-26 22:56:18.301328: Epoch 91
2024-06-26 22:56:18.301930: Current learning rate: 0.00011
2024-06-26 22:57:22.053233: meanmse:       0.01590425
2024-06-26 22:57:22.054182: meanr2:        0.8716390275635771
2024-06-26 22:57:22.054689: train_loss 0.3938
2024-06-26 22:57:22.055118: val_loss 0.4955
2024-06-26 22:57:22.055538: Pseudo dice [0.5]
2024-06-26 22:57:22.055966: Epoch time: 63.76 s
2024-06-26 22:57:23.942313: 
2024-06-26 22:57:23.943170: Epoch 92
2024-06-26 22:57:23.943754: Current learning rate: 0.0001
2024-06-26 22:58:28.001921: meanmse:       0.020865163
2024-06-26 22:58:28.002993: meanr2:        0.8348705997409313
2024-06-26 22:58:28.003499: train_loss 0.3697
2024-06-26 22:58:28.003935: val_loss 0.5857
2024-06-26 22:58:28.004289: Pseudo dice [0.5]
2024-06-26 22:58:28.004763: Epoch time: 64.07 s
2024-06-26 22:58:29.850443: 
2024-06-26 22:58:29.851336: Epoch 93
2024-06-26 22:58:29.851839: Current learning rate: 9e-05
2024-06-26 22:59:33.957262: meanmse:       0.01774663
2024-06-26 22:59:33.958451: meanr2:        0.8596295486129826
2024-06-26 22:59:33.958966: train_loss 0.3805
2024-06-26 22:59:33.959403: val_loss 0.542
2024-06-26 22:59:33.959831: Pseudo dice [0.5]
2024-06-26 22:59:33.960292: Epoch time: 64.12 s
2024-06-26 22:59:35.890247: 
2024-06-26 22:59:35.891198: Epoch 94
2024-06-26 22:59:35.891828: Current learning rate: 8e-05
2024-06-26 23:00:40.048448: meanmse:       0.01621126
2024-06-26 23:00:40.050924: meanr2:        0.8664420977548439
2024-06-26 23:00:40.051419: train_loss 0.3783
2024-06-26 23:00:40.051817: val_loss 0.5099
2024-06-26 23:00:40.052194: Pseudo dice [0.5]
2024-06-26 23:00:40.052602: Epoch time: 64.17 s
2024-06-26 23:00:42.132756: 
2024-06-26 23:00:42.133415: Epoch 95
2024-06-26 23:00:42.133910: Current learning rate: 7e-05
2024-06-26 23:01:46.316859: meanmse:       0.01787604
2024-06-26 23:01:46.318154: meanr2:        0.8544863545120307
2024-06-26 23:01:46.318779: train_loss 0.3567
2024-06-26 23:01:46.319408: val_loss 0.5374
2024-06-26 23:01:46.319944: Pseudo dice [0.5]
2024-06-26 23:01:46.320554: Epoch time: 64.19 s
2024-06-26 23:01:48.191525: 
2024-06-26 23:01:48.193442: Epoch 96
2024-06-26 23:01:48.194078: Current learning rate: 6e-05
2024-06-26 23:02:52.395878: meanmse:       0.014869416
2024-06-26 23:02:52.396829: meanr2:        0.8786455924109388
2024-06-26 23:02:52.397342: train_loss 0.4133
2024-06-26 23:02:52.397781: val_loss 0.4935
2024-06-26 23:02:52.398203: Pseudo dice [0.5]
2024-06-26 23:02:52.398609: Epoch time: 64.21 s
2024-06-26 23:02:52.399034: Yayy! New best R2: 0.8786
2024-06-26 23:02:54.672009: 
2024-06-26 23:02:54.673122: Epoch 97
2024-06-26 23:02:54.673766: Current learning rate: 4e-05
2024-06-26 23:03:58.807202: meanmse:       0.021934709
2024-06-26 23:03:58.808178: meanr2:        0.8257769053098403
2024-06-26 23:03:58.808622: train_loss 0.3675
2024-06-26 23:03:58.808988: val_loss 0.6119
2024-06-26 23:03:58.809338: Pseudo dice [0.5]
2024-06-26 23:03:58.809711: Epoch time: 64.14 s
2024-06-26 23:04:00.900320: 
2024-06-26 23:04:00.900901: Epoch 98
2024-06-26 23:04:00.901289: Current learning rate: 3e-05
2024-06-26 23:05:05.190680: meanmse:       0.014736033
2024-06-26 23:05:05.191571: meanr2:        0.883108104197519
2024-06-26 23:05:05.192009: train_loss 0.3499
2024-06-26 23:05:05.192399: val_loss 0.4713
2024-06-26 23:05:05.192755: Pseudo dice [0.5]
2024-06-26 23:05:05.193134: Epoch time: 64.3 s
2024-06-26 23:05:05.193512: Yayy! New best R2: 0.8831
2024-06-26 23:05:07.594303: 
2024-06-26 23:05:07.594950: Epoch 99
2024-06-26 23:05:07.595471: Current learning rate: 2e-05
2024-06-26 23:06:12.027155: meanmse:       0.018323941
2024-06-26 23:06:12.028492: meanr2:        0.8499994598672758
2024-06-26 23:06:12.029145: train_loss 0.3611
2024-06-26 23:06:12.029685: val_loss 0.5411
2024-06-26 23:06:12.030174: Pseudo dice [0.5]
2024-06-26 23:06:12.030631: Epoch time: 64.44 s
2024-06-26 23:06:14.499089: Training done.
